# Linxu运维 学习笔记

## 第一章 Linxu基础

### 1.1 Linux简介

Linux 内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。

Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。

Linux 能运行主要的 UNIX 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。

**Linux的发行版：**

Linux 的发行版说简单点就是将 Linux 内核与应用软件做一个打包。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\1511849829609658.jpg)

目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\wKioL1bvVPWAu7hqAAEyirVUn3c446.jpg-wh_651x-s_3197843091.jpg)

**Linux 应用领域：**

今天各种场合都有使用各种 Linux 发行版，从嵌入式设备到超级计算机，并且在服务器领域确定了地位，通常服务器使用 LAMP（Linux + Apache + MySQL + PHP）或 LNMP（Linux + Nginx+ MySQL + PHP）组合。

目前 Linux 不仅在家庭与企业中使用，并且在政府中也很受欢迎。

- 巴西联邦政府由于支持 Linux 而世界闻名。
- 有新闻报道俄罗斯军队自己制造的 Linux 发布版的，做为 j.H.ost 项目已经取得成果。
- 印度的 Kerala 联邦计划在向全联邦的高中推广使用 Linux。
- 中华人民共和国为取得技术独立，在龙芯处理器中排他性地使用 Linux。
- 在西班牙的一些地区开发了自己的 Linux 发布版，并且在政府与教育领域广泛使用，如 Extremadura 地区的 gnuLinEx 和 Andalusia 地区的 Guadalinex。
- 葡萄牙同样使用自己的 Linux 发布版 Caixa Mágica，用于 Magalh?es 笔记本电脑和 e-escola 政府软件。

- 法国和德国同样开始逐步采用 Linux。

Linux是一个全面、丰富多彩的生态圈，主流的IT技术都是各路大牛基于linux环境开发：

- 数据库 MySQL、PostgreSQL
- Web Server Nginx
- 大数据 Hadoop、Spark
- 消息队列 kafka
- 虚拟化技术 kvm
- 容器 Docker、Kubernetes

这些软件你都能够很轻松的找到Linux环境下的使用手册，其他系统平台则不然。

运维工程师的职责：保障服务器运行稳定，网站7*24小时正常运转，负责Linux服务器部署、不断学习互联网相关运维技术、通过有效手段不断解决运维问题，是集网络、数据库、Web开发、运维、安全等诸多技能的工程师。

运维领域不同于其他岗位，涉及专业领域较宽、技能知识可以深度挖掘：

- 系统运维
- 网络运维
- 安全运维
- 数据库运维
- 大数据运维
- 运维开发

在大数据、人工只能、容器云技术的到来，运维工程师已然是任何一家技术公司必须依赖和大力投入的核心技术部门。

- 容器化加速，快速交付、持续创新、高效运维
- 云计算/IAAS 加速，不再需要维护机房，服务器托管的费劲的活
- devops解决传统运维痛点，促进Dev和ops之间的沟通，提升运维效率，软件能够更快速、频繁、可靠的交付

**Linux VS Windows：**

目前国内 Linux 更多的是应用于服务器上，而桌面操作系统更多使用的是 Windows。主要区别如下

| 比较     | Windows                                                      | Linux                                                        |
| :------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 界面     | 界面统一，外壳程序固定所有 Windows 程序菜单几乎一致，快捷键也几乎相同 | 图形界面风格依发布版不同而不同，可能互不兼容。GNU/Linux 的终端机是从 UNIX 传承下来，基本命令和操作方法也几乎一致。 |
| 驱动程序 | 驱动程序丰富，版本更新频繁。默认安装程序里面一般包含有该版本发布时流行的硬件驱动程序，之后所出的新硬件驱动依赖于硬件厂商提供。对于一些老硬件，如果没有了原配的驱动有时很难支持。另外，有时硬件厂商未提供所需版本的 Windows 下的驱动，也会比较头痛。 | 由志愿者开发，由 Linux 核心开发小组发布，很多硬件厂商基于版权考虑并未提供驱动程序，尽管多数无需手动安装，但是涉及安装则相对复杂，使得新用户面对驱动程序问题（是否存在和安装方法）会一筹莫展。但是在开源开发模式下，许多老硬件尽管在Windows下很难支持的也容易找到驱动。HP、Intel、AMD 等硬件厂商逐步不同程度支持开源驱动，问题正在得到缓解。 |
| 使用     | 使用比较简单，容易入门。图形化界面对没有计算机背景知识的用户使用十分有利。 | 图形界面使用简单，容易入门。文字界面，需要学习才能掌握。     |
| 学习     | 系统构造复杂、变化频繁，且知识、技能淘汰快，深入学习困难。   | 系统构造简单、稳定，且知识、技能传承性好，深入学习相对容易。 |
| 软件     | 每一种特定功能可能都需要商业软件的支持，需要购买相应的授权。 | 大部分软件都可以自由获取，同样功能的软件选择较少。           |

Windows的使用由于美观，便捷，早已深入人心，但是也仅限在PC端耀武扬威，由于Linux的开源、稳定、安全性、开发灵活性，同时因为WIndows系统的自身缺陷，也奠定了Linux操作系统在服务端的位置。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\28.jpeg)

虽说如此，普通用户想要转变Windows的使用，转变使用Linux还是比较费劲的，因为你曾经的点点点...全部变成了 `命令行` 形式。必须系统的、全面的学习Linux基础知识，方可使用。

**免费与收费：**

- 最新正版Windows10官方售价￥888
- Linux几乎免费（更多人愿意钻研开源软件，而收费的产品出现更多的盗版）

**软件与支持：**

- Windows平台：数量和质量的优势，不过大部分为收费软件；由微软提供技术支持和服务
- Linux平台：大多为开源软件，用户可以修改定制与发布，由于免费没有资金支持，部分软件质量可能欠缺

**安全稳定性：**

- Windows平台：三天两头修复补丁，仍然会中毒（即便装了360，瑞星，金山毒霸。。。。）
- Linux平台：安全问题很少，无需安装xx杀毒，xx卫士

 **使用习惯：**

- Windows：普通用户基本依靠图形界面操作，鼠标和键盘完成一切需求，上手简单容易
- Linux：兼具图形界面（需要带有桌面环境的发行版Linux）和完全命令行操作，无法使用鼠标，新手入门困难，需要学习后方可使用，熟练后效率极高！

**应用领域：**

- Linux：人们日常在Windows上访问的百度、谷歌、淘宝、qq、迅雷（xxxx大片），支撑这些软件运行的，后台是成千上万的Linux服务器，它们时时刻刻进行着忙碌的数据处理和运算
- Windows：可以运行英雄联盟、绝地求生、仙剑三、地下城与勇士、我的世界。。。等等游戏，而Linux开发的游戏几乎很少

---

### 1.2 计算机基础

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\43.gif)

如果计算机是一个人体，那么计算机也是有胳膊腿的

-  `CPU=脑袋瓜子` ：每个人会作的事情都不一样（微指令集的差异），但主要都是通过脑袋瓜子来进行判断与控制身体各部分的活动；
-  `内存=脑袋中放置正在被思考的数据的区块` ：在实际活动过程中，我们的脑袋瓜子需要有外界刺激的数据 （例如光线、环境、语言等） 来分析，那这些互动数据暂时存放的地方就是内存，主要是用来提供给脑袋瓜子判断用的信息。
-  `硬盘=脑袋中放置回忆的记忆区块` ：跟刚刚的内存不同，内存是提供脑袋目前要思考与处理的信息，但是有些生活琐事或其他没有要立刻处理的事情， 就当成回忆先放置到脑袋的记忆深处吧！那就是硬盘！主要目的是将重要的数据记录起来，以便未来将这些重要的经验再次的使用；
- `主板=神经系统`：好像人类的神经一样，将所有重要的元件连接起来，包括手脚的活动都是脑袋瓜子发布命令后， 通过神经（主板）传导给手脚来进行活动啊！
-  `各项周边设备=人体与外界沟通的手、脚、皮肤、眼睛等` ：就好像手脚一般，是人体与外界互动的重要关键！
-  `显卡=脑袋中的影像` ：将来自眼睛的刺激转成影像后在脑袋中呈现，所以显卡所产生的数据来源也是CPU控制的。
-  `电源供应器 （Power）=心脏` ：所有的元件要能运行得要有足够的电力供给才行！这电力供给就好像心脏一样，如果心脏不够力， 那么全身也就无法动弹的！心脏不稳定呢？那你的身体当然可能断断续续的～不稳定！

人体在活动的时候，最重要的就是脑袋瓜子，而大脑活动最重要的就是和记忆的交互。

任何外界的接触都必须由记忆记录下来，然后大脑中的CPU再进行判断，再告诉周边的设备，胳膊腿给与响应，如果想要找到以往的经验，那就得去更老的记忆中寻找。

- 人最重要的是脑瓜子
- 计算机最重要的是CPU和内存，CPU的数据来源与内存
- 过往的记忆如同计算机的磁盘，想起来了，就读取

**计算机的组成部分，就以普通家用主机分析，依次有：**

- 输入设备，键盘、鼠标
- 输出设备：屏幕
- 主机部分：机箱、机箱内的零件

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\37.jpeg)

我们主要通过键盘鼠标将数据输入到主机里面，再由主机的处理器进行计算，输出图像或是文本信息输出到屏幕中。

![img](http://book.luffycity.com/linux-book/pic/36.jpeg)

计算机硬件分类：

![image-20211008181018527](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008181018527.png)

其他系统硬件：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\55-16336876836319.jpeg)

主板上有很多接口，包括网卡、显卡、磁盘阵列等，以及与游戏玩家最看重的显卡，它控制着游戏画面、色彩、分辨率，以及存储相关，包括内存条、硬盘、软盘、光驱等。

输入设备指的就是触摸屏、键盘鼠标、VR体感游戏设备。

输出设备指的就是如电脑屏幕、打印机、音响、投影仪等等。

**计算机用途：**

`超级计算机:`

超级计算机是运行速度最快的电脑，但是他的维护、操作费用也最高！主要是用于需要有高速计算的计划中。 例如：国防军事、气象预测、太空科技，用在仿真的领域较多。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\57.jpeg" alt="img" style="zoom:50%;" />



`大型计算机:`

大型计算机通常也具有数个高速的CPU，功能上虽不及超级计算机，但也可用来处理大量数据与复杂的运算。 例如大型企业的主机、全国性的证券交易所等每天需要处理数百万笔数据的企业机构， 或者是大型企业的数据库服务器等等。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\58.jpeg" alt="img" style="zoom:50%;" />

`工作站:`

工作站的价格又比迷你电脑便宜许多，是针对特殊用途而设计的电脑。在个人电脑的性能还没有提升到目前的状况之前， 工作站电脑的性能/价格比是所有电脑当中较佳的，因此在学术研究与工程分析方面相当常见。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\59.jpeg)

`微型电脑:`

属于个人笔记本、台式机等

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\60.jpeg)

**计算机单位：**

`容量单位:`

对于计算机而言，只认识一个叫做`二进制`的容量单位，我们称之为 `bit` ，但是由于 `bit` 单位太小，计算机又用 `Byte` 单位来统计

```	
1 Byte = 8 bit
```

同样的，由于计算机存储越来越大，Byte也太小了，计算机又出现简化的单位KB、MB、GB、TB

![image-20211008181435739](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008181435739-163368807701515.png)

`速度单位:`

CPU的运算单位通常用MHz或者GHz这样的单位，Hz意味 `秒分之一` 

网络传输数据中以bit为单位，因此网络单位通常是 ** `Mbps（Mbits per second） 每秒多少Mbit` **

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\63.jpg" alt="img" style="zoom:50%;" />

如图3000Mbps传输速度理论上是除以8，5G速度也就是375MByte每秒下载速度。

我们常遇见这样的问题：

你买了一块1TB的硬盘，但是格式化后发现只有930GB左右。这是因为计算机单位是1024GB=1TB

而厂家磁盘计算是1000GB=1TB，因此容量在转换的时候，产生了缩水。

---

#### 1.2.1 CPU

服务器的 CPU 相当于人体的大脑，负责计算机的运算和控制，是服务器性能效率的最核心部件。 常见品牌:Intel，AMD

一般企业里的服务器，CPU 个(颗)数为 2-4 颗，单个(颗)CPU 是四核，内存总量一般是 16G-256G(32G， 64G)

做虚拟化的宿主机(eg:安装 vmware(虚拟化软件)的服务器)，CPU 颗数 4-8 颗，内存总量一般是 48G-128G，6- 10 个虚拟机。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\134.jpg" alt="img" style="zoom:50%;" />

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\42.gif)

无论是手机还是电脑，整个机器的最核心部件就是 `中央处理器(Central Processing Unit, CPU)` 

CPU作为特有的功能芯片，芯片内部有微指令集。

CPU工作主要在于调度与运算，在CPU内部主要分为两个单元：`算数逻辑单元`、`控制单元`

-  `算术逻辑单元(arithmetic and logic unit)` 是能实现多组算术运算和逻辑运算的组合逻辑电路，简称ALU。

![image-20211008182614318](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008182614318-163368877672419.png)

-  `控制单元（Control Unit）` 负责程序的流程管理。正如工厂的物流分配部门，控制单元是整个CPU的指挥控制中心，由指令寄存器IR(Instruction Register)、指令译码器ID(Instruction Decoder)和操作控制器OC(Operation Controller)三个部件组成，对协调整个电脑有序工作极为重要。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\47.jpg)

CPU读取的数据都是从内存而来，内存中的数据从键盘等输入单元而来。

CPU处理完毕的数据也必须写回内存中，最后到输出设备

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\48.png)

如图的流向，所有的数据都是经过内存再转出去，这个出/入是CPU发出的控制命令

CPU要处理的数据完全来自于内存，无论是应用程序还是文件读取，都是加载到内存中。

这就是为什么买计算机内存一定要足够大，机器运转就很流畅。

**CPU架构：**

所有的软件都需要被读取到内存中通过CPU内部的`微指令`调度。

世界现主流的CPU架构是：**精简指令集 `RISC` **和 **复杂指令集 `CISC` 两种**

**精简指令集 （Reduced Instruction Set Computer, RISC）**

此类CPU设计中，`微指令`比较精简，每个指令执行时间很短，完成的动作也很单纯，指令的执行性能较差。

常见的`RISC`微指令集CPU有甲骨文`Oracle`公司的SPARC系列、IDM公司的Power Architecture 系列、安某公司的ARM CPU系列。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\50.jpeg)

日常生活离不开CPU，华为的麒麟芯片基于ARM的架构之上，进行了自主研发的芯片，世界上95%的电子产品都是使用的ARM的架构，包括手机与平板，诸如 `苹果、高通、三星、华为` 、这些企业的产品都是在ARM上发展起来的。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\51.jpeg)

**复杂指令集（Complex Instruction Set Computer, CISC）**

CISC微指令集每个小指令都可以执行一些底层的硬件操作，指令数量多且复杂。

常见的CISC微指令集CPU是大家所熟悉的 `AMD、Intel`等`X86` 架构的CP。

x86架构的CPU大量使用在 `PC个人电脑` 上面。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\52.jpeg" alt="img" style="zoom: 33%;" />

想必大家用电脑这么久了，也听过32位、64位这样的词，这是2003年以前Intel开发的x86架构的CPU由8位升级到16、32位、后来最新的64位，个人笔记本CPU也被称作是x86_64的架构。

这里的位指的是CPU一次能够读取数据的最大量，64位CPU表示一次可以读写64bit的数据，而32位则是32bit的数据，因此从内存中读取数据是有限制的，32位的CPU最多只能搭配4G的内存。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\53.jpeg)

不同的x86架构的CPU差异在于微指令集的不同，先进的微指令集可以加速多媒体程序的解析，如4k视频加速，也能加强虚拟化技术(Intel-VT)的性能，再如节省电量损耗，省点电费也是不错的。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\54.jpeg)

**Intel主板架构：**

主板是连接各个元件的一个重要主体，在主板上沟通各个元件的芯片组设计尤为重要，很大程度影响计算机性能。

早期的芯片组通常分为两个桥接器来控制各个元件的沟通。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\65.jpg" alt="img" style="zoom:50%;" />

- 北桥：负责链接速度较快的CPU、内存条、显卡等
- 南桥：负责连接速度较慢的硬盘、USB、网卡等

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\66.png" alt="img" style="zoom:50%;" />

由于CPU需要大量运算，因此发热量很高，必须安装一颗风扇主动进行散热。

不同的CPU型号有着不同的脚位，搭配的主板芯片也不同，因此如果你要升级电脑性能，购买CPU的时候一定要看好插槽是否支持。

目前主流的CPU是以稳定性著称的Intel和速度快的AMD，因此办公类使用Intel较多，游戏玩家机器多是AMD。

**CPU主流封装类型：**

**封装的类型主要为三种：** **LGA，PGA，BGA**。

**PGA：**

PGA的全称叫做“pin grid array”，或者叫“插针网格阵列封装”。

PGA的特点就是针脚在CPU上，而主板上是一片小洞洞（针孔）

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\67.jpg" alt="img" style="zoom:50%;" />

**LGA：**

LGA的全称叫做“land grid array”，或者叫“平面网格阵列封装”。

LGA去掉了钎料和铜柱针脚，只留触点，针脚是在主板上的。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\68.jpg" alt="img" style="zoom:50%;" />

**BGA：**

BGA的全称叫做“ball grid array”，或者叫“球柵网格阵列封装”。

BGA封装也就是焊接的。 焊接方法就是通过植球板将焊锡球先用热风枪吹在CPU触点上，然后对准主板PCB加热即可。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\69.jpg" alt="img" style="zoom:50%;" />

---

#### 1.2.2 内存

内存是计算机中最重要的部件，它是计算机中的一个中间件。

解决了CPU和硬盘之间速度严重不对等的问题，是CPU和硬盘数据交互的桥梁。

默认情况下，CPU从内存读写数据，内存从硬盘读写数据。

为了提升效率，一般在开机或者软件在运行的时候，会将常用数据直接从硬盘直接读入内存，以待后续CPU使用，提高计算机运行效率。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\78.jpg" alt="img" style="zoom:50%;" />

- 内存是电脑的一个临时存储器，它只负责电脑数据的中转而不能永久保存。

- 作用:内存是 CPU 能够直接访问的存储器，CPU 从内存中读取操作指令和数据，又把运算或处

  理结果送回内存

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\71.png)

**特点：**

- 内存的容量和处理速度直接决定了电脑数据传输的快慢。

- 一般程序运行的时候会被调度到内存中执行，服务器关闭或程序关闭之后，数据自动从内存中

释放掉。

- 内存和 CPU、硬盘一起并称为电脑的三大件。

**内存种类：**

CPU使用的所有数据都来自于内存(main memory)，无论是软件或是数据，都必须读入到内存CPU方可调用。

我们平时使用的主要叫做`动态随机存取内存`，`（Dynamic Random Access Memory, DRAM）`

此内存特点是必须通电时才可以使用，断电后数据丢失

内存的发展主要是

- DDR
- DDR2
- DDR3
- DDR4

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\70.jpeg)

**多通道设计：**

电脑卡？加内存就行！

由于所有的数据都放在内存才行，所以内存量是越大电脑越快

但是你得注意两只内存最好相同品牌，相同频率，型号最好都一样，这是因为芯片组厂商将两个内存汇聚到一起，好比一只是64位，两只就是128位，这是`双通道`的概念。

双通道下内存下，数据是`同步读/写`到一对内存中，能够提升整体性能

台式机内存：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\72.jpg)



笔记本内存：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\73.gif)

**程序、进程、守护进程：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\135.gif)

程序：python / golang语言编写的代码文件，存放在磁盘中的静态数据

进程：已经执行的程序，进程数据已经加载到内存中

守护进程：daemon，伴随着主任务的结束就随之结束的程序

- 程序-----电脑里的文件
- 进程-----双击打开了文件
- 守护进程---正在编辑文件
- 关闭文件，内存中的数据被释放

**内存数据提升用户体验：**

`门户(大网站 )极端案例：`

大并发写入案例(抢红包、微博) 高并发、大数据量“写”数据:会把数据先写到内存，积累一定的量后，然后再定时或者定量的写到磁盘(减轻磁盘的压力，减少磁盘 IO Input/Output 磁盘的输入/输出 磁盘读写)，最终还是会把数据加载到内存中再对外提供访问。

**特点：** 

- 优点：写数据到内存，性能高速度快(微博，微信，SNS，秒杀)。

- 缺点：可能会丢失一部分在内存中还没有来得及存入磁盘的数据。 解决数据不丢的方法：
  - 服务器主板上安装蓄电池，在断电瞬间把内存数据回写到磁盘。
  - UPS(一组蓄电池)不间断供电(持续供电 10 分钟，IDC 数据中心机房-UPS 1 小时)。 UPS (Uninterruptible Power System/Uninterruptible Power Supply)，即不间断电源，是将蓄电池(多 为铅酸免维护蓄电池)与主机相连接，通过主机逆变器等模块电路将直流电转换成市电的系统 设备。
  - 选双路电的机房，使用双电源、分别接不同路的电，服务器要放到不同的机柜、地区。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\136.jpg)

`中小企业案例：`

对于并发不是很大、数据也不是特别大的网站，读多写少的业务，会先把数据写入到磁盘，然后再通过程序把写到磁盘的数据读入到内存里，再对外通过读内存提供访问服务。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\137.jpg)

核心思想就是，由于内存特性，将数据放入内存读写，比磁盘要快的多。

---

#### 1.2.3 显卡

显卡是计算机中最重要的图像输出设备；是将计算机系统所需要的显示信息进行转换驱动显示器，并向显示器提供逐行或隔行扫描信号，控制显示器的正确显示；是连接显示器和个人计算机主板的重要组件；是“人机对话”的重要设备之一；

显卡对于图像的显示至关重要，因为图像的显示会占用内存，因此显卡一般都会有一个内存的容量，这个显存容量的大小影响到屏幕分辨率与色彩的深度。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\80.gif)

早期一些3D运算工作还是交给CPU完成，但是CPU的任务太多，无力再去处理大量的图形计算，显卡厂家在显卡中嵌入了GPU加速的芯片。

对于一些大型3D游戏，要求显卡的运算能力也很高，数据传输也是越快越好。

**挖矿与显卡：**

简单来说，挖矿就是**利用芯片进行一个与随机数相关的计算**，得出答案后以此换取一个虚拟币。

虚拟币则可以**通过某种途经换取各个国家的货币**。

运算能力越强的芯片就能越快找到这个随机答案，理论上单位时间内能产出越多的虚拟币。

由于关系到随机数，只有恰巧找到答案才能获取奖励。

有可能一块芯片下一秒就找到答案，也有可能十块芯片一个星期都没找到答案。

越多芯片同时计算就越容易找到答案，内置多芯片的矿机就出现了。

而多台矿机组成一个“矿场”同时挖矿更是提高效率。

而矿池则是由多个“个体户”加入一个组织一起挖矿，无论谁找到答案挖出虚拟币，所有人同时按贡献的计算能力获得相应的报酬，这种方式能使“个体户”收入更稳定。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\84.jpg)

举一个通俗的例子：

- 我在一张纸上随便写一串数字，给出部分提示，谁猜对就给他奖金（挖矿）
- 聪明的人根据提示能作出更多猜测（计算能力）
- 有人出钱请许多人回来一起猜测（矿场）
- 有人召集大家一起猜测，无论谁猜到，按照每个人猜测次数比例分配奖金（矿池）

由这个例子可以看出，越聪明的人能够计算的越快，机会也越大，收益也更多。

因此比特币挖矿所需的计算芯片也需要更强大，芯片也从CPU 升级到GPU，计算能力更强，相应的越容易挖出虚拟币。

**显卡参数：**

如果你不是为了挖矿或是游戏使用，仅仅作为网络服务器，入门级的简单显卡也就足够，因为很少会进行图像处理，且linux服务器大多都是无图形化的。

显卡与主机连接的接口如下：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\85.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\86.jpg)

- VGA，模拟信号传输，主要为15针的连接，较老
- DVI，常与液晶屏幕的连接
- HDMI， 主流连接方式可以同时传输图像与声音，广泛应用在电视屏幕与台式机
- Display port，未来传输方式主流，可以同时传输图像与声音

---

#### 1.2.4 磁盘

我们的生活已经离不开电脑，日常工作，娱乐，影音都离不开计算机，它能够帮我存储大量的资料

- 学籍档案
- 工业数据
- 股票资产
- 英语资料

这些数据都是需要被记录与读取，能够存储很久的，存储介质有

- 机械硬盘
- 固态硬盘
- 软盘
- DVD
- U盘
- ...

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\87.jpg)

**由于计算机在工作时，CPU、输入输出设备与存储器之间要大量地交换数据，因此存储器的 存取速度和容量也是影响计算机运行速度的主要因素之一。特别是在服务器优化场景，硬盘的性能 是决定网站性能的重要因素。**

磁盘就是永久存放数据的存储器，磁盘上也是有缓存的(芯片)。

常用的磁盘(硬盘)都是 3.5 英寸的(sas,sata)，常规的机械硬盘，读取(性能不高)性能比内存 差很多，所以，在企业工作中，我们才会把大量的数据缓存到内存，写入到缓冲区，这是当今互联 网网站必备的解决网站访问速度慢的方案。

目前常用的硬盘分为`机械硬盘`和`固态硬盘`两种，相比来说，`固态硬盘速度快但是容量较小，价格高`；

机械硬盘`速度慢但是容量大，价格便宜。`

**磁盘接口：**

磁盘的接口：IDE，SCSI，SAS，SATA，IDE(SCSI 退出历史舞台)

磁盘的类型：`机械磁盘` 和 `ssd 固态硬盘`

性能与价格：SSD(固态)>SAS> SATA

**机械硬盘：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\88.jpg)

常见的硬盘内部构造如图，由圆形盘片、磁头臂、磁头、主轴马达组成。

我们电脑的数据都是写入在磁性的磁盘片上，`对磁盘进行读写的操作其实是这样`

- 主轴马达让磁盘转动
- 机械手臂可以伸展让磁头在磁盘上进行读写的动作

**盘片数据：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\90.jpg)

磁盘数据有三个核心概念，分别是

- 扇区 `sector，512bytes` 
- 磁道 `track `
- 柱面 `Cylinder` 

盘片在设计时就是在`盘片的同心圆上，切出一个个的小区块，这些小区块整合成一个圆形，让机器手臂上的磁头去存取。`

这个小区块，称之为 `扇区 sector` 

在同一个同心圆里的扇区组合而成的圆称之为 `磁道 track` 

磁盘里面可能会有多个盘片，因此所有盘片上的同一个磁道组成所谓的 `柱面cylinder` 

**数据写入方式**

由于同心圆的外圈比较大，因此更大程度利用磁盘空间，外围的圆拥有更多的扇区，因此数据读写通常由外圈到内圈的形式。

**磁盘分区：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\327.jpg)

**磁盘接口：**

磁盘也是主板上的一部分，通过接口连接，为了不断的提升传输速度，接口也分为了SATA、SAS、IDE与SCSI等。

外接式磁盘也包括USB、eSATA等接口，目前IDE已经被SATA取代，而SCSI被SAS取代。

那么主流的磁盘接口如下，SATA、USB、SAS

**SATA接口：**

SATA是Serial ATA的缩写，即串行ATA。它是一种电脑总线，主要功能是用作主板和大量存储设备(如硬盘及光盘驱动器)之间的数据传输之用。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\91.jpg)

SATA是市场目前主流的接口，在机械硬盘上使用广泛，且主要分三代：

| 版本     | 速度(MBytes) |
| -------- | ------------ |
| SATA 1.0 | 150          |
| SATA 2.0 | 300          |
| SATA 3.0 | 600          |

由于机械磁盘的物理特性，实际速度一般极限在150~200Mbytes/s

**SAS接口：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\92.jpg)

SAS(Serial Attached SCSI)即串行连接SCSI，是新一代的SCSI技术，和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等。

SAS是`并行SCSI接口之后`开发出的全新接口。

此接口的设计是为了`改善存储系统的效能、可用性和扩充性`，并且提供与SATA硬盘的兼容性。

能为带宽要求更高的主流服务器和企业级存储提供所需的`高性能、高扩展性`和`可靠性`。

SAS满足了诸如网上购物和银行交易等事务性数据应用环境中对`高频率和即时`、`随机数据存取`的需求。

**SAS和SATA磁盘在价格上差别很大，SATA磁盘价格低廉，SAS有着更高的存储附加属性，很多企业在数据中心还是使用的SAS磁盘，并且企业级磁盘阵列卡的连接插槽也是由SAS接口开发的**

**USB接口：**

USB即通用串行总线是连接计算机系统与外部设备的一种串口总线标准，也是一种输入输出接口的技术规范，被广泛地应用于个人电脑和移动设备等信息通讯产品，并扩展至摄影器材、数字电视(机顶盒)、游戏机等其它相关领域。

最新一代是USB 3.1，传输速度为10Gbit/s，三段式电压5V/12V/20V，最大供电100W ，新型Type C插型不再分正反。

如果磁盘是外接式的接口，那么与主板连接的就是USB接口了。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\93.jpg)

传统USB2.0速度较慢，新一代USB3.0则快很多了

| 版本    | 速度MB/s |
| ------- | -------- |
| USB 1.0 | 1.5      |
| USB 2.0 | 60       |
| USB 3.0 | 500      |
| USB 3.1 | 1000     |

以上只是一个理论数值，USB 3.0真实读写速度差不多在100Mbytes而已了。

**固态磁盘：**

![image-20211008211604470](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008211604470-163369896686012.png)

传统机械磁盘由于在启动的时候，需要驱动马达转动磁盘片，然后再确定数据再哪个扇区，再让磁头正确的读取数据，整个读取速度延迟是很高的！并且机械磁盘由于马达转动，工作时候会有震动伴随着些噪音。

因此厂商就用闪存制作成大容量的设备，依旧使用SATA或是SAS的接口，此类设备没有磁头与盘片，都是内存，和传统机械磁盘不同（Hard Disk Drive ，HDD），称之为固态硬盘（Solid State Disk 或 Solid State Driver, SSD）。

**HDD在运行时需要转动，所以抗震能力和性能比较弱，而且待机转动时功耗也更高一些（停转除外），读写时会有明显“吱”的声响。**

**SSD没有机械结构转动，所以抗震能力很强，性能也更好，同时功耗也低很多，工作时没有声音。**

SSD通过内存直接读写，数据几乎没有延迟且很快速

SSD的缺陷是有写入次数限制，通常SSD寿命为两年，可以用RAID机制保护SSD。

**设备文件：**

Linux**在设计时是一切接文件**

**零散文件整理：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\95.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\96.jpg)

想必同学们曾经使用过这个碎片整理功能，那会你可知道为什么要碎片整理吗？

磁盘在使用一段时间之后，由于反复写入与删除文件，磁盘的空闲扇区会分散到不同的物理位置上，从而使得文件不是在连续的扇区中，这样的话，大部分情况下是读写一堆零散文件，磁头到底要转多久才能找到数据呢？导致降低了磁盘的访问速度。因此偶尔进行碎片整理，减少文件碎片，能够提升体统速度。

**因此在测试磁盘性能时候，用每秒读写操作次数 （Input/Output Operations Per Second, IOPS）表示磁盘性能，数值越大，性能越高**

**Raid卡：**

磁盘阵列（Redundant Arrays of Independent Drives，RAID），有“独立磁盘构成的具有冗余能力的阵列”之意。

磁盘阵列是由很多块独立的磁盘，组合成一个容量巨大的磁盘组，利用个别磁盘提供数据所产生加成效果提升整个磁盘系统效能。

利用这项技术，将数据切割成许多区段，分别存放在各个硬盘上。

磁盘阵列还能利用同位检查（Parity Check）的观念，在数组中任意一个硬盘故障时，仍可读出数据，在数据重构时，将数据经计算后重新置入新硬盘中。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\139.jpg)

**你有很多土地，单独管理不方便，且效率很低，整合到一块，统一管理，发挥最大性能。**

RAID有多种整合方式

- 0
- 1
- 5
- 10
- ...

**Raid级别：**

互联网公司一般都会购买Raid卡

冗余从好到坏：raid1、raid10、raid5、raid0

性能从好到坏：raid0、raid10、raid5、raid1

成本从低到高：raid0、raid5、raid1、raid10

- 单台服务器，很重要，盘不多，系统盘 raid1。
- 数据库/存储服务器，主库 raid10，从库 raid5\ raid0(为了维护成本，raid10)
- web 服务器，如果没有太多数据的话，raid5,raid0(单盘)
- 有多台，监控\应用服务器，raid0,raid5。

---

#### 1.2.5 主板

主板是计算机中最重要的平台部件，也是电脑中最大的集成电路板，它直接或间接的将所有的设备连接在一起。主板的好坏直接决定了计算机速度的快慢和运行稳定。

同时主板也提供了大量的设备接口，为计算机扩展功能提供了可能。

主板一般为矩形电路板，上面安装了组成计算机的主要电路系统，一般有BIOS芯片、I/O控制芯片、键和面板控制开关接口、指示灯插接件、扩充插槽、主板及插卡的直流电源供电接插件等元件。

现在主板一般情况下都集成了三卡(显卡、网卡、声卡)，也有的只集成了声卡和网卡。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\98.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\145.jpg)

**BIOS：**

很多游戏玩家都喜欢玩“超频”，将CPU的倍频或者外频通过主板的BIOS功能调整为更高的频率，从而提升性能。

但是由于频率非正常速度，可能会造成死机蓝屏。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\75.jpg)

这样的修改动作是被记录到主板上的一个CMOS的芯片上，这个芯片需要额外的供电才可以达到记录功能，这就是为什么主板上有一块电池的原因！

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\74.jpg)



这个BIOS是在你开机的时候，按下【DEL、F12、F11】等按键进入的一个系统画面

BIOS（Basic Input Output System）是一套程序，这套程序是写死到主板上面的一个内存芯片中， 这个内存芯片在没有通电时也能够将数据记录下来，那就是`只读存储器（Read Only Memory, ROM`）。

BIOS 对电脑系统来讲是非常重要的，因为他掌握了系统硬件的详细信息与开机设备的选择等等。

Tip：

在学习虚拟化技术的时候，如果不支持VT，也是在BIOS这里修改CPU的设置的！

---

#### 1.2.6 电源

机箱用来装载计算机硬件，对硬件起到防尘，保护的作用，也有相应的防静电等作用

1）抗静电

2）机箱质量

3）机箱散热

4）机箱质量不易变形

5）机箱空间能满足扩展需求

**电源供应器（Power）**

在机箱内，有一个大大的特盒子，包含着很多电源线，这个就是电源供应器了。

计算机硬件中的CPU、内存、主板、硬盘等等都必须得供电方可使用，随着硬件性能逐步提升，性能较差的电源很可能造成供电不足，导致内存数据丢失等等问题。

- 服务器电源就是指使用在服务器上的电源(POWER)，它和 PC(个人电脑)电源一样，都是 一种开关电源。
- 服务器电源按照标准可以分为 ATX 电源和 SSI 电源两种。ATX 标准使用较为普遍，主要用于台 式机、工作站和低端服务器;而 SSI 标准是随着服务器技术的发展而产生的，适用于各种档次的服务器。
- 服务器电源相当于人体的心脏，保障电源供应，要选择质量好的电源。
- 生产中一般单个服务器核心业务最好使用双电源 **AB** 线路。
- 如果集群(一堆机器做一件事)的情况可以不用双电源。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\133.jpg)

**UPS不间断电源：**

UPS（Uninterruptible Power System/Uninterruptible Power Supply），即不间断电源，是将蓄电池（多为铅酸免维护蓄电池）与主机相连接，通过主机逆变器等模块电路将直流电转换成市电的系统设备。

主要用于给单台计算机、计算机网络系统或其它电力电子设备如电磁阀、压力变送器等提供稳定、不间断的电力供应。

当市电输入正常时，UPS 将市电稳压后供应给负载使用，此时的UPS就是一台交流式电稳压器，同时它还向机内电池充电；

当市电中断（事故停电）时， UPS 立即将电池的直流电能，通过逆变器切换转换的方法向负载继续供应220V交流电，使负载维持正常工作并保护负载软、硬件不受损坏。

UPS 设备通常对电压过高或电压过低都能提供保护。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\102.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\103.jpg)

---

#### 1.2.7 网卡

**有线网卡：**

网卡，又称网络适配器或网络接口卡（NIC），英文名为Network Interface Card。

在网络中，如果有一台计算机没有网卡，那么这台计算机将不能和其他计算机通信，它将得不到服务器所提供的任何服务了。

当然如果服务器没有网卡，就称不上服务器了，所以说网卡是服务器必备的设备，就像普通PC（个人电脑）要配处理器一样。

平时我们所见到的PC机上的网卡主要是将PC机和LAN（局域网）相连接，而服务器网卡，一般是用于服务器与交换机等网络设备之间的连接。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\105.jpg)

**无线网卡：**

无线网卡和普通电脑网卡作用一样，用来连接局域网。

它只是一个信号收发的设备，必须在已有无线网络环境下才能够使用，和有线网卡的区别只是不通过有线连接，采用无线信号连接的网卡设备。

无线网卡区分：

- USB无线上网卡

  ![image-20211008212924691](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008212924691-163369976673726.png)

- 台式机的PCI无线接口网卡

  ![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\236.jpeg)

- 笔记本电脑内置的 MINI-PCI 无线网卡。

  ![image-20211008212953586](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211008212953586-163369979478727.png)

**常见网卡故障：**

现在大多数笔记本都自带了无线网卡设备，在没有有线环境下，无线网卡能够更灵活的接入wifi上网，在网络出现故障的时候，该如何解决呢

1. 网卡是否插好
   - 确保不同接口类型的网卡，比如插入到位，确保网卡与计算机插槽之间紧密接触，否则计算机无法识别。

2. USB驱动是否正常
   - 在使用USB无线网卡时，确保USB端口打开，驱动正常工作，计算机能够识别USB硬件，方可使用

3. 无线客户端无法上网
   - 确保无线网卡在无线网络范围内，举例合适，否则信号很弱
   - 确保无线网络正常供电，否则无线客户端也无法连接
   - 确保正确的IP网络配置

---

#### 1.2.8 服务器基本知识

**什么是服务器：**

服务器也就是台计算机而已，同样的由CPU、主板、内存、磁盘、网卡等硬件组成。

不同的是，服务器的定义是`高性能计算机`，作为网络中的节点，处理网络通信中的数据、信息，是网络时代的根本灵魂。

服务器通常指`一个管理资源且未用户提供服务的计算机`，通常服务器分为`文件服务器`、`数据库服务器`、`应用程序服务器`。

服务器对比普通PC、`稳定性`、`安全性`、`性能`、`可扩展性`、`可管理性`等方面要求更高。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\106.gif)

我们能够7*24小时的访问淘宝网，这是因为服务器强大的稳定性，它甚至可以十年不关机，因为你无法保证某一个时段没有用户在购买商品，并且能够承受大量用户并发的访问网站压力。

因此服务器的硬件配置更加强悍，需要大量的进行计算、处理，服务器可以安装多个处理器、更多的内存、更多的磁盘，因此主板、机箱都较大。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\112.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\109.jpg)

服务器对于屏幕显示的要求很低，基本上都是无显示器，通过`远程管理`的方式即可，因此服务器基本都是集成显卡，而无需单独装显卡。

我们很难见识到真实的物理服务器，因为服务器一般都防止在`机房`托管，闲人免进，比如appe.com苹果公司网站的数据就放在了 `云上贵州`的服务器机房。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\111.png)

**服务器分类：**

`按规模分：`

- 大型服务器，计算中心、企业级
- 中级服务器，公司部分级
- 小型服务器，入门级服务器，个人云服务器

`按用途分：`

- web服务器
- 数据库服务器
- 文件服务器
- 邮件服务器
- 视频点播服务器

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\240.jpeg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\241.jpg)

**服务器以外形分类：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\113.jpg)

**`机架式服务器：`**

- 机架式服务器的外形看来不像计算机，而像“抽屉”，有 1U、2U、4U 等规格。
- 机架式服务器安装在标准的 19 英寸机柜里面。这种结构的多为功能型服务器。

机架式服务器如下图所示。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\124.jpg)

**`刀片式服务器：`**

- 刀片式服务器是指在标准高度的机架式机箱内可插装多个卡式的服务器单元，实现高可用和高密度。
- 打个形象的比喻，刀片式服务器就像是箱子里摆放整齐的书。

每一块"刀片"实际上就是一块系统主板。

它们可以通过"板载"硬盘启动自己的操作系统，如`Windows NT/2000、Linux·`等，类似于一个个独立的服务器，在这种模式下，每一块母板独立运 行自己的系统，服务于指定的不同用户群，相互之间没有关联，因此相较于机架式服务器和机 柜式服务器，单片母板的性能较低。

不过，管理员可以使用系统软件将这些母板集合成一个服 务器集群。在集群模式下，所有的母板可以连接起来提供高速的网络环境，并同时共享资源， 为相同的用户群服务。在集群中插入新的"刀片"，就可以提高整体性能。而由于每块"刀片"都是热插拔的，所以，系统可以轻松地进行替换，并且将维护时间减少到最小。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\125.jpg)

**`塔式服务器-更强壮的服务器：`**

- 塔式服务器(Tower Server)应该是最容易理解的一种服务器结构类型。
- 因为它的外形以及结构都跟立式 PC 差不多，当然，由于服务器的主板扩展性较强、插槽也多出一堆，所以个头比普通主板大一些，因此塔式服务器的主机机箱也比标准的 ATX 机箱要大，一般都会预留足够的内部空间以便日后进行硬盘和电源的冗余扩展。
- 但这种类型服务器也有不少局限性，在需要采用多台服务器同时工作以满足较高的服务器应用需求时，由于其个体比较大，占用空间多，也不方便管理，便显得很不适合。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\126.jpg)

**服务器尺寸：**

- 服务器以高度分类
- 高度指的是以Unit做统计单位，1U=1.75英寸=4.445 cm

**互联网常见服务器品牌：**

- DELL（大多数公司在用）
- HP
- IBM（百度，银行，政府）（贵）
- 浪潮
- 联想

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\114.jpeg)

**服务器品牌与型号：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\127-163370013259140.jpg)

Dell服务器品牌：

```
https://www.dell.com/zh-cn/work/shop/category/servers
```

**Dell R720：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\128.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\129.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\130.jpg)

**Dell R620：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\131.jpg)

IBM品牌：

- 1U服务器 ：3550/m3 3550/m5
- 2U服务器：3650
- 3U服务器：3850
- 8U服务器：3950

互联网公司在去IOE活动下，已不常用IBM品牌

```
IOE =  IBM Oracle Emc  （甲骨文 数据库  Emc存储设备）
```

HP品牌：

**DL380G7：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\132.jpg)

**机房托管：**

首先，为什么说要将服务器放入机房而不是直接放在办公室或企业小机房，有以下几个原因：

1. 企业的机房无法保证365天7*24小时都供电充足；

2. 企业的机房无硬件防护，病毒容易入侵；

3. 企业的机房接入的宽带或光纤是经过分流的民用带宽，速度慢；

4. 企业必须以较高成本雇佣较高技术能力的工程师进行长期维护；

5. 企业无法为服务器提供一个真正的机房运营环境，服务器使用寿命会缩短，并且容易出现故障，造成数据流失或损毁。

那么，真正的数据机房正是为了服务器更好、更稳、更快、更安全运行而建设的，IDC数据中心服务器托管业务它能提供更适合服务器运行的环境，能提供更强有力的安全保障，能提供更高效的带宽资源。

其次，在当下机房林立的IDC环境中，选择哪些机房做服务器托管会更安全，性价比更高呢？

1. 专业的电信或联通或双线机房更能保证稳定；

2. 位于国家CHINA NET骨干网上的机房更能保证速度；

3. 技术和业务口碑都比较好的机房更能提供好的技术服务和安全防护

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\115.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\116.gif)

**云服务器：**

**1、云服务器操作及升级更方便**

传统服务器中的资源都是有限的，如果想要获得更好的技能，只能升级云服务器，所谓“云”，就是网络、互联网的意思，云服务器就是一种简单高效、安全可靠、处理能力可弹性伸缩的计算服务。其操作起来更加简便，如果原来使用的配置过低，完全可以在不重装系统的情况下升级CPU、硬盘、内存等，不会影响之前的使用。

**2、云服务器的访问速度更快**

云服务器又叫云主机。其使用的带宽通常是多线互通，网络能够自动检测出那种网络速度更快，并自动切换至相对应的网络上进行数据传输。

**3、云服务器的存储更便捷**

云服务器上能够进行数据备份，因此即使是硬件出现问题，其数据也不会丢失。并且，使用云服务器只需要服务商后期正常维护就可以了，为企业解决了很多后顾之忧。

**4、云服务器安全稳定**

云服务器是一种集群式的服务器，可以虚拟出多个类似独立服务器的部分，具有很高的安全稳定性。而且云服务器是支持异节点快速重建的，即使计算节点异常中断或损坏，也可以在极短时间内通过其他不同节点重建虚拟机，且不影响数据完整。

**5、云服务器有更高的性价比**

云服务器是按需付费的，与传统服务器相比，具有更高的性价比，而且并不会造成资源浪费。

当然，除开这些显著特点以外，更重要的是要选择一个知名的服务商，这样云服务器才能更加简便高效，不会给企业带来不必要的损失。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\107.gif)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\239.jpg)

**服务器与远程管理卡：**

远程管理卡是安装在服务器上的硬件设备，提供一个以太网接口，使它可以连接到局域网内，提供远程访问。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\140.jpg)

远程管理卡有服务器自带的，也有独立的。

服务器自带的远程管理卡，可以关机、开机，但是看不到开关的显示过程。

所以，选择独立的远程管理卡，稍微 200 块钱。

有了管理卡就可以快速恢复服务。

大客户有 KVM 远程管理，特大客户会有自己的人员驻扎机房。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\141.jpg)

**机房服务器布线：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\142.jpg)

**专业和非专业：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\143.jpg)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\144.jpg)

---

#### 1.2.9 软件基本知识

**计算机数据记录单位：**

由于计算机是通过电位记录信息的，因此仅能识别 0 和 1 这两个数字，故而在计算机内部，数据都 是以二进制的形式存储和运算的，下面列出来计算机数据的常用计量单位。

二进制，是计算技术中广泛采用的一种数制，由德国数理哲学大师莱布尼茨于1679年发明。

二进制数据是用0和1两个数码来表示的数。它的基数为2，进位规则是“逢二进一”，借位规则是“借一当二”。

当前的计算机系统使用的基本上是二进制系统，数据在计算机中主要是以补码的形式存储的。

计算机中的二进制则是一个非常微小的开关，用“开”来表示1，“关”来表示0。

1. `位(bit)` 计算机存储数据的最小单位为位(bit)，中文称为`比特`，一个二进制位表示 0 或 1 两种状态，要表 示更多的信息，就要把多个位组合成一个整体，一般以 8 位二进制数组成一个基本单位。

2. `字节(Byte)` 字节是计算机数据处理的基本单位。字节(Byte)简记为 B，规定一个字节为 8 位，即 1B=8b 每个 字节都是由 8 个二进制位组成。
3. 数据换算关系

```
1Bite=8bit，1KB=1024B，1MB=1024KB，1GB=1024MB
1TB=1024GB，1PB=1024TB，1EB=1024PB，1ZB=1024EB
```

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\117.jpg)

早期的电脑利用真空管是否通电的特性，如果通电就是1，没通电就是0，言传至今，只有0/1的环境称之为二进制（binary）。

在线进制转换：https://tool.lu/hexconvert/

十进制是`逢十进一，在个位数归零，十位数加一`

二进制是`逢二进一位`

十进制数的含义

```
3456 = 3*10**3 + 4*10**2 + 5*10**1 + 6*10**0
```

二进制数与十进制

二进制转为十进制要从右到左用二进制的每个数去乘以2的相应次方，小数点后则是从左往右。

如果首位是0就表示正整数，如果首位是1则表示负整数，正整数可以直接换算，负整数则需要先取反再换算。

因为计算机内部表示数的字节单位是定长的。如8位、16位、32位。所以位数不够时，高位补零。

- 如要想二进制00101010转为十进制，因为以0开头，所以这是正整数，计算如下所示：

> （0 * 2的零次方） + （1* 2的一次方）+ （0 * 2 的二次方）+（1 * 2的三次方） +（0 * 2的四次方） +（1* 2的五次方） + （0 * 2的六次方） + （0 * 2的7次方） = 0 + 2 + 0 + 8 + 0 + 32 + 0 + 0 = 42

```
100100  = 1*2**5 + 0*2**4 + 0*2**3 + 1*2**2 + 0*2**1 + 0*2**0 = 36
```

十进制转二进制

- 采用"除2取余，逆序排列"法：

> 1.首先用2整除一个十进制整数，得到一个商和余数
> 2.然后再用2去除得到的商，又会得到一个商和余数
> 3.重复操作，一直到商为小于1时为止
> 4.然后将得到的所有余数全部排列起来，再将它反过来（逆序排列），切记一定要反过来！

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\v2-8a76f66b03a3d63176194f2cb4f96ca2_720w.jpg)

**字符编码：**

>  请看： https://www.cnblogs.com/alex3714/articles/5465198.html

**软件编译运行：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\119.gif)

计算机插上电之后，只是一堆机器而已，想要赋予灵魂，还得安装软件、操作系统。

对于计算机而言只认识0或1 ，计算机进行运算靠的是CPU，CPU是有自己的`微指令`的，想要计算机帮你干活，就得发送微指令集，CPU才能听懂去执行。

- 撰写机器语言
- 了解所有硬件参数，操控硬件指令
- 不可移植性，每个CPU的微指令集都是独特的，更换机器无法运行
- 专一性，如果程序员针对浏览器开发，换成视频播放器，又得重新编写

由于直接撰写机器语言太过于恐怖，电脑科学家设计出人类看得懂的程序语言，通过`编译器`将人类可以理解的语言再翻译为机器看得懂的语言，如此一来，学习撰写程序就容易多了。

从字面上看，“编译”和“解释”的确都有“翻译”的意思，它们的区别则在于翻译的时机安排不大一样。

打个比方：假如你打算阅读一本外文书，而你不知道这门外语，那么你可以找一名翻译，给他足够的时间让他从头到尾把整本书翻译好，然后把书的母语版交给你阅读；

或者，你也立刻让这名翻译辅助你阅读，让他一句一句给你翻译，如果你想往回看某个章节，他也得重新给你翻译。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\120.png)

**操作系统：**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\121.jpeg)

操作系统（Operating System, OS）其实也是一组程序， 这组程序的重点在于管理电脑的所有活动以及驱动系统中的所有硬件。

操作系统的核心功能就是让CPU可以开始判断逻辑与运算数值、 让内存可以开始载入/读出数据与程序码、让硬盘可以开始被存取、让网卡可以开始传输数据、 让所有周边可以开始运行等等。

总之，硬件的所有动作都必须要通过这个操作系统来达成就是了。

**系统调用：**

操作系统通常会提供好一组开发接口给程序员，程序员只需要遵循该接口就可以轻易的调用系统功能，例如python语言调用相关函数，操作系统核心就会主动将python语言转换成系统可以理解的语言。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\122.jpg)

- 操作系统核心是参考硬件的调用，所以同一个操作系统不能在不同的硬件架构下运行，好比Windows操作系统无法直接给手机使用
- 操作系统只是在管理硬件资源，如CPU、内存、文件系统等，只是让计算机处于准备工作的状态，想要达到如游戏、视频、浏览器等需求，还得额外开发软件
- 应用程序的开发是针对操作系统提供的接口，因此市面上有多种操作系统，也存在不同的开发人员、如Mac OS、IOS、windows等

---

## 第二章 Linux系统与环境搭建

### 2.1 Linux系统诞生

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\19.gif)

**操作系统：是一个人与计算机硬件的中介。**

操作系统，英文名称 Operating System，简称 OS，是计算机系统中必不可少的基础系统软件，它是应用程序运行以及用户操作必备的基础环境支撑，是计算机系统的核心。

操作系统的作用是管理和控制计算机系统中的硬件和软件资源，例如：它负责直接管理计算机系统 的各种硬件资源，如对 CPU、内存、磁盘等的管理，同时对系统资源供需的优先次序进行管理。

操作系统还可以控制设备的输入、输出以及操作网络与管理文件系统等事务。

同时，它也负责对计算机系统中各类软件资源的管理。例如各类应用软件的安装、运行环境设置等。下图给出了操作系统 与计算机硬件、软件之间的关系示意图。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\147.jpg)

操作系统就是处于用户与计算机硬件之间用于传递信息的系统程序软件。

操作系统在接收到用户输入后，将其传递给计算机系统硬件核心进行处理，然后再讲计算机硬件的处理结果返回给用户。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\148.jpg)

目前 PC（Intel x86 系列）计算机上比较常见的操作系统有 Windows、Linux、DOS、Unix 等。

**什么是Linux：**

Linux类似Windows，也就是款操作系统软件。

Linux是一套开放源代码程序的、可以自由传播的类Unix操作系统软件，且支持多用户、多任务且支持多线程、多CPU的操作系统。

Linux主要用在服务器端、嵌入式开发和个人PC桌面中，服务器端是重中之重。

我们熟知的大型、超大型互联网企业(百度，Sina，淘宝等)都在使用 Linux 系统作为其服务器端的程序运行平台，全球及国内排名前十的网站使用的主流系统几乎都是 Linux 系统。

从上面的内容可以看出，Linux 操作系统之所以如此流行，是因为它具有如下一些特点:

- 是开放源代码的程序软件，可自由修改;
- Unix系统兼容，具备几乎所有Unix的优秀特性;
- 可自由传播，无任何商业化版权制约;
- 适合 Intel 等 x86 CPU 系列架构的计算机，可移植性很高

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\u=1831492981,10389919&fm=26&fmt=auto.jpeg)

**Unix历史：**

Unix系统在1969年的AT&T的贝尔实验室诞生，20世纪70年代，它逐步盛行，这期间，又产生 了一个比较重要的分支，就是大约 1977 年诞生的 BSD(Berkeley Software Distribution)系统。

从BSD 系统开始，各大厂商及商业公司开始了根据自身公司的硬件架构，并以 BSD 系统为基础进行Unix 系统的研发，从而产生了各种版本的 Unix 系统。

- SUN公司的Solaris
- IBM公司的AIX
- HP公司的HP UNIX

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\151.jpg)

在上图中可以看到，本章的“主人公”Linux 系统，诞生于 1991 年左右，因此，可以说 Linux 是从 Unix 发展而来的。

**Unix的五大优势：**

- 技术成熟、可靠性高
- 可伸缩性，Unix 支持的 CPU 处理器体系架构非常多，包括 Intel/AMD 及 HP-PA、MIPS、PowerPC、UltraSPARC、ALPHA 等 RISC 芯片，以及 SMP、MPP 等技术。
- 强大的网络功能，Internet 互联最重要的协议 **TCP/IP** 就是在 Unix 上开发和发展起来的。此外，Unix 还支持非常多的 常用的网络通信协议，如 NFS、DCE、IPX/SPX、SLIP、PPP 等。
- 强大的数据库能力，Oracle、DB2、Sybase、Informix 等大型数据库，都把 Unix 作为其主要的数据库开发和运行平台， 一直到目前为止，依然如此。
- 强大的开发性，促使C语言诞生

**Unix操作系统的革命：**

- 70 年代中后期，由于各厂商及商业公司开发的 Unix 及内置软件都是针对自己公司特定硬件的，因此在其他公司的硬件上基本上无法直接运行。
- 70年代末，Unix又面临了突如其来的被AT&T回收版权的重大问题，特别是要求`禁止对学生群体提供Unix系统源码`。
- 在80年代初期，同样是之前Unix系统版权和源代码限制的问题，使得大学授课Unix系统束缚很多，因此，一位名为`Andrew Tanenbaum（谭宁邦）`的大学教授为了教学开发了`Minix`操作系统。
- 1984年，Richard Stallman斯托曼发起了`开发自由软件`的运动，且成立自有软件基金会（Free Software Foundation，FSF）和GNU项目

**GNU项目：**

当时发起这个自由软件运动和创建 GNU 项目的目的其实很简单，就是想开发一个类似 Unix 系统、 并且是自由软件的完整操作系统，也就是要解决 70 年代末 Unix 版权问题以及软件源代码面临闭源的问题，这个系统叫做 `GNU 操作系统` 。

这个 GNU 系统后来没有流行起来。现在的 GNU 系统通常是使用 Linux 系统的内核， 以及使用了GNU项目贡献的一些组件加上其它相关程序组成，这样的组合被称为 `GNU/Linux`操作 系统。

![image-20211009101245720](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009101245720-16337455667788.png)

**Linux系统诞生：**

 ![img](J:\149.jpeg) 

Linux 系统的诞生开始于芬兰赫尔辛基大学的一位计算机系的学生，名字为 Linus Torvalds。

Linux 的标志和吉祥物为一只名字叫作 Tux 的企鹅——Torvalds’Unix，下图所示。

 ![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\150.jpg) 

Linux Torvalds 林纳斯·托瓦兹1988年进入赫尔辛基大学选读计算机科学，他在学校接触到Unix这个操作系统，当时的Unix只提供16个终端，早期的计算机只有运算功能，终端提供输入输出，光是等待Unix的时间就很长，林纳斯这样的大神就决定自己开发一个操作系统！

**Linux系统发展史：**

1. 1984 年，Andrew S. Tanenbaum 开发了用于教学的 Unix 系统，命名为 MINIX。
2. 1989 年，Andrew S. Tanenbaum 将 MINIX 系统运行于 x86 的 PC 计算机平台。
3. 1990年，芬兰赫尔辛基大学学生LinusTorvalds首次接触MINIX系统。
4. 1991年，LinusTorvalds开始在MINIX上编写各种驱动程序等操作系统内核组件。
5. 1991 年底，Linus Torvalds 公开了 Linux 内核源码 0.02 版(http://www.kernel.org)，注意，这里公开的 Linux 内核源码并不是我们现在使用的 Linux系统的全部，而仅仅是 Linux 内核 kernel部分的代码。
6. 1993 年，Linux 1.0 版发行，Linux 转向 GPL 版权协议。
7. 1994 年，Linux 的第一个商业发行版 Slackware 问世。
8. 1996 年，美国国家标准技术局的计算机系统实验室确认Linux版本 1.2.13 (由 Open Linux公司打包)符合 POSIX 标准。
9. 1999 年，Linux 的简体中文发行版问世。
10.  2000 年后，Linux 系统日趋成熟，涌现大量基于 Linux 服务器平台的应用，并广泛应用于基于 ARM 技术的嵌入式系统中。

**Linux发展历程相关人物：**

我们一定要向前辈们致以深深地敬意，没有他们，就没有今天的 Linux 优秀系统存在了(下图所示)。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\152.jpg)

**Linux核心概念**

**自由软件：**

自由软件的核心就是没有商业化软件版权制约，源代码开放，可无约束自由传播。

> 注意：自由软件强调的是权利问题，而非是否免费的问题。大家一定要理解这个概念，自由软件中的自由是“言论自由”中的“自由”，而不是“免费啤酒”中的“免费”。

自由意味着 freedom，而免费意味着 free，这是完全不同的概念。

例如：Red Hat Linux 自由但不免费，CentOS Linux 是自由且免费的。

**自由软件关乎使用者运行、复制、发布、研究、修改和改进该软件的自由。**

**自由软件基金会FSF：**

FSF(Free Software Foundation)的中文意思是`自由软件基金会`，是 Richard Stallman于 1984年发起和创办的。

FSF 的主要项目是 GNU 项目。

GNU 项目本身产生的主要软件包括：`Emacs 编辑软件` 、 `gcc 编译软件` 、 `bash命令解释程序` 和 `编程语言` ，以及  `gawk (GNU’s awk)` 等。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\157.jpeg)

**GNU知识：**

GNU，GNU 计划，又称 **革奴计划** ，是由Richard Stallman 在 1984 年公开发起的，是 FSF 的主要项目。前面已经提到过，这个项目的目标是建立一套完全自由的和可移植的类 Unix 操作系统。但是 GNU 自己的内核 Hurd 仍在开发中，离实用还有一定的距离。

现在的 GNU 系统通常是使用 Linux 系统的内核、加上 GNU 项目贡献的一些组件，以及其他相关程 序组成的，这样的组合被称为 **GNU/Linux** 操作系统。

到 1991 年 Linux 内核发布的时候，GNU 项目已经完成了除系统内核之外的各种必备软件的开发。

在 Linus Torvalds 和其他开发人员的努力下， GNU 项目的部分组件又运行到了 Linux 内核之上，例如：GNU 项目里的 Emacs、gcc、bash、gawk 等，至今都是 Linux 系统中很重要的基础软件。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\158.jpeg)

**GPL知识：**

GPL 全称为 `General Public License` ，中文名为通用公共许可，是一个最著名的开源许可协议，开源社区最著名的 Linux 内核就是在 GPL 许可下发布的。

GPL 许可是由自由软件基金会(Free Software foundation)创建的。

1984 年，Richard Stallman 发起开发自由软件的运动后不久，在其他人的协作下，他创立了通用公共许可证(GPL)，这对推动自由软件的发展起了至关重要的作用，那么，这个 GPL 到底是什么意思呢?

GPL许可的核心，**是保证任何人有 `共享` 和 `修改自由软件` 的自由权利，任何人有权 `取得` 、 `修改` 和 `重新发布` 自由软件的源代码权利，但是必须同时给出具体更改的源代码。**

**重点回顾：**

- FSF 自由软件基金会(公司)==> GNU(项目)==> emacs gcc bash(命令解释器) gawk
- FSF(公司)===>GPL(员工守则)==>自由传播 修改源代码 但是必须把修改后也要发布出来。
- Linus Torvalds==>linux 内核

Linux 操作系统=linux 内核+GNU 软件及系统软件+必要的应用程序。

**Linux 系统各组成部分的贡献人员**

| **Linux** 内核        | **GNU** 组件(**gcc,bash**)          | 其他必要应用程序                         |
| --------------------- | ----------------------------------- | ---------------------------------------- |
| 开发者 Linus Torvalds | 项目发起人 Richard Stallman(斯托曼) | BSD Unix和X Windows 以及成千上万的程序员 |

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\154.jpg)

**Linux特点：**

Linux 系统之所以受到广大计算机爱好者的喜爱，主要原因有两个：

- Linux 属于自由软件，用户不用支付任何费用就可以获得系统和系统的源代码，并且可以根据自己的需要对源代码进行必要的修改，无偿使用，无约束地自由传播。
- Linux 具有 Unix 的全部优秀特性，任何使用 Unix 操作系统或想要学习 Unix 操作系统的人，都可以通过学习 Linux 来了解 Unix，同样可以获得 Unix 中的几乎所有优秀功能，并且Linux 系统更开放，社区开发和全世界的使用者也更活跃。

**Linux的应用领域：**

与 Windows 操作系统软件一样，Linux 也是一个操作系统软件。

但与 Windows 不同的是，Linux 是 一套开放源代码程序的，并可以自由传播的类 UNIX 操作系统软件，随着信息技术的更新变化，Linux 应用领域已趋于广泛。

如今的 `IT 服务器` 领域是  `Linux` 、 `UNIX` 、 `Windows` 三分天下，Linux 系统可谓是后起之秀，尤其是近 几年，服务器端 Linux 操作系统不断地扩大着市场份额，每年增长势头迅猛，并对 Windows 及UNIX 服务器市场的地位构成严重的威胁。

Linux 作为企业级服务器的应用十分广泛，利用 Linux 系统可以为 `企业构架 WWW 服务器` 、 `数据库 服务器` 、 `负载均衡服务器` 、 `邮件服务器` 、 `DNS 服务器` 、 `代理服务器(透明网关)` 、 `路由器` 等，不但使企业降低了运营成本，同时还获得了 Linux 系统带来的 `高稳定性` 和 `高可靠性` 。

随着 Linux 在服务器领域的广泛应用，从近几年的发展来看，该系统已经渗透到了 `电信、金融、政府、教育、银行、石油、科研` 等各个行业，同时各大硬件厂商也相继支持 Linux 操作系统。

这一切都在表 明，Linux 在服务器市场的前景是 `光明` 的 。

同时， `大型、超大型互联网企业(百度、新浪、淘宝等)` 都在使用 Linux 系统作为其服务器端的程序运行平台，全球及国内排名前十的网站使用的几乎都是Linux 系统，Linux 已经逐步渗透到 `各个领域的企业` 里。

**嵌入式 Linux系统应用领域：**

由于 Linux 系统开放源代码，功能强大、可靠、稳定性强、灵活，而且具有极大的伸缩性，再加上 它广泛支持大量的微处理器体系结构、硬件设备、图形支持和通信协议，因此，在嵌入式应用的领域里，从因特网设备（路由器、交换机、防火墙、负载均衡器等）到专用的控制系统（自动售货机、手 机、PDA、各种家用电器等），Linux 操作系统都有很广阔的应用市场。

特别是经过这几年的发展， 它已经成功地跻身于主流嵌入式开发平台。

例如，在 `智能手机领域` ， `Android Linux` 已经在智能手机 开发平台牢牢地占据了一席之地。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\u=3742930320,3001723047&fm=26&fmt=auto.jpeg)

**个人桌面 Linux 应用领域：**

所谓个人桌面系统，其实就是我们在办公室使用的个人计算机系统， 例如： `Windows XP、Windows 7、MAC` 等。Linux 系统在这方面的支持也已经非常好了，完全可以满足日常的办公及家 用需求，例如：

- 浏览器上网浏览(例如:Firefox 浏览器);
- 办公室软件(OpenOffice，兼容微软 Office 软件)处理数据;
- 收发电子邮件(例如:ThunderBird 软件);
- 实时通信(例如:QQ 等);
- 文字编辑(例如:vi、vim、emac);
- 多媒体应用。

虽然 Linux 个人桌面系统的支持已经很广泛了，但是在当前的桌面市场份额还远远无法与 Windows系统竞争，这其中的障碍可能不在于 Linux 桌面系统产品本身，而在于用户的使用观念、操作习惯 和应用技能，以及曾经在 Windows 上开发的软件的移植问题。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\160.gif)

**Linux的发行版本介绍：**

Linux 内核(kernel)版本主要有 4 个系列，分别为 `Linux kernel 2.2`、`Linux kernel 2.4`、`Linux kernel 2.6`，`Linux kernel3.x` ，更多更新的内核版本请浏览： https://www.kernel.org/

Linux 的发行商包括 Slackware、Redhat、Debian、Fedora、TurboLinux、Mandrake、SUSE、**CentOS**、**Ubuntu**、红旗、麒麟......

其中几个重要的发行版本：

**Red Hat**：Red Hat Linux 9.0 的内核为 2.4.20。在版本 9.0 后，Red Hat 不再遵循 GPL 协议，成为收费 产品(但仍开源)，发展的新版本依次为 Red Hat 3.x、Red Hat 4.x、Red Hat 5.x、Red Hat 6.x、Red Hat 7.x、Red Hat Enterprise 6.x。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\161.jpeg)

**Fedora**：为 Red Hat 的一个分支，仍遵循 GPL 协议，可以认为是 Red Hat 预发布版。(游戏公测)

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\162.jpg)

**CentOS (Community Enterprise Operating System)**：与 redhat 做到二进制级别的一模一样。

Red Hat的另一个重要分支，以 Red Hat 所发布的源代码重建符合 GPL 许可协议的 Linux 系统，即将 Red Hat Linux 源代码的商标 LOGO 以及非自由软件部分去除后再编译而成的版本，目前 CentOS 已被Red Hat 公司收购，但仍开源免费。

CentOS Linux 是国内互联网公司使用最多的 Linux 系统版本，后面所有的内容讲解都是基于 CentOS 这个操作系统的，绝大部分内容 几乎无需任何修改同样适合其它操作系统版本。

提示：有关 Linux 操作系统，记住 `Redhat、CentOS、Ubuntu、Fedora、SUSE、Debian`  等即可。

**Redhat 与CentOS 的区别和联系，有时会被面试官问到，需要重点了解。**

| Linux发行版选择     |                                                             |
| ------------------- | ----------------------------------------------------------- |
| 服务器端 linux 系统 | 首选 Redhat(有钱任性)或 CentOS 这两者当中选                 |
| Linux桌面系统       | Ubuntu开发人员开放平台                                      |
| 安全性要求很高      | Debian或FreeBSD                                             |
| 数据库高级服务      | SUSE德国                                                    |
| 新技术，新功能      | Fedora > 稳定测试后 > redhat （去除logo、收费条款，Centos） |
| 中文版              | 红旗Linux、麒麟Linux                                        |

**选择 CentOS Linux 的版本：**

本章讲解的 Linux 运维技术主要是基于 CentOS x86_64 Linux 的，绝大部分知识几乎无需任何修改同样也适用于 Red Hat Linux 等同源或类似 Linux 系统版本。

下面是 CentOS 的主流版本在国内互联网企业的使用现状说明：

- `CentOS 5 系列`:占 25%左右，主流版本有 CentOS 5.5、CentOS 5.8、CentOS 5.10、CentOS 5.11，不推荐新手学习了。===>linux 2.4
- `CentOS 6 系列`:占 45%左右，主流版本有 CentOS 6.2、CentOS 6.4、CentOS 6.6、CentOS 6.8， 推荐新手学习。===>linux 2.6
- `CentOS 7 系列`:刚刚发布不久，目前极少企业正式使用，因此不建议先去玩它。这里有一个企业使用的大环境问题，因为学会了没地方用不说，企业实际用的 CentOS 5 和 CentOS 6 系列你反而不会，那样就舍本逐末了，先学习6，再过 2 年等企业用上了 CentOS7，轻松就可以转过去， 根据企业的主流应用选择才是明智的。不要盲目选择最高的系统版本。

**面试技巧：大家被面试官问及使用的是什么操作系统时，一定要一次性说出来(系统版本、内核版 本、32 位还是 64 位)，例如：我的工作中使用的是 CentOS 6.9 x86_64 位 Linux 系统，内核版本为 2.6.32-573，这才是一个合格的 Linux 运维人员的表现。**

**查看操作系统发行版本**

![image-20211009114334858](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009114334858.png)

**下载CentOS系统ISO镜像：**

要安装 CentOS 系统,就必须有 CentOS 系统软件安装程序，可以通过浏览器访问 CentOS 的官方站点[http://www.centos.org](http://www.centos.org/), 然后在导航栏找到

Downloads->Mirrors 链接

点击进入后即可下载，但这是 国外的站点下载速度受限。

- 阿里巴巴开源镜像站：https://opsx.alibaba.com/mirror 
- 网易开源镜像站：http://mirrors.163.com/ 
- 清华大学开源镜像站：https://mirror.tuna.tsinghua.edu.cn/ 

**企业生产环境使用64位操作系统：**

目前绝大多数企业生产环境中，使用的都是 64 位 CentOS 系统，32 位与 64 位系统的定位和区别。

- 系统设计时的定位区别：

  64 位操作系统的设计定位是：满足 `机械设计和分析`、`三维动画`、`视频编辑和创作`，以及 `科学计算` 和 `高性能计算应用` 程序等领域，这些应用领域的共同特点就是需要有 `大量的系统内存` 和 `浮点性能` 。

  简单地说，**64** 位操作系统是为**高科技人员**使用**本行业特殊软件**的运行平台而设计的。

  而32位系统为普通计算机用户而设计，对系统硬件要求不高。

- 安装配置不同：

  64位操作系统只能安装在64位电脑上(CPU 必须是 64 位的)，并且只在针对64位的软件时才能发挥其最佳性能。

  32位操作系统既可以安装在32位(32位CPU)电脑上，也可以安装在 64 位 (64位CPU)电脑上。

  当然，此时 32位的操作系统是无法发挥64位硬件性能的。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\164.jpg)

- 运算速度不同：

  64 位==>8车道大马路

  32 位==>4车道马路

  64 位CPU GPRs(General-Purpose Registers，通用寄存器)的数据宽度为64 位，64 位指令集可以运行64位数据指令，也就是说处理器一次可提取64位数据(只要两个指令，一次提取8个字节的数据)，比32位提高了一倍(32位需要四个指令，一次只能提取4个字节的数据)，性能会相应提升。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\165.jpg)

- 寻址能力不同：

  支持的最大内存不同：

  32 位系统 4GB 内存 3.5GB ===>PAE 技术支持更大内存

  64 位 Windows 7 x64 Edition 支持多达 128 GB 的物理内存。

  64 位处理器的优势还体现在操作系统对内存的控制上。

  由于地址使用的是特殊整数，因此一个 ALU(算术逻辑运算器)和寄存器可以处理更大的整数，也就是更大的地址。

  比如，Windows 7 x64 Edition 支持多达 128 GB 的物理内存和 16 TB 的虚拟内存，而 32 位的 CPU 和操作系统理论上最大只可支持 4GB 的内存，实际上也就是 3.2GB 左右的内存，当然 32 位系统是可以通过扩展来支持大 内存的，扩展所采用的是 PAE 技术。

**Linux历史回顾：**

- 贝尔实验室研发出 unix，后来停止公开源代码
- 谭宁邦教授为了教学，研发出 Minix 类 unix 系统
- 后 来 Linus Torvalds 接触到 Minix 之后想将这个系统移植到自己的 386 计算机上
- 1991 年将 0.02 内核 版本发到网上，才有了现在的 Linux

**本章重点回顾：**

- 了解什么是操作系统以及操作系统简单原理图。
- 了解Unix 的发展历史。
- 了解市面上的常见 Unix 系统版本。
- 了解Unix 及 Linux 诞生发展的几个关键人物。
- 重点了解 GNU,GPL 的知识。
- 了解Linux 系统的特点。
- 重点Linux 系统的常见发行版本，不同场景选择。
- 重点了解 CentOS 和 Redhat 的区别和联系。
- 了解 CentOS 各个版本的应用场景及企业应用情况。
- 学会搭建学习 Linux 的环境。注意：最好是能口头表达出上述了解的内容。

**本章考题：**

- 请详细描述 GNU 的相关知识和历史事件?(记忆-看图说话 蛋(unix)-人(谭宁邦)-人(斯 托曼)-人(Torvalds))
- 请描述什么是 GPL 以及 GPL 的内容细节?
- 企业工作中如何选择各 Linux 发行版?
- Red Hat Linux 和 CentOS Linux 有啥区别和联系?

---

### 2.2 VMware安装Linux

**虚拟机介绍：**

VMWare (Virtual Machine ware)是一个“虚拟PC”软件公司

它的产品可以使你在一台机器上同时运行二个或更多Windows、DOS、LINUX系统。

与“多启动”系统相比，VMWare采用了完全不同的概念。多启动系统在一个时刻只能运行一个系统，在系统切换时需要重新启动机器。

VMWare是真正“同时”运行，多个操作系统在主系统的平台上，就象标准Windows应用程序那样切换。

而且每个操作系统你都可以进行虚拟的分区、配置而不影响真实硬盘的数据，你甚至可以通过网卡将几台虚拟机用网卡连接为一个局域网，极其方便。

安装在VMware操作系统性能上比直接安装在硬盘上的系统低不少，因此，比较适合学习操作系统使用。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\166.jpg)

**VMware优点：**

通过虚拟机软件学习是初学者学习 Linux 运维的最佳方式。

利用虚拟机软件搭建 Linux 学习环境简单，容易上手，最重要的是利用虚拟机模拟出来的 **Linux** 和 真实的 **Linux** 几乎没有任何区别。

- 以后工作都是通过 ssh 连接到服务器，而不是直接跑机房，因此，用虚拟机软件来搭建环境是最接近企业工作环境的。
- 搭建 Linux 集群等大规模环境有时需要同时开启几台虚拟机(每台虚拟机仅需 256~512MB 内存、6~8GB 的硬盘空间即可)，虚拟机就可以轻松满足需求内存够大(8G 以上即可)。
- 自己租服务器?很贵成本太大。
- 方便修改配置，不会影响你的电脑，删除虚拟机你的电脑不会受影响，虚拟机只是一个运行在电脑上的程序。

>  企业真正服务器硬件手把手介绍(14-1)： http://v.qq.com/page/g/x/y/g016789xvxy.html

**下载安装VMware:**

> 下载链接：https://customerconnect.vmware.com/en/downloads/info/slug/desktop_end_user_computing/vmware_workstation_pro/15_0

根据机器配置，系统32/64位，xp/windows7/8/10 选择低/高版本VMware软件

**配置VMware：**

vmware系统服务必须开启：

```python
win键+r 输入services.msc
```

![image-20211009123319912](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009123319912.png)

**安装CentOS 7操作系统：**

**创建新的虚拟机===购买电脑**

1. 在 VMware 软件中，单击左上角的“文件”，在下拉菜单中选择“新建虚拟机”。

   ![image-20211009134026300](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009134026300-163375802881623.png)

2. 在弹出的“新建虚拟机向导”选项卡里面，选择“自定义(高级)”。选择完毕后，点击“下一步”。

   ![image-20211009134117008](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009134117008-163375807955824.png)

**选择虚拟机硬件兼容性**===**购买主机箱**

3. “硬件兼容性”一项，选择最新的，我用的是 VMware18 版本，所以我能选到的最新的是Workstation15.0。

   如果使用的是 VMware14 版本，那此处就选择 Workstation14.0。 选择完毕后，点击“下一步”。

![image-20211009150048522](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009150048522-163376285004025.png)

**安装客户机操作系统**

4. 我们后面要自己定制化安装 CentOS7 系统，所以此处选择“稍后安装操作系统”。选择完毕后，点击“下一步”。

![image-20211009150116180](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009150116180-163376287759826.png)

**选择客户机操作系统**===**安装什么样的系统**

5. 我们要学习的是 linux 系统，CentOS 也属于 Linux 系统的一种，所以此处当然要选择“Linux”，版本选择“CentOS 64 位”。选择完毕后，点击“下一步”。

![image-20211009150148136](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009150148136-163376291010227.png)

**命名虚拟机**===**专业规范**

6. 关于虚拟机名称，给一个建议，叫做所见即所得，或者叫见名知意，就是说打开 VMware 软件，不需要一台台的开启虚拟机去检查它是做什么用途的，只要看见每一台虚拟机的名字，就能够知道它是用来做什么的，这样能够增加规范性，也能够减少误操作的概率。

   位置一项，点击“浏览”后选择 事先规划好的位置即可。选择完毕后，点击“下一步”。

![image-20211009151228321](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009151228321-163376354946729.png)

**虚拟机硬件配置**

7. 我们学习的时候都是在自己的笔记本电脑上安装虚拟机，所以处理器(其实就是 CPU 的意思)都给1个就可以。选择完毕后，点击“下一步”。

![image-20211009151301150](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009151301150-163376358252230.png)

**内存配置**

8. 内存这一项需要注意，安装系统的时候，最好选择2G 或更多，待安装完系统后，再改成1G 即可。

   此处可以在左边的树状条直接用鼠标点击选择内存大小，也可以在右边的框内手动输入数字，需要注意单位是 MB，所以 2G 内存需要输入的数字是 2048，而不是2。选择完毕后，点击“下一步”。

![image-20211009151441075](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009151441075-163376368214731.png)

**选择网络类型**

9. 为了方便学习，“网络类型”这项，必须选择“使用网络地址转换(**NAT**)”，想要尝试其余几种网络类型的话，等变成 linux老鸟之后，再自行研究。选择完毕后，点击“下一步”。

![image-20211009151523549](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009151523549-163376372473032.png)

- VMware虚拟机常见的网络类型有 `bridged(桥接)`、 `**NAT**(地址转换`)、 `host-only(仅主机)` 3种，在分析如何选择之前，先要简单和大家介绍下这三种网络类型。
- 每家每户都有家庭住址号。
- 服务器上也需要家庭住址号，叫ip地址。
- vmware 虚拟机里 centos 系统获取 ip 地址有 3 种方式。

----

**NAT(地址转换)**

NAT(Network Address Translation)，网络地址转换，NAT模式是比较简单的实现虚拟机上网的方式，简单的理解，NAT模式的虚拟机就是通过宿主机(物理电脑)上网和交换数据的。

在NAT 模式下，`虚拟机的网卡` 连接到 `宿主机的 VMnet8`  上。

此时系统的 VMWare NAT Service 服务就充当了路由器， 负责将虚拟机发到 VMnet8 的包进行地址转换之后发到实际的网络上，再将实际网络上返回的包进行地址转换后通 过 VMnet8 发送给虚拟机。

VMWare DHCP Service 负责为虚拟机分配 IP 地址。NAT 网络类型的原理逻辑图如图所示。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\179.jpg)

NAT 网络特别适合于家庭里电脑直接连接网线的情况，当然办公室的局域网环境也是适合的，优势就是不会和其他物理主机 IP 冲突，且在没有路由器的环境下也可以通过 SSH NAT 连接虚拟机学习，换了网络环境虚拟机 IP 等不影响，这是推荐的选择。

**Bridged(桥接模式)：**

桥接模式可以简单理解为通过物理主机网卡架设了一座桥，从而连入到了实际的网络中。

因此，虚拟机可以被分配与物理主机相同网段的独立IP，所有网络功能和网络中的真实机器几乎完全一样。

桥接模式下的虚拟机和网内真实计算机所处的位置是一样的。

在 Bridged 模式下，电脑设备创建的虚拟机就像一台真正的计算机一样，它会直接连接到实际的网络上，逻辑上网与宿主机(电脑设备)没有联系。

Bridged 网络类型的原理逻辑图如图所示。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\180.jpg)

Bridged网络类型适合的场景：

- `特别适合于局域网环境` ，优势是虚拟机像一台真正的主机一样
- 缺点是可能会和其他物理主机 IP 冲突，并且在和宿主机交换数据时，都会经过实际的路由器，当不考虑 NAT 模式的时候，就选这个桥接模式，桥接模式下换了网络环境后所有虚拟机的 IP 都会受影响。

**Host-only**(仅主机)**===自娱自乐：**

在 Host-only 模式下，虚拟机的网卡会连接到宿主的 VMnet1上，但宿主系统并不为虚拟机提供任何路由服务，因此虚拟机只能和宿主机进行通信，不能连接到实际网络上，即无法上网。

Host-only 网络类型的原理逻辑图如图所示。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\181.jpg)

----

**选择 I/O 控制类型**

10. “I/O 控制器类型”这一项，直接默认默认即可，不需要改动。选择完毕后，点击“下一步”。

    ![image-20211009152603581](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009152603581-163376436489736.png)

**选择磁盘类型：**

11. “磁盘类型”这一项，也直接默认即可，不需要改动。选择完毕后，点击“下一步”。

    ![image-20211009152627283](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009152627283-163376438835137.png)

**选择磁盘**

12. “磁盘”这一项，选择“创建新虚拟机磁盘”，我们要安装新系统嘛，自然也创建一块空的新磁盘 是最好的。

    **注意千万别选“使用物理磁盘”，如果选了此项，那你那块盘里的所有东西就都被格式 化没了，会哭的哦。**

    选择完毕后，点击“下一步”。

    ![image-20211009152739850](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009152739850-163376446100138.png)



**指定磁盘容量**

13. “磁盘容量”这一项，学习期间不会产生多少数据，所以磁盘大小只要至少给到 10G 就行，当然如果你的硬盘很大很任性要给个 1T 这也是完全没有问题的。

    建议将磁盘存储为单个文件，比较方便，但这里不是硬性要求，看个人喜好。选择完毕后，点击“下一步”。

![image-20211009153009174](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009153009174.png)

**指定磁盘文件===存放vwware虚拟机系统文件**

14. “磁盘文件”的名字，保持默认的即可。选择完毕后，点击“下一步”。

![image-20211009153114363](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009153114363.png)

**完成创建虚拟机----完成配置准备付款购买**

15. 此时，一台新虚拟机的硬件就全部配置完毕了，检查确认无误后，就可以开机装系统了。确认完毕后，点击“完成”。

![image-20211009153217998](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009153217998-163376474008640.png)

**挂载 CentOS 镜像====光驱放入了DVD系统光盘**

16. 在“硬件”选项卡里面，还需要配置一下要使用的操作系统 iso 文件。在左侧选中“新CD/DVD(SATA)”，在右侧选中“使用 ISO 映像文件”，点击“浏览”按钮，在弹出的窗口中找到本地的 CentOS 系统 iso文件。选择完毕后，点击“关闭”。

![image-20211009153424417](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009153424417.png)

**开启虚拟机：**

17. 安装系统的第一步，要从开机开始。开机之前，请再次确认一下两块网卡的类型，一定要确保分别 是NAT。确认完毕硬件后，点击“开启此虚拟机”。

![image-20211009153600966](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009153600966.png)

**开始安装CentOS 7.5操作系统：**

18. 虚拟机开机后，选择“Install CentOS7”这一项。此时鼠标是不好用的，都是使用键盘的上下箭头来进行操作的，选好后按键盘上的回车键即可。

![image-20211016211646571](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211016211646571.png)

**修改CentOS7网卡命名规则，仍然以eth0命名：**

19. 开机后进入下面界面的时候 按↑键选择“**install CentOS**”，然后按下 **tab** 键，增加下面内容：

    ```
    net.ifnames=0 biosdevname=0
    ```

    输入完成后检查，并按下回车继续安装系统

![image-20211016211646571](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211016211646571.png)

**选择安装使用的语言**

20. 作为一个 linux 的学习者，要适应英文环境，所以强烈建议此处选择英文，而不选择中文。 选择完毕后，点击“Continue”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\192.jpg" alt="img" style="zoom:50%;" />

**设置时区**

21. 配置时区，点击“DATE&TIME”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\193.jpg" alt="img" style="zoom: 50%;" />

    ​	我们生活在中国嘛，所以时区选择“亚洲-上海”。时间不用管，待装完系统后，同步一下即可。 选择完毕后，点击“Done”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\194.jpg" alt="img" style="zoom:50%;" />

**最小化安装系统**

22. 选择需要安装的软件，点击“SOFTWARE SELECTION”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\195.jpg" alt="img" style="zoom:50%;" />

    安装 linux 系统，一般都采用最小化安装的原则，在初始时，只选择必要的几个软件包即可。学习期间，请点击跟下图中的红色框里的选择一模一样。选择完毕后，点击“Done”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\196.jpg" alt="img" style="zoom:50%;" />

**关闭KDUMP**

23. 配置“KDUMP”，这是一个内核崩溃时使用的东西，暂时不需要开启，把它关闭掉。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\197.jpg" alt="img" style="zoom:50%;" />

    把“Enable kdump”的勾选去掉即可。选择完毕后，点击“Done”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\198.jpg" alt="img" style="zoom:50%;" />

----

**磁盘分区：**

常规分区，企业生产场景中 Linux 系统的分区方案：

```
1.如果根据Red Hat的建议
	他们建议是分配RAM 20%的换空间，也就是RAM是8GB，分配1.6GB交换空间。

2.CentOS建议
	如果RAM小于2GB，就分配和RAM同等大小的Swap交换空间。
	如果RAM大于2GB，就分配2GB交换空间

3.Ubuntu考虑到系统需要休眠，
	如果RAM小于1GB，Swap空间至少要和RAM一样大，甚至是要为RAM的两倍大小
	如果RAM大于1GB，Swap交换空间应该至少等于RAM大小的平方根，并且最多为RAM大小的两倍
	如果要休眠，Swap交换大小应该等于RAM的大小加上RAM大小的平方根
```

- 常见网络集群架构中的节点服务器（多个功能一样的服务器，服务器数据有多份）分区方案：
  - /boot分区：存放引导程序，centos-6 给200M
  - swap：虚拟内存
    - 物理内存 < 8G ，swap分配 内存*1.5数量
    - 物理内存 > 8G，swap就给8G
  - / 根目录，存放所有数据，剩余空间都给根目录（/usr，/home，/var等分区共用/目录，如同c盘下的系统文件夹）

- 数据库角色的服务器，有大量数据需要访问（重要数据单独分区，便于备份和管理）
  - /boot ：存放引导程序，CentOS6分配200M，centOS7分配200M
  - Swap：虚拟内存
    - 物理内存 < 8G ，swap分配 8*1.5数量
    - 物理内存 > 8G，swap就给8G
  - /：根目录，50-200G，只存放系统相关文件，不存放数据文件
  - /data：剩余硬盘空间全部给/data

- 大型门户网站，大型企业分区思路
  - /boot:存放引导程序，CentOS6 给 200M，CentOS7 给 200M
  - swap:虚拟内存，1.5 倍内存大小
    - 工作中:物理内存<8G，SWAP 就 1.5
    - 物理内存>8G，SWAP 就 8G
  - / 根目录，50-200G，放系统相关文件
  - 剩余磁盘空间，保留，由业务需求决定分区

- LVM性能差
- 操作系统自带软RAID不用，性能差、没有冗余，生产环境用硬件raid

----

24. 配置磁盘，`点击“INSTALLATION DESTTINATION”。`

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\199.jpg" alt="img" style="zoom:50%;" />

    选中磁盘后，选择“I will configure partitioning”。选择完毕后，点击“Done”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\200.jpg" alt="img" style="zoom:50%;" />

**分区方式**

25. 在左侧中间的下拉菜单里面，选择“Standard Partition”。然后点击左下方的“+”号，添加/boot 分区。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\201.jpg" alt="img" style="zoom:50%;" />

​	在弹出的对话框中，请按照下图中的内容配置。选择完毕后，点击“Add mount point”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\202.jpg" alt="img" style="zoom:50%;" />

​	再次点击左下角的“+”号，添加 swap 分区。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\203.jpg" alt="img" style="zoom:50%;" />

​	在弹出的对话框中，请按照下图中的内容配置。选择完毕后，点击“Add mount point”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\204.jpg" alt="img" style="zoom:50%;" />

​	再次点击左下角的“+”号，添加根分区。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\205.jpg" alt="img" style="zoom:50%;" />

​	在弹出的对话框中，请按照下图中的内容配置。选择完毕后，点击“Add mount point”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\206.jpg" alt="img" style="zoom:50%;" />

​	/boot、/、swap 三个分区都添加完毕后，检查确认无误，就可以写入磁盘了。确认完毕后，点击“Done”

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\207.jpg" alt="img" style="zoom:50%;" />

**最终结果**

26. 再次确认分区无误后，选择同意更改，即可将分区设置写入磁盘了。选择完毕后，点击“Accept Changes”。

    <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\208.jpg" alt="img" style="zoom:50%;" />

    **提示：**这里采用的是生产环境中集群节点下的节点服务器的分区方式，即系统坏掉后硬盘数据不需要保留。此分区方式也适合大多数生产环境的服务器，如果是数据库以及存储等有重要数据的特殊业务服务，一般会单独分存放数据的分区如 /data。

    除了/boot、swap 和/三个分区外，还可以加/usr、/home、/var 等分区，具体要根据服务器的需求来决 定，一般情况下，只配置这三个分区足够了。

    这种分区方案最大`优点`就是简单，使用方便，可批量安装部署，而且不会存在有的分区满了，有的分区还剩余了很多空间又不能被利用的情况(LVM 的情况这里先不阐述)。

    该分区方案的`缺点`是如果系统坏了，重新装系统时，因为数据都在/(根)分区，导致数据备份很麻 烦，如果设置了/usr、/home、/var 等分区，即使系统出了故障，也可以直接在/(根)分区装系统， 这样并不会破坏其他分区的数据。

    当然，刚才也说了，如果是不存在备份数据的集群节点，那采用 这种分区方案是很明智的，不需要特别担心某个分区暴满的问题。

**设置主机名和ip：**

27. 配置网络，点击“NETWORK$HOSTNAME”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\209.jpg" alt="img" style="zoom:50%;" />

​	**选择完毕后，点击“Begin Installation”。****设置root登录密码**

​	给 root 用户设置密码，点击“ROOT PASSWORD”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\210.jpg" alt="img" style="zoom:50%;" />

​		学习期间为了练习方便，root 用户的密码简单的设置为 123456 即可。但工作环境中，无论什么用 户，密码一定要设置得复杂些，增加安全性。

​		配置完后，点击“Done”。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\211.jpg" alt="img" style="zoom:50%;" />**提示：**如果是生产环境，root 口令要尽量复杂。比如，设置 8 位以上包含数字字母大小写甚至是特殊字符的口令。在企业运维工作中安全是至关重要的一环，安全要从每一件小事做起。

**安装结束重启**

28. 待软件安装完毕后，点击“Reboot”，系统就安装完毕啦。

**系统安装后的基本配置：**

29. 新安装完的 CentOS7.5 系统，登录界面如下图所示。

    需要输入用户及其密码后，方可登录进入系统。

![image-20211009160055734](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009160055734-163376645684763.png)

**安装额外系统开发包：**

由于在安装系统时选择的是最小化安装，所以在系统最小化安装成功以后，有些非常常用的命令默认未被安装，需要自行手动安装，此时可以安装一下。

命令如下:

```
yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap tree dos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc
```

使用 yum 命令安装软件的固定格式

**本章重点回顾：**

- 下载 CentOS 系统 ISO 镜像说明;

- 单台物理服务器安装系统准备;

- 安装 CentOS 7 操作系统过程;

  ◼ 选择系统引导方式 ◼ 进入安装下一步界面 ◼ 安装过程语言选择 ◼ 选择键盘布局 ◼ 选择适合的物理设备 ◼ 初始化硬盘提示 ◼ 初始化主机名及配置网络◼ 系统时钟及时区设置 ◼ 设置超级用户 root 口令

- 磁盘分区类型选择与磁盘分区配置过程;

- 系统无法联网的故障排除方法;

- 配置 vmware 整个网络;

- 额外安装一些有用的软件包。

---

### 2.3 Linux远程连接

在实际的工作场景中，虚拟机界面或者物理服务器本地的终端都是很少接触的，因为服务器装完系统之后，都要拉到IDC机房托管，如果是购买的云主机，那更碰不到服务器本体了，只能通过远程连接的方式管理自己的Linux系统。

因此在装好Linux系统之后，使用的第一步应该是配置好客户端软件**（ssh软件进行连接）**连接Linux系统。

**远程连接软件**

- Xshell
- SecureCRT
- Mobaxterm

**Mobaxterm**

重点介绍这款工具，集万千于一身的全能型终端神器——**MobaXterm**！

先说说这款神器的优点：

> 1. 功能十分强大，支持SSH，FTP，串口，VNC，X server等功能；
>
> 2. 支持标签，切换也十分方便；
> 3. 众多快捷键，操作方便；
> 4. 有丰富的插件，可以进一步增强功能；
> 5. 虽然有收费版，但免费版已经可以让我们装逼装上天……

当然优点、功能远不止这些，更多闪光点期待读者去发掘！

**1. 软件的安装**

直接去官网下载，有免费版下载。当然你要是够土豪买收费版良许也不介意。免费版不仅提供了安装版，还提供了免安装的绿色版，我们甚至可以将它拷到U盘，在家里有公司所有配置都一样。

**2. 创建SSH session**

安装完毕之后界面长这个样。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009175301379.png" alt="image-20211009175301379" style="zoom:50%;" />

**3.创建SSH session**

点击菜单栏 「sessions」 --> 「SSH」，即可弹出 「session setting」 对话框。由上面那一大串的连接方式我们就可以知道Moba的强大之处。

![image-20211009175525256](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009175525256.png)

点击确定后，输入密码(输入密码时并不会显示，只管输入后按确定即可，第一次登陆成功后会提示保存密码，一般选择同意)，就可以连接上虚拟机了。而且边上虚拟机之后，它会自动通过FTP也连接到虚拟机，直接拖拽就可以进行文件复制了。

登陆后界面主要分两块，左边的是主机的文件，右边是终端。勾选左下角的 “Follow terminal folder” 可以让两个的工作路径保持一致。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009175843579.png" alt="image-20211009175843579"  />

**4.个性化设置，设置终端字体，右键复制、文件保存路径等**

![image-20211009180308642](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009180308642.png)

**5.快捷键配置**

![image-20211009181119252](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009181119252.png)

![image-20211009181127400](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009181127400.png)

**6.同时操作多个终端**

![image-20211009181206664](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009181206664.png)

![image-20211009181216606](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009181216606.png)

**7.支持搜到目录及文件**

![image-20211009181314233](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009181314233.png)



---

## 第三章 Linux命令基础

### 3.1 Linux目录结构

**Linux命令格式：**

> 命令    参数    对象/文件/目录  

- 一般情况下，【参数】是可选的，一些情况下【文件或路径】也是可选的
- 参数： 同一个命令，加上不同的参数执行的功能也不同

执行linux命令，添加参数的目的是让命令更加贴切实际工作的需要！

**注：**linux命令，参数之间，普遍应该用一个或多个空格进行分割！

**Linux命令行：**

![image-20211009182411116](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009182411116.png)

```powershell
# 超级用户身份提示符
$ 普通用户身份提示符
```

**操作系统命令分隔符：**

windows平台命令行目录分隔符：

![image-20211009184120031](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009184120031.png)

linux平台命令行目录分隔符：

![image-20211009184207986](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009184207986.png)

**Linux与Windows的目录结构比较：**

Linux首先是建立一个根"/"文件系统，所有的目录也都是由根目录衍生出来。

登录系统后，在当前命令窗口输入命令：

```powershell
ls /
```

结果如图：

![image-20211009185240940](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009185240940.png)

在Linux底下，所有的文件与目录都是由根目录开始，是目录与文件的源头，然后一个个的分支下来，如同树枝状，因此称为这种目录配置为：**目录树**。

目录树的特点：

- 目录树的起始点是根目录(/,root);
- 每一个目录不止能使用本地的文件系统，也可以使用网络上的文件系统，可以利用NFS服务器挂载特定目录。
- 每一个文件在此目录树中的文件名，包含完整路径都是独一无二的。

**图解linux与Windows目录**

Linux与windows区别

- windows特点：E:\学习视频\高清视频\
- Linux目录特点：/etc/hosts /root/data/test.txt

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\248.jpg" alt="img" style="zoom:50%;" />

**Linux** 系统目录结构基本特点：

1. Linux下一切从 <font color="red">**根**</font> 开始

2. Linux下面的目录是一个有层次的目录结构

3. 在linux中每个目录可以挂载到不同的设备(磁盘)上

4. Linux 下设备不挂载不能使用，不挂载的设备相当于没门没窗户的监狱(进不去出不来)，挂载相当于给设备创造了一个入口(挂载点，一般为目录)

**Linux目录挂载：**

**挂载** 通常是将一个<font color="red">存储设备</font>挂接到一个已经存在的目录上，访问这个目录就是访问该存储设备的内容。

对于Linux系统来说，一切接文件，所有文件都放在以`根目录`为起点的树形目录结构中，任何硬件设备也都是文件形式。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\321.jpg" alt="img" style="zoom:50%;" />

如图所示，是U盘存储设备和Linux系统自己的文件系统结构，此时Linux想要使用U盘的硬件设备，必须将Linux **本身的目录**和硬件设备的文件目录合二为一，此过程就称之为**挂载**。

> 挂载操作会隐藏原本Linux目录中的文件，因此选择Linux本身的目录，最好是新建空目录用于挂载
> 挂载之后，这个目录被称为挂载点

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\322.jpg" alt="img" style="zoom:50%;" />

此时U盘文件系统已经是Linux文件系统的一部分，访问/sdb-u文件夹，即是访问访问U盘系统中的文件夹。

---

**Linux目录结构：**

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\251.png" alt="img" style="zoom:50%;" />

- **/dev**：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。

- **/boot：**这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。

- **/lib**：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。

- **/bin**：bin是Binary的缩写, 这个目录存放着最经常使用的命令。

- **/sbin**：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。

- **/usr**：这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。

- **/usr/bin：**系统用户使用的应用程序。

- **/usr/sbin：**超级用户使用的比较高级的管理程序和系统守护程序。

- **/usr/src：**内核源代码默认的放置目录。

- **/var**：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。

- **/mnt**：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。

- **/home**：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。

- **/root**：该目录为系统管理员，也称作超级权限者的用户主目录。

- **/etc：**这个目录用来存放所有的系统管理所需要的配置文件和子目录。

- **/opt**： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。

- **/lost+found**：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。

- **/media**：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。

- **/proc**：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：

  ```
  echo 1 > /proc/sys/net/ipv4/icmp_echo_ignore_all
  ```

- **/selinux**： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。

- **/srv**： 该目录存放一些服务启动之后需要提取的数据。

- **/sys**：这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。

  sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。

- **/tmp**：这个目录是用来存放一些临时文件的。

**在linux系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。**

**/etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。**

**/bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。**

**值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。**

**/var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。**

---

**为什么要学Linux命令：**

- Linux从诞生就是黑屏界面，所有操作倚靠命令完成，如磁盘读写、文件操作、网络管理等
- 企业中，服务器的维护工作都是 `ssh客户端` 完成，没有图形界面
- 程序员想要管理linux服务器，必须学习常用命令

Linux命令学习方法

- 熟能生巧，多敲打，多练习即可
- 不可能一下子掌握所有命令用法，学会使用搜索引擎查阅命令资料

---

### 3.2 Linux基本命令

**Linux文件及目录管理命令**

| 命令         | 对应英文             | 作用                   |
| ------------ | -------------------- | ---------------------- |
| ls           | list                 | 查看文件夹内容         |
| pwd          | print work directory | 查看当前所在目录       |
| cd 目录名    | Change directory     | 切换文件夹             |
| touch 文件名 | touch                | 如果文件不存在，则创建 |
| mkdir 目录名 | Make directory       | 创建目录               |
| rm 文件名    | Remove               | 删除指定文件           |

我们知道切换目录的指令是cd，那么首先得知道如何切换目录。

```powershell
.    当前目录
..    上一层目录
-    前一个工作目录
~    当前【用户】所在的家目录
/            顶级根目录
```

#### 3.2.1 **cd命令**

cd 是 change directory 的缩写，这是用来变换工作目录的命令，注意命令和目录之间有一个空格。

需要注意的是，在所有目录底下都存在两个目录，分别是【.】和【..】，分别代表当前目录，上层目录！那么如何证明它的存在呢？

> 命令： ls -la /
> 查看命令解释：man ls  (Linux下的帮助指令)
> 结论：ls - list directory contens (列出目录内容)
> ls -la /  以竖状格式化显示列出 / 目录所有内容

![image-20211009210615709](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009210615709.png)

#### 3.3.2 **tree命令**

以树形结构显示目录下的内容。

tree命令可能需要单独安装，命令：

```powershell
yum install tree -y
```

> tree命令语法：
>
> tree常用参数：
>
> -C  在文件和目录清单加上色彩，便于区分各种类型。
> -d  显示目录名称而非内容。
> -D  列出文件或目录的更改时间。
> -f   在每个文件或目录之前，显示完整的相对路径名称。
> -F  在条目后加上文件类型的指示符号(* ， /， = ， @ ， | ，其中的一个) 目录/

```powershell
mkdir -p   a/b     c/d
tree
```

#### 3.3.3 **ls命令**

显示目录下内容及属性信息的命令。

> ```powershell
> ls命令语法：
> 
> ls常用参数：
> 
> -a 显示指定目录下所有子目录与文件，包括以.开头的隐藏文件
> -l 以列表方式显示文件的详细信息   ls -l 等于ll 用法
> -h, --human-readable          与-l 一起，以易于阅读的格式输出文件大小
>                              	  (例如 1K 234M 2G)
> -t 根据最后修改时间排序，默认是以文件名排序，通常与-l 连用
> -F 在条目后加上文件类型的指示符号(* ， /， = ， @ ， | ，其中的一个)
>  注:可以标识文件类型
>  加上 * 代表可执行的普通文件
>  加上 = 表示套接字
>  加上 | 表示FIFOS(队列系统)
> 加上 @表示符号链接
> 加上 / 表示文件夹
> 
> -d 显示目录本身的信息 而不是显示目录的内容
> -r, --reverse                 逆序排列
> -S                            根据文件大小排序,从大到小排序
> -i 显示索引节点信息(索引节点相当于身份证号)
> --full-time 以完整的时间格式输出(也就是按照中国的时间日期显示)
> ```

示例：

```powershell
ls -lt 		按照时间进行排序
ls -lrt 	找出最新的文件
ls -d */    列出当前所有目录
ll -hS ./*    显示出当前目录下所有内容详细，且以kb,mb,gb单位从大到小排序
```

![image-20211009211748709](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009211748709.png)

![image-20211009212124186](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009212124186-163378568572770.png)

#### 3.3.4 **mkdir命令**

创建文件夹

> ```powershell
> mkdir命令语法：
> 用法：mkdir [选项]... 目录...
> 若指定目录不存在则创建目录。
> 
> mkdir常用参数：
> -m, --mode=模式    # 设置权限模式(类似chmod)，而不是rwxrwxrwx 减umask
> -p, --parents     # 需要时创建目标目录的上层目录，但即使这些目录已存在也不当作错误处理，递归创建目录
> mkdir # {1..3}加花括号创建连续的目录，用..隔开 花括号内可以是连续的数字、连续的字母mkdir {a..e}
> ```

示例：

```powershell
mkdir {name1,name2,name3}  # 创建三个文件夹，逗号隔开
mkdir aa{1..5}   # 创建连续的目录
mkdir bb1 bb2  # 创建少量连续目录
mkdir -p test1/test2/test3  # 创建递归目录
```

![image-20211009213114704](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009213114704.png)

#### 3.3.5 **touch命令**

创建文件或修改文件时间戳

> ```powershell
> touch命令语法：
> 用法：touch [选项]... 文件...
> 将每个文件的访问时间和修改时间改为当前时间。
> 
> touch常用参数：
> 不存在的文件将会被创建为空文件，除非使用-c 或-h 选项。
> 
> touch {连续数字或字母} # 创建多个文件序列
> touch {1..10}
> touch {a..z}
> 
> -c, --no-create       # 不创建任何文件
> -t STAMP              # 使用[[CC]YY]MMDDhhmm[.ss] 格式的时间替代当前时间
> -r, --reference=文件   # 使用指定文件的时间属性替代当前文件时间
> ```

![image-20211010110233510](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010110233510.png)

```shell
修改文件时间
touch -t 10011010 test1.txt    # 修改test1.txt文件的时间是 10月1号10点10分
touch -r test1.txt test2.txt   # 把test2.txt的时间改成test1.txt 一样
```

![image-20211010110430699](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010110430699.png)

#### 3.3.6 **cp命令**

复制命令

```powershell
cp语法命令：
用法：cp [选项]... [-T] 源文件 目标文件
　或：cp [选项]... 源文件... 目录
　或：cp [选项]... -t 目录 源文件...
将源文件复制至目标文件，或将多个源文件复制至目标目录。

cp常用参数：
-r 递归式复制目录，即复制目录下的所有层级的子目录及文件 -p 复制的时候 保持属性不变
-d 复制的时候保持软连接(快捷方式)
-a 等于-pdr
-p                等于--preserve=模式,所有权,时间戳，复制文件时保持源文件的权限、时间属性
-i, --interactive        覆盖前询问提示
```

示例：

```powershell
语法：复制 > copy > cp

# 1.复制xxx.py到/tmp目录下
cp xxx.py /tmp/

# 2.复制xxx.py顺便改名为chaoge.py
cp xxx.py /tmp/chaoge.py

# Linux下面很多命令，一般没有办法直接处理文件夹,因此需要加上（参数） 
cp -r 递归,复制目录以及目录的子孙后代
cp -p 复制文件，同时保持文件属性不变，可以用 stat
cp -a 相当于-pdr

# 3.递归复制test文件夹，为test2
cp -r test test2

# 4.cp是个好命令，操作文件前，先备份
cp main.py main.py.bak

# 5.移动多个文件，放入文件夹c中
cp -r  文件1  文件2  文件夹a   文件夹c
```

![image-20211010112934869](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010112934869.png)

![image-20211010113133343](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010113133343.png)

取消cp别名的方式

- 使用命令绝对路径
- 命令开头用反斜线 \
- 取消cp命令别名
- 写入环境变量配置文件

```powershell
1.
[root@localhost tmp]# which cp
alias cp='cp -i'
    /usr/bin/cp
[root@localhost tmp]# /usr/bin/cp name1.zip name1.zip.bak

2.
[root@localhost tmp]# \cp name1.zip name1.tar

3.
[root@localhost tmp]# unalias cp
[root@localhost tmp]#
[root@localhost tmp]# cp name1.zip name1.zip.bak

4.
[root@localhost tmp]# vim ~/.bashrc  # 可以注释掉如下配置
# .bashrc

# User specific aliases and functions

alias rm='rm -i'
# alias cp='cp -i'
alias mv='mv -i'
```

#### 3.3.7 **scp命令**

```powershell
# 连接服务器
ssh -p 22 root@192.168.88.128

# 从服务器上下载文件
scp username@servername:/path/filename /var/www/local_dir（本地目录）

# 本地文件到服务器
scp /path/filename username@servername:/path   

# 从服务器下载整个目录
scp -r username@servername:/var/www/remote_dir/（远程目录） /var/www/local_dir（本地目录）

# 上传目录到服务器
scp  -r local_dir username@servername:remote_dir
```

示例：

```powershell
# 把192.168.0.101上的/var/www/test.txt 的文件下载到/var/www/local_dir（本地目录）
scp root@192.168.0.101:/var/www/test.txt

# 把本机/var/www/目录下的test.php文件上传到192.168.0.101这台服务器上的/var/www/目录中
scp /var/www/test.php  root@192.168.0.101:/var/www/

# 从服务器下载/var/www/test目录到 /var/www/目录
scp -r root@192.168.0.101:/var/www/test  /var/www/  

# 把当前目录下的test目录上传到服务器的/var/www/ 目录
scp -r test  root@192.168.0.101:/var/www/
```

#### 3.3.8 **alias命令**

为什么rm命令默认会有一个让用户确认删除的动作呢？

```powershell
# 是因为 rm 的-i参数作用

# 直接输入可以查看当前系统的别名
alias
```

示例：给系统添加一个别名

```powershell
# 当你敲下start就是在执行后面的长串命令，很方便 
alias start="python3  /home/mysite/manager.py runserver  0.0.0.0:8000"
```

#### 3.3.9 **mv命令**

```powershell
# mv语法命令：
mv命令就是move的缩写，作用是移动或是重命名文件
用法：mv [选项]... [-T] 源文件 目标文件
　或：mv [选项]... 源文件... 目录
　或：mv [选项]... -t 目录 源文件...
将源文件重命名为目标文件，或将源文件移动至指定目录。

# mv常用参数：
-f, --force                  # 覆盖前不询问
-i, --interactive            # 覆盖前询问
-n, --no-clobber             # 不覆盖已存在文件如果您指定了-i、-f、-n 中的多个，仅最后一个生效。
-t, --target-directory=DIRECTORY      # 将所有参数指定的源文件或目录移动至 指定目录
-u, --update                  # 只在源文件文件比目标文件新，或目标文件不存在时才进行移动
```

![image-20211010115944683](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010115944683.png)

#### 3.4.0 **rm命令**

Linux在使用rm（删除）、cp（覆盖）、mv（搬家）等命令的时候，必须非常小心，因为这些命令都是“炸弹”，想必大家都听过“删库到跑路”，一言不合“rm -rf /”，假如你真的这么做了，那么。。。上帝保佑你

```powershell
# rm命令语法:
用法：rm [选项]... 文件...
删除 (unlink) 文件。

# rm常用参数：
# rm命令就是remove的含义，删除一个或者多个文件，这是Linux系统重要命令
-f, --force           # 强制删除。忽略不存在的文件，不提示确认
-i                    # 在删除前需要确认
-I                    # 在删除超过三个文件或者递归删除前要求确认。
-d, --dir    		  # 删除空目录
-r, -R, --recursive   # 递归删除目录及其内容
-v, --verbose         # 详细显示进行的步骤
      --help          # 显示此帮助信息并退出
      --version       # 显示版本信息并退出
```

![image-20211010121501619](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010121501619.png)

**炸弹命令**

<font color="red">务必看清楚敲打的命令，是否正确、不得有空格</font>

<font color="red">务必看清楚敲打的命令，是否正确、不得有空格</font>

<font color="red">务必看清楚敲打的命令，是否正确、不得有空格</font>

```powershell
# 1.强制删除且不让用户确认
rm -rf 文件夹

# 2.强制删除且显示过程
[root@pylinux tmp]# rm -rfv ./*
已删除"./456.txt"
已删除目录："./q/w/e/r/t/yt"
已删除目录："./q/w/e/r/t"
已删除目录："./q/w/e/r"
已删除目录："./q/w/e"
已删除目录："./q/w"
已删除目录："./q"
```

![image-20211010121744645](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010121744645.png)

**注意文件恢复**

rm命令删除文件后可以通过如ext3grep工具恢复数据，若是想要粉碎文件，还有其他方式

#### 3.4.1 **help命令**

```powershell
# 语法：
命令  --help

# 帮助命令的精简版
如: ls --help
```

**linux帮助命令**

当你不知道linux命令如何使用的时候，使用man命令帮助你

```powershell
# 命令语法;
man 命令  

# 如：
man  ls  

# 进入man帮助文档后，按下q退出
```

![image-20211010122036216](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010122036216.png)

#### 3.4.2 **echo命令**

echo命令如同python的print一样，能够输出字符串到屏幕给用户看

```powershell
[root@localhost tmp]# name="这是崇拜的你"
[root@localhost tmp]# echo $name
这是崇拜的你
[root@localhost tmp]# echo "正在打印内容"
正在打印内容
```

#### 3.4.3 **info命令**

```powershell
# 语法：
info 命令
```

#### 3.4.4 **Linux开关机命令**

shutdown重启或者关机

重启

```powershell
# 语法：
shutdown -r 参数    -r --reboot    Reboot the machine

shutdown -r 10    # 十分钟后重启
shutdown -r 0     # 立刻重启
shutdown -r now   # 立刻重启
```

关机

```powershell
# 语法：
shutdown -h    --halt  # 停止的含义

shutdown -h 10 		   # 十分钟后关机
shutdown -h 0		   # 立即关机
shutdown -h now 	   # 立即关机
```

halt，poweroff，reboot命令关机与重启

```powershell
reboot    # 重启
poweroff  # 关机
halt      # 关机
```

关机、重启、注销命令列表

| 命令                              | 说明                              |
| --------------------------------- | --------------------------------- |
| shutdown -h now                   | 立刻关机，企业用法                |
| shutdown -h 1                     | 1分钟后关机，也可以写时间如 11:30 |
| halt                              | 立刻关闭系统，需手工切断电源      |
| init 0                            | 切换运行级别为0，0表示关机        |
| poweroff                          | 立刻关闭系统，且关闭电源          |
| <font color="red">重启</font>     |                                   |
| reboot                            | 立刻重启机器，企业用法            |
| Shutdown -r now                   | 立刻重启，企业用法                |
| shutdown -r 1                     | 一分钟后重启                      |
| Init 6                            | 切换运行级别为6，此级别是重启     |
| <font color="red">注销命令</font> |                                   |
| logout                            | 注销退出当前用户                  |
| exit                              | 注销退出当前用户，快捷键ctrl + d  |

#### 3.4.5 **Linux命令行常用快捷键**

```shell
ctrl + c    # cancel取消当前操作
ctrl + l    # 清空屏幕内容
ctrl + d    # 退出当前用户
ctrl + a    # 光标移到行首
ctrl + e    # 光标移到行尾
ctrl + u    # 删除光标到行首的内容
ctrl + k	# 删除当前光标后面的文字
ctrl + w	# 删除光标前面的单词的字符
```

#### 3.4.6 **Linux的环境变量**

同学们应该都会配置windows下的环境变量（PATH），都知道系统会按照PATH的设定，去每个PATH定义的目录下搜索可执行文件。

那么如何查看Linux下的PATH环境变量呢？

```powershell
# 1.为什么系统能够直接找到python解释器？
[root@localhost tmp]# which python  # 输出命令所在的绝对路径
/bin/python

# 2.学习linux的PATH，
[root@localhost tmp]# echo $PATH
/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin

# 注意，PATH的路径，是有先后顺序的，从左往右，读取的
3.如果编译安装了一个python3，装在了 /opt/python36/目录下，怎么添加PATH？

# 这个变量赋值的代码，就是添加python3到环境变量中了
PATH="/opt/python36/bin/:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:"
4.由于上述变量赋值的操作，只是临时生效，我们想永久的更改PATH的值，还得修改/etc/profile 

# 打开文件，在文件末尾，添加PATH值的修改
vim  /etc/profile 
PATH="/opt/python36/bin/:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:"

# 执行命令：
echo $PATH
# echo命令是有打印的意思
# $符号后面跟上PATH,表示输出PATH的变量
```

PATH(一定是大写的)这个变量是由一堆目录组成，分隔符是":"号，而不同于windows的";"号。

![image-20211012113727856](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211012113727856.png)

**修改linux的全局配置文件**

```powershell
# 1.名字叫做 /etc/profile，里面是shell脚本语言

# 2.编辑这个文件，写入你想永久生效的变量和值，系统每次开机都会读取这个文件，让其生效

vim  /etc/profile 
# 写入如下内容
# 这是自定义的变量，每次开机都能够读取了，第一条是设置系统中文的变量
export LC_ALL=zh_CN.UTF-8  #打开注释，系统支持中文
#export LC_ALL=en_US.UTF-8  #打开注释，系统就支持英文了
```

**绝对路径与相对路径**

Linux中非常重要的概念--路径，路径用来定位如何找到某个文件。

Linux下特别注意文件名/路径的写法，可以将所谓的路径(path)定义为绝对路径(absolute)和相对路径(relative)。这两种文件名/路径的写法依据是这样的：

- 绝对路径：由根目录(/)为开始写起的文件名或者目录名称，如/home/oldboy/test.py;
- 相对路径：相对于目前路径的文件名写法。例如./home/oldboy/exam.py或../../home/oldboy/exam.py

简单来说只要开头不是/，就是属于相对路径。

因此你必须了解，相对路径是：**以你当前所在路径的相对路径来表示的。**

例如：你现在在/home 这个目录下，如要进入/var/log这个路径，如何写呢？

```powershell
1. cd /var/log   (绝对路径)
2. cd ../var/log (相对路径)
```

---

### 3.3 Linux系统文件与启动流程

**/etc初始化系统重要文件**

> - /etc/sysconfig/network-scripts/ifcfg-eth0:网卡配置文件
> - /etc/resolv.conf:Linux系统DNS客户端配置文件
> - /etc/hostname (CentOS7) /etc/sysconfig/network:(CentOS 6)主机名配置文件
> - /etc/hosts:系统本地的DNS解析文件
> - /etc/fstab:配置开机设备自动挂载的文件
> - /etc/rc.local:存放开机自启动程序命令的文件
> - /etc/inittab:系统启动设定运行级别等配置的文件
> - /etc/profile及/etc/bashrc:配置系统的环境变量/别名等的文件
> - /etc/profile.d:用户登录后执行的脚本所在的目录
> - /etc/issue和/etc/issue.net:配置在用户登录终端前显示信息的文件
> - /etc/init.d:软件启动程序所在的目录(centos 6)
> - /usr/lib/systemd/system/ 软件启动程序所在的目录(centos 7)
> - /etc/motd:配置用户登录系统之后显示提示内容的文件
> - /etc/redhat-release:声明RedHat版本号和名称信息的文件
> - /etc/sysctl.conf:Linux内核参数设置文件

**/proc重要路径**

> - /proc/meminfo:系统内存信息
>
> - /proc/cpuinfo:关于处理器的信息，如类型，厂家，型号，性能等
> - /proc/loadavg:系统负载信息，uptime 的结果
> - /proc/mounts:已加载的文件系统的列表

**/var目录下文件**

> - /var/log:记录系统及软件运行信息文件所在的目录
>
> - /var/log/messages:系统级别日志文件
>
> - /var/log/secure:用户登录信息日志文件
>
> - /var/log/dmesg:记录硬件信息加载情况的日志文件

**Linux开机启动流程**

作为一个运维人，必须得保障服务器正确工作，机器宕机了，也得明确是什么问题，从何查起，那么了解启动流程就能够对症下药，排查问题。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\275.jpg)

- BIOS自检

  检查硬件是否健康。如：cpu 风扇是否正常，内存是否正常，时钟是否正常，这个过程是读取 ROM 上的指令执行的。

- 微控制器

  系统想要启动必须先加载 BIOS，按下电源键时，给微控制器下达一条复位指令，各寄存器复位，最后下达一条跳转指令，跳转到 BIOS 的 ROM，使得硬件去读取主板上的 BIOS 程序，在这之前都是 由硬件来完成，之后硬件就会把控制权交给 BIOS。

- BIOS->POST

  随后 BIOS 程序加载 CMOS(可读写的 RAM 芯片，保存 BIOS 设置硬件参数的数据)的信息，借 CMOS 取得主机的各项硬件配置。取得硬件配置的信息之后，BIOS 进行加电自检(Power-on self Test，POST)过程,检测计算机各种硬件信息，如果发现硬件错误则会报错(发出声音警告)。之后 BIOS 对硬件进行初始化。BIOS 将自己复制到物理内存中继续执行，开始按顺序搜寻可引导存储设 备，决定存储设备的顺序(即定义第一个可引导的磁盘，当然是在有两个磁盘的前提)，接下来就 会读取磁盘的内容，但是要读取磁盘文件必须要有文件系统，这对 BIOS 挂载文件系统来说是不可 能，因此需要一个不依赖文件系统的方法使得 BIOS 读取磁盘内容，这种方法就是引入 MBR。最后 BIOS 通过 INT13 硬件中断功能读取第一个可引导的存储设备的 MBR(0 磁道 0 扇区)中的 boot loader。将 MBR 加载到物理内存中执行。MBR 载入内存后，BIOS 将控制权转交给 MBR(准确的 说应该是 MBR 中的 boot loader)，然后 MBR 接管任务开始执行。

- MBR引导

  载入了第一个可引导的存储设备的 MBR 后，MBR 中的 boot loader 就要读取所在磁盘的操作系统核 心文件(即后面所说的内核)了。 但是不同操作系统的文件系统格式不同，还有一个磁盘可以安装多个操作系统，如何让 boot loader 做到引导的就是用户想要的操作系统，这么多不同的功能单靠一个 446 字节的 boot loader 是远远不 够的。必须有一个相对应的程序来处理各自对应的操作系统核心文件，这个程序就是操作系统的 loader(注意不是 MBR 中的 boot loader)，这样一来 boot loader 只需要将控制权交给对应操作系统 的 loader，让它负责去启动操作系统就行了。 一个硬盘的每个分区的第一个扇区叫做 boot sector，这个扇区存放的就是操作系统的 loader，所以常 说一个分区只能安装一个操作系统。MBR 的 boot loader 有三个功能:提供选单，读取内核文件，转 交给其它 loader。 提供选单就是给用户提供一张选项单，让用户选择进入哪个操作系统;读取内核文件的意思是，系 统会有一个默认启动的操作系统，这个操作系统的 loader 在所在分区的 boot sector 有一份，除此之 外，也会将这个默认启动的操作系统的 loader 复制一份到 MBR 的 boot loader 中，这样一来 MBR 就 会直接读取 boot loader 中的 loader 了，然后就是启动默认的操作系统;转交给其它的 loader，当用 户选择其它操作系统启动的时候，boot loader 会将控制权转交给对应的 loader，让它负责操作系统的 启动。

- GRUB引导

  grub 是 boot loader 中的一种，就 grub 来说，为了打破在 MBR 中只有 446Bytes 用于存放 boot loader这一限制，所以这一步的实现是这样的:grub 是通过分成三个阶段来实现加载内核这一功能的，这三个阶段分别是:stage1, stage1.5 以及 stage2。 stage1:存放于 MBR 的前 446Bytes，用于加载 stage1.5 阶段，目的是为了识别并驱动 stage2(或者 /boot)所在分区的文件系统。 stage1.5:存放于 MBR 之后的扇区，加载 stage2 所在分区的文件系统驱动，让 stage1 中的 boot loader 能识别 stage2 所在分区的文件系统。 stage2:存放于磁盘分区之上，具体存放于/boot/grub 目录之下，主要用于加载内核文件(vmlinuz- VERSION-RELEASE)以及 ramdisk 这个临时根文件系统(initrd-VERSION-RELEASE.img 或 initramfs- VERSION-RELEASE.img)。 概述:假如要启动的是硬盘设备，首先硬件平台主板 BIOS 必须能够识别硬盘，然后 BIOS 才能加载 硬盘中的 boot loader，而 boot loader 自身加载后就能够直接识别当前主机上的硬盘设备了;不过， 能够识别硬盘设备不代表能够识别硬盘设备中的文件系统，因为文件系统是额外附加的一层软件组 织的文件结构，所以要对接一种文件系统，就必须要有对应的能够识别和理解这种文件系统的驱 动，这种驱动就称为文件系统驱动。而 stage1.5 就是向 grub 提供文件系统驱动的，这样 stage1 就能 访问 stage2 及内核所在的分区(/boot)了。

- 加载内核

  内核(Kerenl)在得到系统控制权之后，首先要进行自身初始化，而初始化的主要作用是: 探测可识别到的所有硬件设备; 加载硬件驱动程序，即加载真正的根文件系统所在设备的驱动程序(有可能会借助于 ramdisk 加载 驱动); 以只读方式挂载根文件系统(如果有借助于 ramdisk 这个临时文件系统(虚根)，则在这一步之后 会执行根切换;否则不执行根切换); 运行用户空间的第一个应用程序:/sbin/init。 到这里内核空间的启动流程就结束了，而接下来是用户空间完成后续的系统启动流程。 注意:ramdisk 和内核是由 boot loader 一同加载到内存当中的，ramdisk 是用于实现系统初始化的、 基于内存的磁盘设备，即加载至内存(的某一段空间)后把内存当磁盘使用，并在内存中作为临时 根文件系统提供给内核使用，帮助内核挂载真正的根文件系统。而之所以能够帮助内核挂载根文件 系统是因为在 ramdisk 这个临时文件系统的/lib/modules 目录下有真正的根文件系统所在设备的驱动 程序;除此之外，这个临时文件系统也遵循 FHS，例如有这些固定目录结构:/bin, /sbin, /lib, /lib64, /etc, /mnt, /media, ... 因为 Linux 内核有一个特性就是通过使用缓冲/缓存来达到加速对磁盘上文件的访问的目的，而 ramdisk 是加载到内存并模拟成磁盘来使用的，所以 Linux 就会为内存中的“磁盘”再使用一层缓冲 /缓存，但是 ramdisk 本来就是内存，它只不过被当成硬盘来使用罢了，这就造成双缓冲/缓存了，而 且不会起到提速效果，甚至影响了访问性能;CentOS 5 系列以及之前版本的 ramdisk 文件为 initrd- VERSION-RELEASE.img，就会出现上述所说到的问题;而为了解决一问题，CentOS 6/7 系列版本就将其改为 initramfs-VERSION-RELEASE.img，使用文件系统的方式就可以避免双缓冲/缓存了，可 以说这是一种提速机制。

- 启动init进程

  grub 中默认指定 init=/sbin/init 程序，可以在 grub.conf 中 kernel 行自定义执行程序 init=/bin/bash,此时 可以绕过下面步骤直接进入 bash 界面。 内核源代码文件中显示 996 行左右，规定了 init 启动的顺序，/sbin/init->/etc/init->/bin/init->/bin/sh。

- 读取/etc/inittab 文件

  inittab 文件里面定义了系统默认运行级别，这一步做了一些工作如下: 初始运行级别(RUN LEVEL); 系统初始化脚本; 对应运行级别的脚本目录; 定义 UPS 电源终端/恢复脚本; 在虚拟控制台生成 getty,以生成终端; 在运行级别 5 初始化 X。

- 执行/etc/rc.d/rc.sysinit 程序

  系统初始化一些脚本，主要完成以下工作。 设置主机名; 设置欢迎信息;

  激活 udev 和 selinux 可以在 grub.conf 中,kernel 行添加 selinux=0 以关闭 selinux; 挂载/etc/fstab 文件中定义的文件系统; 检测根文件系统，并以读写方式重新挂载根文件系统; 设置系统时钟;

  激活 swap 设备; 根据/etc/sysctl.conf 文件设置内核参数; 激活 lvm 及 software raid 设备; 加载额外设备的驱动程序; 清理操作。 /etc/rc*.d/文件(各种服务) 里面定义的是各种服务的启动脚本，可以 ls 查看，S 开头代表开机启动的服务，K 开头的是关机要 执行的任务。#代表数字，一个数字代表一个运行级别，共 7 个运行级别。 /etc/rc.d/rc.local 文件 这里面可以自定义开机启动的命令。

- 执行/bin/login

  执行/bin/login 程序，等待用户登录。

**centos7启动流程**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\276.jpg)

CentOS7 和 CentOS6 启动流程差不多，只不过到 init 程序时候，改为了 systemd，因此详细解释一下 systemd 后的启动流程。

- uefi或BIOS初始化，开始post开机自检;
- 加载mbr到内存
- 加载内核和inintamfs模块
- 内核开始初始化，使用systemd代替centos6的init程序

1. 执行initrd.target，包括挂载/etc/fstab文件中的系统，此时挂载后，就可以切换到根目录了

2. 从initramfs根文件系统切换到磁盘根目录

3. systemd执行默认target配置

CentOS7 系表面是有“运行级别”这个概念，实际上是为了兼容以前的系统，每个所谓的“运行级 别”都有对应的软连接指向，默认的启动级别是/etc/systemd/system/default.target，根据它的指向可 以找到系统要进入哪个模式。

centos7的7个启动模式是：

> - 0 ==> runlevel0.target, poweroff.target
> - 1 ==> runlevel1.target, rescue.target
> - 2 ==> runlevel2.target, multi-user.target
> - 3 ==> runlevel3.target, multi-user.target
> - 4 ==> runlevel4.target, multi-user.target
> - 5 ==> runlevel5.target, graphical.target
> - 6 ==> runlevel6.target, reboot.target
> - systemd执行sysinit.target;
> - systemd启动multi-user.target下的本机与服务器服务;
> - systemd执行multi-user.target下的/etc/rc.d/rc.local。
> - Systemd 执行 multi-user.target 下的 getty.target 及登录服务;
> - systemd 执行 graphical 需要的服务。

---

## 第四章 Linux文件目录管理命令

### 4.1 vim的用法

**vim与程序员**

所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。

但是目前我们使用比较多的是 vim 编辑器。

vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。

**什么是vim**

Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。

简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。

**vi/vim 的使用**

基本上 vi/vim 共分为三种模式，分别是**命令模式（Command mode）**，**输入模式（Insert mode）**和**底线命令模式（Last line mode）**。 

**vim工作模式**

命令模式：进入 vim 默认的模式

编辑模式：按 a/ i / o 进入输入模式

底行模式：按下:(冒号)之后进入到的模式

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\282.png" alt="img"  />

**vim基础用法**

- vim text1.txt #打开文件
- 打开后无法直接编辑，需要按 i 进入编辑模式
- 修改这个文件内容吧
- 修改完后，按 esc 退出编辑模式:wq
- 保存退出 #注意":"必须是英文符号

```powershell
:wq  # 保存并退出
:q!  # 强制退出不保存
:wq! # 强制保存退出
```

**命令模式**

用户刚刚启动 vi/vim，便进入了命令模式。

此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\283.jpg)

**移动光标**

```shell
w(e)　　# 移动光标到下一个单词
b　　　　# 移动到光标上一个单词

数字0　　# 移动到本行开头
$　　　　# 移动光标到本行结尾

H　　　　# 移动光标到屏幕首行
M　　　　# 移动到光标到屏幕的中间一行
L　　　　# 移动光标到屏幕的尾行
gg　　　 # 移动光标到文档的首行
j　　　　# 移动光标到文档尾行

ctrl + f　　# 下一页
ctrl + b　　# 上一页

`.　　# 移动光标到上一次的修改行
```

**查找**

```shell
/chaoge     # 在整篇文档中搜索chaoge字符串,向下查找
?chaoge     # 在整篇文档中搜索chaoge字符串,向上查找

*        	# 查找整个文档，匹配光标所在的所有单词,按下n查找下一处,N上一处
#        	# 查找整个文档，匹配光标所在的所有单词,按下n查找下一处,N上一处

gd       	# 找到光标所在单词匹配的单词，并停留在非注释的第一个匹配上

%　　　　	  # 找到括号的另一半！！
```

**复制、删除、粘贴**

```shell
yy    # 拷贝光标所在行
dd    # 删除光标所在行
D     # 删除当前光标到行尾的内容
dG    # 删除当前行到文档尾部的内容
p     # 粘贴yy所复制的内容
x　　  # 向前删除字符
X	  # 向后删除字符
u     # 撤销上一步的操作
ctrl + r	# 回到上一步的操作
.     # 重复前一个执行过的动作
```

**数字与命令**

```shell
3yy　　　　# 拷贝光标所在的3行
5dd　　　　# 删除光标所在5行
```

**快捷操作**

```shell
C(大写字母)  # 删除光标所在位置到行尾的内容并进入编辑模式 
i o a       # 在命令模式下按下字母i o a ，即可进入输入模式，可以编写代码啦。。。
o(小写字母)  # 在当前行下面插入一行并进入编辑模式 
O(大写字母)  # 在当前行上面插入一行并进入编辑模式 
 A         # 快速到达行尾并进入编辑模式
ZZ         # 快速保存并退出 
```

**批量快捷操作**

```shell
# 批量删除:

ctrl+v   	# 进入批量编辑模式(可视块)
上下左右  	  #  批量选择
d 		    # 删除 

# 批量增加:
ctrl+v   	# 进入批量编辑模式(可视块)
选择区域
输入大写的 I  # 进入编辑模式 编辑
按下ESC键

# 批量去掉注释：
# 1. 进入命令行模式，按ctrl + v进入 visual block模式，按字母l横向选中列的个数，例如 // 需要选中2列
# 2. 按字母j，或者k选中注释符号
# 3. 按d键就可全部取消注释
```

vim批量缩进

```shell
:set tabstop=4 # 设定tab宽度为4个字符
:set shiftwidth=4 设# 定自动缩进为4个字符
:set expandtab # 用space替代tab的输入
:set noexpandtab # 不用space替代tab的输入

# 1.命令模式，按下v，进入可视模式
# 2.光标移动选择行，输入 > 大于号，缩进，输入< 缩进

# 输入行号缩进：
1.显示行号
:set nu        # 显示
:set nonu    # 关闭

# 2.行号缩进
:10,20 >    #10到20行，缩进一次
```

**文本替换**

注意：这里的from和to都可以是任何字符串，其中from还可以是正则表达式

```shell
# 1.替换当前行中的内容	:s/from/to/    （s即substitude）
	:s/from/to/		# 将当前行中的第一个from，替换成to。如果当前行含有多个from，则只会替换其中的第一个。
	:s/from/to/g	# 将当前行中的所有from都替换成to。
	:s/from/to/gc	# 将当前行中的所有from都替换成to，但是每一次替换之前都会询问请求用户确认此操作。
    
# 2.替换某一行的内容：	:33s/from/to/g
	:.s/from/to/g	# 在当前行进行替换操作。
	:33s/from/to/g  # 在第33行进行替换操作。
	:$s/from/to/g	# 在最后一行进行替换操作。
	
# 3.替换某些行的内容：	:10,20s/from/to/g
	:10,20s/from/to/g	# 对第10行到第20行的内容进行替换。
	:1,$s/from/to/g  	# 对第一行到最后一行的内容进行替换（即全部文本）。
	:1,.s/from/to/g  	# 对第一行到当前行的内容进行替换。
	:.,$s/from/to/g  	# 对当前行到最后一行的内容进行替换。
	:'a,'bs/from/to/g	# 对标记a和b之间的行（含a和b所在的行）进行替换，其中a和b是之前用m命令所做的标记。
	
# 4.替换所有行的内容：	:%s/from/to/g
	:%s/from/to/g	# 对所有行的内容进行替换。
	
# 5.替换命令的完整形式	# :[range]s/from/to/[flags]
	# 5.1 s/from/to/	# 把from指定的字符串替换成to指定的字符串，from可以是正则表达式。
	# 5.2 [range]
		# 有以下一些表示方法：
		不写range	   # 默认为光标所在的行。
		.       	# 光标所在的行。
		1       	# 第一行。
		$       	# 最后一行。
		33      	# 第33行。
		'a      	# 标记a所在的行（之前要使用ma做过标记）。
		.+1     	# 当前光标所在行的下面一行。
		$-1     	# 倒数第二行。（这里说明我们可以对某一行加减某个数值来取得相对的行）。
		22,33   	# 第22～33行。
		1,$     	# 第1行 到 最后一行。
		1,.     	# 第1行 到 当前行。
		.,$     	# 当前行 到 最后一行。
		'a,'b   	# 标记a所在的行 到 标记b所在的行。
		%       	# 所有行（与 1,$ 等价）。
		?chapter	# 从当前位置向上搜索，找到的第一个chapter所在的行。（其中chapter可以是任何字符串或者正则表达式。
		/chapter	# 从当前位置向下搜索，找到的第一个chapter所在的行。（其中chapter可以是任何字符串或者正则表达式。
		
		# 注意，上面的所有用于range的表示方法都可以通过 +、- 操作来设置相对偏移量。

    # 5.3 [flags]
    	# 这里可用的flags有：

        无	# 只对指定范围内的第一个匹配项进行替换。
        g	# 对指定范围内的所有匹配项进行替换。
        c	# 在替换前请求用户确认。
        e	# 忽略执行过程中的错误。

        # 注意：上面的所有flags都可以组合起来使用，比如 gc 表示对指定范围内的所有匹配项进行替换，并且在每一次替换之前都会请用户确认。
```

**底线命令模式**

在命令模式下输入冒号（英文的:），就进入了底线命令模式，在底线命令模式下可以输入单个或多个字符的命令，常用命令有：

```shell
:q!     # 强制退出
:wq!    # 强制写入退出
:set nu # 显示行号
:数字　　# 调到数字那行
:set nonu # 取消显示行号

# 随时按下esc可以退出底线命令模式
```

**重定向符号**

| 符号                                 | 解释                        |
| ------------------------------------ | --------------------------- |
| 重定向的意思是，”将数据传到其他地方“ |                             |
| < 或者<<                             | 标准输入stdin，代码为0      |
| >或>>                                | 标准输出stdout，代码为1     |
| 2>或2>>                              | 标准错误输出stderr，代码为2 |

**特殊符号**

| 符号 | 解释                                                      |        |                     |      |                                     |
| ---- | --------------------------------------------------------- | ------ | ------------------- | ---- | ----------------------------------- |
| *    | 匹配任意个字符                                            |        |                     |      |                                     |
| ?    | 匹配一个字符                                              |        |                     |      |                                     |
| \|   |                                                           | 管道符 |                     |      |                                     |
| &    | 后台进程符                                                |        |                     |      |                                     |
| &&   | 逻辑与符号，命令1 && 命令2 ，当命令1执行成功继续执行命令2 |        |                     |      |                                     |
| \    | \                                                         |        | 逻辑或符号，命令1 \ | \    | 命令2，当命令1执行失败才会执行命令2 |
| #    | 注释符                                                    |        |                     |      |                                     |
| " "  | 双引号表示字符串，能够识别，``反引号，$符，\ 转义符       |        |                     |      |                                     |
| ' '  | 单引号表示普通字符串，无特殊含义                          |        |                     |      |                                     |
| $    | 变量符 如 $name                                           |        |                     |      |                                     |
| \    | 转义字符                                                  |        |                     |      |                                     |

---

### 4.2 查看文件命令

#### 4.2.1 **cat命令**

cat命令用于查看纯文本文件（常用于内容较少的）， 可以理解为是`猫`，瞄一眼文件内容，其单词是 `concatenate`，指的是可以连接多个文件且打印到屏幕，或是重定向到文件中。

cat功能

| 功能                   | 说明                                                        |
| ---------------------- | ----------------------------------------------------------- |
| 查看文件内容           | cat file.txt                                                |
| 多个文件合并           | cat file.txt file2.txt > file3.tx                           |
| 非交互式编辑或追加内容 | cat >> file.txt << EOF 把什么内容写入到文件内 EOF           |
| 清空文件内容           | cat /dev/null > file.txt 【/dev/null是linux系统的黑洞文件】 |

```powershell
# cat命令语法：
用法：cat [选项] [文件]...
将[文件]或标准输入组合输出到标准输出。

清空文件内容,慎用
> 文件名

# 常用参数：
-A, --show-all           等价于 -vET
-b, --number-nonblank    对非空输出行编号
-e                       等价于 -vE
-E, --show-ends          在每行结束处显示 $
-n, --number             对输出的所有行编号
-s, --squeeze-blank      不输出多行空行
-t                       与 -vT 等价
-T, --show-tabs          将跳格字符显示为 ^I
-u                       (被忽略)
-v, --show-nonprinting   使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外
--help     显示此帮助信息并退出
--version  输出版本信息并退出

如果[文件]缺省，或者[文件]为 - ，则读取标准输入。
```

```shell
# 猫,查看文件
[root@localhost tmp]# cat file.txt

# 在每一行的结尾加上$符
[root@localhost tmp]# cat -E file.txt

# 追加文字到文件
[root@localhost tmp]# cat >> file.txt << EOF
> 唧唧复唧唧
> 木兰开飞机
> 开的什么机
> 波音747
> EOF


# 输出非空内容的编号
[root@localhost tmp]# cat -b file.txt
     1  我要写c入内容
     2  实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
     3  唧唧复唧唧
     4  木兰开飞机

     5  开的什么机

     6  波音747

# 输出所有内容的编号
[root@localhost tmp]# cat -n file.txt
     1  我要写c入内容
     2  实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
     3  唧唧复唧唧
     4  木兰开飞机
     5
     6  开的什么机
     7
     8  波音747


# -s参数把多个空行，换成一个，可以让文件更精炼阅读
[root@localhost tmp]# cat -s file.txt
我要写c入内容
实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
唧唧复唧唧
木兰开飞机

开的什么机

波音747


# cat写入内容，用ctrl+c 结束，一般不用
[root@localhost tmp]# cat > file.txt       

# 查看服务器核数
cat /proc/cpuinfo | grep "flags" | wc -l
```

#### 4.2.2 **tac命令**

与cat命令作用相反，反向读取文件内容

```shell
[root@localhost tmp]# cat file.txt
我要写c入内容
实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
唧唧复唧唧
木兰开飞机

开的什么机

波音747
[root@localhost tmp]#
[root@localhost tmp]#
[root@localhost tmp]# tac file.txt
波音747

开的什么机

木兰开飞机
唧唧复唧唧
实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
我要写c入内容
```

#### 4.2.3 **grep命令**

grep是linux强大的三剑客之一，从文本中，过滤有用信息的命令

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\291.png)

```powershell
# 1.语法
grep "你想过滤的字符串"   需要过滤的文件  #用法一

# 2.过滤文件中的相关内容
# 找出文件中含有 "大" 字的行，且显示此内容，在哪一行
grep -n "大"   test.txt   # -n 参数是显示行号

# 忽略大小写，找出ALLOW_HOSTS=[]参数是否被修改
grep -i "al"  test.txt  # -i 忽略大小写

# 过滤出此文件非空白行，如何操作？
# 拆解如下，找出所有的空白行 
[root@localhost ~]# grep "^$"   test.txt   #  "^$"  以空开头，以空结尾，因此是空白行
# -v 参数是反转过滤结果 ，找出 空白行以外的内容
[root@localhost ~]# grep -v   "^$"   test.txt

# 过滤掉注释行，和空白行 ，如何操作？
[root@localhost ~]# grep -v  "^#"   test.txt | grep -v "^$"
```

```powershell
# Linux提供的管道符“|”将两条命令隔开，管道符左边命令的输出会作为管道符右边命令的输入。
常见用法：

# 检查python程序是否启动
ps -ef|grep "python"

# 找到/tmp目录下所有txt文件
ls /tmp|grep '.txt'

# 检查nginx的端口是否存活
netstat -tunlp |grep nginx

# 命令格式 cat  文件   |   grep   "你想要的内容" 
# 1.找出linux的用户信息文件，中 有关joker的行信息  /etc/passwd
[root@localhost ~]# cat /etc/passwd  |  grep  "joker"
```

#### 4.2.4 **more命令**

More是一个过滤器, 用于分页显示 (一次一屏) 文本，以当前屏幕窗口尺寸为准。

```powershell
# 命令语法
more 参数  文件

-num # 指定屏幕显示大小为num行
+num # 从num行开始显示

# 交互式more的命令：
空格     # 向下滚动一屏
Enter	# 向下显示一行
=    	# 显示当前行
```

示例：

```powershell
# 显示5行内容
[root@localhost tmp]# more -5 alex.txt
root:x:0:0:root:/root:/bin/bash
root:x:0:0:root:/root:/bin/bash
root:x:0:0:root:/root:/bin/bash
root:x:0:0:root:/root:/bin/bash
root:x:0:0:root:/root:/bin/bash
--More--(0%)

# 从6行开始输出内容到屏幕
more +6 test.txt


#将显示结果分页输出，需控制窗口大小
[root@localhost tmp]# netstat -tunlp |more -3
```

#### 4.2.5 **less命令**

less命令是more的反义词

```powershell
# less命令语法：
less [选项]... [文件]...

# 常用参数：
-N  # 显示每行编号
-e  # 到文件结尾自动退出，否则得手动输入q退出

子命令

# 整个的翻页
b    # 向前一页
f 	 # 向后一页

空格  # 查看下一行，等于 ↓
y    # 查看上一行，等于↑

q	 # 退出
```

#### 4.2.6 **head命令**

用于显示文件内容头部，默认显示开头10行。

```shell
# head命令语法：
head [选项]... [文件]...

# 将每个指定文件的头10 行显示到标准输出。
# 如果指定了多于一个文件，在每一段输出前会给出文件名作为文件头。
# 如果不指定文件，或者文件为"-"，则从标准输入读取数据。

# 常用参数：
  -c,  --bytes=[-]K     显示每个文件的前K 字节内容；
                        如果附加"-"参数，则除了每个文件的最后K字节数据外
                        显示剩余全部内容
  -n, --lines=[-]K      显示每个文件的前K 行内容；
                        如果附加"-"参数，则除了每个文件的最后K 行外显示
                        剩余全部内容
  -q, --quiet, --silent 不显示包含给定文件名的文件头
  -v, --verbose         总是显示包含给定文件名的文件头
      --help            显示此帮助信息并退出
      --version         显示版本信息并退出
```

示例：

```shell
# 显示前5行内容
[root@localhost tmp]# head -5 file.txt
我要写c入内容
实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
唧唧复唧唧
木兰开飞机

# 显示前6个字节内容
[root@localhost tmp]# head -c 6 file.txt
我要[root@localhost tmp]#

# 显示对个文件的内容
[root@localhost tmp]# head joker.txt ike.txt
==> joker.txt <==
joker

==> ike.txt <==
ike
```

#### 4.2.7 **tail命令**

显示文件内容的末尾，默认输出后10行。

```powershell
# tail命令语法：
tail [选项]... [文件]...

# 常用参数：
-c 数字              # 指定显示的字节数
-n 行数              # 显示指定的行数
-f                  # 实时刷新文件变化
-F 等于 -f --retry   # 断打开文件，与-f合用
--pid=进程号         # 进程结束后自动退出tail命令
-s 秒数              # 检测文件变化的间隔秒数
```

示例：

```powershell
# 显示文件后10行
[root@localhost tmp]# tail file.txt

# 显示文件后5行
[root@localhost tmp]# tail -5 file.txt

# 从文件第3行开始显示文件
[root@localhost tmp]# tail -n 3 file.txt

# 检测文件变化
[root@localhost tmp]# tail -f file.txt

# 参数F和f的区别
[root@localhost tmp]# tail -f aa.txt  # 检测文件不存在直接退出
tail: cannot open ‘aa.txt’ for reading: No such file or directory
tail: no files remaining
[root@localhost tmp]#
[root@localhost tmp]#
[root@localhost tmp]# tail -F aa.txt  #  检测文件不存在，等待文件生成
tail: cannot open ‘aa.txt’ for reading: No such file or directory

```

#### 4.2.8 **cut命令**

cut - 在文件的每一行中提取片段，在每个文件FILE的各行中, 把提取的片段显示在标准输出。

```powershell
# cut命令语法：
cut [选项]... [文件]...

# 常用参数：
-b        # 以字节为单位分割
-n        # 取消分割多字节字符，与-b一起用
-c        # 以字符为单位
-d        # 自定义分隔符，默认以tab为分隔符
-f        # 与-d一起使用，指定显示哪个区域
N         # 第 N 个 字节, 字符 或 字段, 从 1 计数 起 
N-        # 从 第 N 个 字节, 字符 或 字段 直至 行尾 
N-M       # 从 第 N 到 第 M (并包括 第M) 个 字节, 字符 或 字段 
-M        # 从 第 1 到 第 M (并包括 第M) 个 字节, 字符 或 字段
```

示例：字节作为分隔符 -b参数

```powershell
# 切割出第5个字节
[root@localhost tmp]# cut -b 5 file.txt

# 切割出第5到第10的字节
[root@localhost tmp]# cut -b 5-10 file.txt

# 切割出第5和第10的字节
[root@localhost tmp]# cut -b 5,10 file.txt

# 切割出第一到第5的字节
[root@localhost tmp]# cut -b -5 file.txt

# 切割出第5到最后的字节
[root@localhost tmp]# cut -b 5- file.txt
```

示例：以字符作为分隔符 -c 参数，区别在于中英文

```powershell
# 切割出第5个字符
[root@localhost tmp]# cut -c 5 file.txt

# 切割出第5到第10的字符
[root@localhost tmp]# cut -c 5-10 file.txt

# 切割出第5和第10的字符
[root@localhost tmp]# cut -c 5,10 file.txt

# 切割出第一到第5的字符
[root@localhost tmp]# cut -c -5 file.txt

# 切割出第5到最后的字符
[root@localhost tmp]# cut -c 5- file.txt
```

示例：自定义分隔符

```shell
# 以冒号切割，显示第七区域信息
[root@localhost tmp]# cut -f 10 -d : file.txt | head -5
我要写c入内容
实现中华民族伟大复兴是全体中华儿女的共同光荣，也是全体中华儿女的共同使命。从历史中汲取经验和智慧，海内外全体中华儿女更加紧密地团结起来，中华民族必将以不可阻挡的步伐迈向伟大复兴。
唧唧复唧唧
木兰开飞机
```

#### 4.2.9 **sort命令**

sort命令将输入的文件内容按照规则排序，然后输出结果。

```powershell
# sort命令语法：
sort [选项]... [文件]...
或：sort [选项]... --files0-from=F

# 串联排序所有指定文件并将结果写到标准输出。

# 常用参数：
 -b, --ignore-leading-blanks   # 忽略前导的空白区域
 -n, --numeric-sort            # 根据字符串数值比较
 -r, --reverse                 # 逆序输出排序结果
 -u, --unique          		   # 配合-c，严格校验排序；不配合-c，则只输出一次排序结果
 -t, --field-separator=分隔符   # 使用指定的分隔符代替非空格到空格的转换
 -k, --key=位置1[,位置2]        # 在位置1 开始一个key，在位置2 终止(默认为行尾)
```

```powershell
# sort 是默认以第一个数据来排序，而且默认是以字符串形式来排序,所以由字母 a 开始升序排序

# 按照数字从大到小排序
[root@localhost tmp]# sort -n file.txt

# 降序排序
[root@localhost tmp]# sort -nr file.txt

# 去重排序
[root@localhost tmp]# sort -u file.txt

# 指定分隔符，指定序列
sort -t " " -k 2 file.txt

# 以顿号分割，对第三列排序，以第一位数字排序
[root@localhost tmp]# cat file.txt | sort -t "、" -k 3

# 以顿号分割，对第一个区域的第2到3个字符排序
[root@localhost tmp]# cat file.txt | sort -t "、" -k 1.2,1.3
```

#### 4.3.0 **uniq命令**

uniq命令可以输出或者忽略文件中的重复行，常与sort排序结合使用。

```powershell
# uniq命令用法：
uniq [选项]... [文件]

# 从输入文件或者标准输入中筛选相邻的匹配行并写入到输出文件或标准输出。
# 不附加任何选项时匹配行将在首次出现处被合并。

# 常用参数：
-c, --count           # 在每行前加上表示相应行目出现次数的前缀编号
-d, --repeated        # 只输出重复的行
-u, --unique          # 只显示出现过一次的行,注意了，uniq的只出现过一次，是针对-c统计之后的结果
```

示例：

```powershell
# 测试文件
[root@localhost tmp]# cat efo.txt

10.0.0.1
10.0.0.1
10.0.0.51
10.0.0.51
10.0.0.1
10.0.0.1
10.0.0.51
10.0.0.31
10.0.0.21
10.0.0.2
10.0.0.12
10.0.0.2
10.0.0.5
10.0.0.5
10.0.0.5
10.0.0.5

# 仅仅在首次出现的时候合并，最好是排序后去重
[root@localhost tmp]# uniq efo.txt

10.0.0.1
10.0.0.51
10.0.0.1
10.0.0.51
10.0.0.31
10.0.0.21
10.0.0.2
10.0.0.12
10.0.0.2
10.0.0.5

# 排序后去重且显示重复次数
[root@localhost tmp]# sort efo.txt | uniq -c
      1
      4 10.0.0.1
      1 10.0.0.12
      2 10.0.0.2
      1 10.0.0.21
      1 10.0.0.31
      4 10.0.0.5
      3 10.0.0.51

# 找出重复的行，且计算重复次数
[root@localhost tmp]# sort efo.txt | uniq -dc
      4 10.0.0.1
      2 10.0.0.2
      4 10.0.0.5
      3 10.0.0.51

# 找到只出现一次的行
[root@localhost tmp]# sort efo.txt | uniq -uc
      1
      1 10.0.0.12
      1 10.0.0.21
      1 10.0.0.31
```

#### 4.3.1 **du命令**

```powershell
# du命令语法;
du 【参数】【文件或目录】

# 常用参数：
-s 显示总计
-h 以k，M,j为单位显示，可读性强
```

示例：

```powershell
# 统计/var/log/文件夹大小 
du -sh  /var/log/  

# 显示当前目录下 所有文件的大小
[root@s25linux tmp]# du  -h ./*
```

#### 4.3.2 **top命令**

windows的任务管理器见过吧

能够显示 动态的进程信息

cpu、内存，网络，磁盘io等使用情况 ，也就是一个资源管理器

那么linux的资源管理器 就是top命令

```shell
# 第一行 (uptime)
系统时间 主机运行时间 用户连接数(who) 系统1，5，15分钟的平均负载

# 第二行:进程信息
进程总数 正在运行的进程数 睡眠的进程数 停止的进程数 僵尸进程数

# 第三行:cpu信息
1.5 us：用户空间所占CPU百分比
0.9 sy：内核空间占用CPU百分比
0.0 ni：用户进程空间内改变过优先级的进程占用CPU百分比
97.5 id：空闲CPU百分比
0.2 wa：等待输入输出的CPU时间百分比
0.0 hi：硬件CPU中断占用百分比
0.0 si：软中断占用百分比
0.0 st：虚拟机占用百分比

# 第四行：内存信息（与第五行的信息类似与free命令）
total：物理内存总量
used：已使用的内存总量
free：空闲的内存总量（free+used=total）
buffers：用作内核缓存的内存量

# 第五行：swap信息
total：交换分区总量
used：已使用的交换分区总量
free：空闲交换区总量
cached Mem：缓冲的交换区总量，内存中的内容被换出到交换区，然后又被换入到内存，但是使用过的交换区没有被覆盖，交换区的这些内容已存在于内存中的交换区的大小，相应的内存再次被换出时可不必再对交换区写入。
```

#### 4.3.3 **ps命令**

用于查看linux进程信息的命令 

```powershell
# ps命令语法
ps  -ef    # -ef，是一个组合参数，-e  -f 的缩写，默认显示linux所有的进程信息，以及pid，时间，进程名等信息 

# 过滤系统有关vim的进程 
[root@localhost ~]# ps -ef |  grep  "vim"
root      24277   7379  0 16:09 pts/1    00:00:00 vim ps 是怎么用的.txt
```

```shell
1.一个django运行后，如何验证django是否运行了，它会产生些什么内容？
能够产生日志，检测到用户的请求，说明django运行了
查看端口情况，django会占用一个端口 
产生一个python相关的进程信息 
```

#### 4.3.4 **kill命令**

杀死进程的命令

```powershell
# 杀进程
kill   进程的id号

# 如果遇见卡死的进程，杀不掉，就发送 -9  强制的信号 
kill -9  pid
```

pkill 命令

pkill 用于杀死一个进程，与不同的是它会杀死指定名字的所有进程，类似于命令。

kill 命令杀死指定进程 PID，需要配合 ps 使用，而 pkill 直接对进程对名字进行操作，更加方便。

```powershell
# pkill -9  php-fpm      # 结束所有的 php-fpm 进程
```

#### 4.3.5 **netstat命令**

查看linux的网络端口情况 

```powershell
# 安装netsta命令
yum -y install net-tools

# netstat命令语法与参数：
常用的参数组合  -t -n -u -l -p    

#显示机器所有的tcp、udp的所有端口连接情况
[root@localhost ~]# netstat -tunlp   

# 例如验证服务器80端口是否存在
netstat  -tunlp  |  grep  80

# 过滤3306端口是否存在 
netstat -tunlp |grep 3306


# 过滤ssh服务是否正常
[root@s25linux tmp]# netstat -tunlp  |   grep   ssh
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1147/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1147/sshd




# 有些公司为了保护服务器安全，更改了默认的远程连接端口
# ssh端口 26674    ip 是  123.206.16.61   账号是 xiaohu   密码是 xiaohu666

# 我怎么登陆服务器呢？用如下的命令去连接服务器 
ssh  -p  26674    xiaohu@123.206.16.61  

ssh  -p 22 root@192.168.178.134
root@192.168.178.134's password:
```

#### 4.3.6 **wc命令**

wc命令用于统计文件的行数、单词、字节数。

```powershell
# wc命令语法：
wc [选项]... [文件]

# 常用参数：
-c, --bytes打印字节数
-m, --chars  打印字符数 
-l, --lines  打印行数 
-L, --max-line-length  打印最长行的长度
-w, --words 打印单词数
```

示例：

```powershell
# 统计文件的行数、单词、字节数
[root@localhost tmp]# wc efo.txt
 17  16 151 efo.txt
 
# 统计文件的行数
[root@localhost tmp]# wc -l efo.txt
17 efo.txt

# 统计文件的单词数
[root@localhost tmp]# wc -w efo.txt
16 efo.txt

# 统计文件的字节数 
[root@localhost tmp]# wc -m efo.txt
151 efo.txt

# 统计文件的字节数
[root@localhost tmp]# wc -c efo.txt
151 efo.txt

# 统计文件最长行的长度
[root@localhost tmp]# wc -L efo.txt
9 efo.txt

# 统计单词数量，以空格区分
[root@localhost tmp]# echo "ike joker mac windows linux python" | wc -w
6

# 证明结尾有个$
[root@localhost tmp]# echo "ike joker mac windows linux python" | wc -m
35
[root@localhost tmp]# echo "ike joker mac windows linux python" | cat -E
ike joker mac windows linux python$

# 统计有几个用户登录
[root@localhost tmp]# who | wc -l
2
```

#### 4.3.7 **tr命令**

tr命令从标准输入中替换、缩减或删除字符，将结果写入到标准输出。

```powershell
# tr命令用法：
tr [选项]... SET1 [SET2]
# 从标准输入中替换、缩减和/或删除字符，并将结果写到标准输出。
# 字符集1：指定要转换或删除的原字符集。
# 当执行转换操作时，必须使用参数“字符集2”指定转换的目标字符集。
# 但执行删除操作时，不需要参数“字符集2”；
# 字符集2：指定要转换成的目标字符集。

# 常用参数：
-c或——complerment： 	 # 取代所有不属于第一字符集的字符；
-d或——delete：	  	 # 删除所有属于第一字符集的字符；
-s或--squeeze-repeats：# 把连续重复的字符以单独一个字符表示；
-t或--truncate-set1：  # 先删除第一字符集较第二字符集多出的字符。
```

示例：

```powershell
# 将输入的字符变为大写
[root@localhost tmp]#  echo "ike joker mac windows linux python" | tr "a-z" "A-Z"
IKE JOKER MAC WINDOWS LINUX PYTHON

# 匹配到3，删除
[root@localhost tmp]#  echo "ike joker mac windows linux python3" | tr -d "3"
ike joker mac windows linux python

# 删除匹配到0-9和a到z的内容，注意，中间的逗号不能有空格
[root@localhost tmp]# echo "Ike joker mac windows linux python3" | tr -d "0-9","a-z"
I

# 把文件中的小写替换成大写
[root@localhost tmp]# tr "a-z" "A-Z" < joker.txt
JOKER

# 删除文中出现的换行符、制表符（tab键）
[root@localhost tmp]# tr -d "\n\t" < joker.txt
jokerjokers[root@localhost tmp]#

# 去重连续的字符，tr是挨个匹配" ab" 每一个字符，包括空格去重
[root@localhost tmp]# echo "aaa   joker  ike bbb red hat" | tr -s "ab"
a   joker  ike b red hat

# -c取反结果，将所有除了'a'以外的全部替换为'A'
[root@localhost tmp]# echo "aaa   joker  ike bbb red hat" | tr -c "a" "A"
aaaAAAAAAAAAAAAAAAAAAAAAAAaAA[root@localhost tmp]#
```

#### 4.3.8 **stat命令**

stat命令用于显示文件的状态信息。stat命令的输出信息比ls命令的输出信息要更详细。

```powershell
# stat命令语法:
stat [选项]... SET1 [SET2]

# 常用参数：
-L, --dereference     # 跟随链接
-f, --file-system     # 显示文件系统状态而非文件状态
-c --format=格式       # 使用指定输出格式代替默认值，每用一次指定格式换一新行
--printf=格式     	 # 类似 --format，但是会解释反斜杠转义符，不使用换行作输出结尾。如果您仍希望使用换行，可以在格式中加入"\n"
-t, --terse           # 使用简洁格式输出
--help            	  # 显示此帮助信息并退出
--version         	  # 显示版本信息并退出

# 有效的文件格式序列(不使用 --file-system)：
%a    # 八进制权限


# 详情介绍
Access: 2019-10-18 14:58:59.465647961 +0800
Modify: 2019-10-18 14:58:57.799636638 +0800
Change: 2019-10-18 14:58:57.799636638 +0800

access  # 最近访问，文件每次被cat之后，时间变化，由于操作系统特性，做了优化，频繁访问，时间不变
modify  # 最近更改，更改文件内容，vim等
change  # 最近改动，文件元数据改变，如文件名
```

示例：

```powershell
# 指定要显示信息的普通文件或者文件系统对应的设备文件名。
[root@localhost tmp]# stat joker.txt
  File: ‘joker.txt’
  Size: 15              Blocks: 8          IO Block: 4096   regular file
Device: 803h/2051d      Inode: 16777289    Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Context: unconfined_u:object_r:user_tmp_t:s0
Access: 2021-10-10 16:11:24.880268490 +0800
Modify: 2021-10-10 16:11:19.143268379 +0800
Change: 2021-10-10 16:11:19.143268379 +0800
 Birth: -

# 显示文件权限
[root@localhost tmp]# stat -c  %a joker.txt
644
```

>  UNIX/Linux文件系统每个文件都有三种时间戳：
>
> - **访问时间**（-atime/天，-amin/分钟）：用户最近一次访问时间（文件修改了，还未被读取过，则不变）。
> - **修改时间**（-mtime/天，-mmin/分钟）：文件最后一次修改时间（数据变动）。
> - **变化时间**（-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。
>
> ![image-20211010165239759](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211010165239759.png)
>
> - 文件任何数据改变，change变化，无论是元数据变动，或是对文件mv，cp等
> - 文件内容被修改时，modify和change更新
> - 当change更新后，第一次访问该文件（cat，less等），access time首次会更新，之后则不会
>
> ```powershell
> touch -a：仅更新Access time（同时更新Change为current time）
> touch -m：仅更新Modify time（同时更新Change为current time）
> touch -c：不创建新文件
> touch -t：使用指定的时间更新时间戳（仅更改Access time与Modify time，Change time更新为current time）
> ```

#### 4.3.9 **find命令**

find命令用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。

如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。

并且将查找到的子目录和文件全部进行显示。

```shell
# find命令语法：
find 路径 -命令参数 [输出形式]

# 参数说明：
路径：告诉find在哪儿去找你要的东西，
```

| 参数                     | 解释                                                         |      |
| ------------------------ | ------------------------------------------------------------ | ---- |
| pathname                 | 要查找的路径                                                 |      |
| **options选项**          |                                                              |      |
| -maxdepth                | <目录层级>：设置最大目录层级；                               |      |
| -mindepth                | <目录层级>：设置最小目录层级；                               |      |
| **tests模块**            |                                                              |      |
| -atime                   | 按照文件访问access的时间查找，单位是天                       |      |
| -ctime                   | 按照文件的改变change状态来查找文件，单位是天                 |      |
| -mtime                   | 根据文件修改modify时间查找文件【最常用】                     |      |
| -name                    | 按照文件名字查找，支持* ? [] 通配符                          |      |
| -group                   | 按照文件的所属组查找                                         |      |
| -perm                    | 按照文件的权限查找                                           |      |
| -size n[cwbkMG]          | 按照文件的大小 为 n 个由后缀决定的数据块。 其中后缀为： b: 代表 512 位元组的区块（如果用户没有指定后缀，则默认为 b） c: 表示字节数 k: 表示 kilo bytes （1024字节） w: 字 （2字节） M:兆字节（1048576字节） G: 千兆字节 （1073741824字节） |      |
| -type 查找某一类型的文件 | b - 块设备文件。 d - 目录。 c - 字符设备文件。 p - 管道文件。 l - 符号链接文件。 f - 普通文件。 s - socket文件 |      |
| -user                    | 按照文件属主来查找文件。                                     |      |
| -path                    | 配合-prune参数排除指定目录                                   |      |
| **Actions模块**          |                                                              |      |
| -prune                   | 使find命令不在指定的目录寻找                                 |      |
| -delete                  | 删除找出的文件                                               |      |
| **-exec 或-ok**          | **对匹配的文件执行相应shell命令**                            |      |
| -print                   | 将匹配的结果标准输出                                         |      |
| **OPERATORS**            |                                                              |      |
| !                        | 取反                                                         |      |
| -a -o                    | 取交集、并集，作用类似&&和\                                  | \    |

示例：

```shell
# 找出 vmware-root_854-2697532808 文件夹并删除
[root@localhost tmp]# ls
efo.txt  file.txt  ike.txt  joker.txt  name2.zip  vmware-root_854-2697532808
[root@localhost tmp]#
[root@localhost tmp]# find . -name "vmware-root_854-2697532808" -delete
[root@localhost tmp]# ls
efo.txt  file.txt  ike.txt  joker.txt  name2.zip

# 找出所有pid结尾的文件
[root@localhost tmp]# touch ptyhon{1..5}.pid
[root@localhost tmp]# ls
efo.txt  file.txt  ike.txt  joker.txt  name2.zip  ptyhon1.pid  ptyhon2.pid  ptyhon3.pid  ptyhon4.pid  ptyhon5.pid
[root@localhost tmp]# find . -name "*.pid"
./ptyhon1.pid
./ptyhon2.pid
./ptyhon3.pid
./ptyhon4.pid
./ptyhon5.pid
```

**find根据修改时间查找文件**

```powershell
# 一天以内，被访问access过的文件
find . -atime -1  

# 一天以内，内容变化的文件
find . -mtime -1 

# 恰好在7天内被访问过的文件
[root@pylinux home]# find /  -maxdepth 3  -type f -atime 7
```

时间说明

- -atime -2 **搜索在2天内被访问过的文件**
- -atime 2 **搜索恰好在2天前被访问过的文件**
- -atime +2 **超过2天内被访问的文件**

**find反向查找**

```powershell
# 在当前目录下 查找最大目录深度为1 文件夹类型的数据
[root@localhost tmp]# find . -maxdepth 1  -type d

# 加上感叹号，后面接条件，代表取除了文件夹以外类型
[root@localhost tmp]# find . -maxdepth 1  ! -type d
```

**根据权限查找**

```powershell
# 寻找权限类型是755的文件
find /etc/ -maxdepth 2  -perm 755 -type f
```

**按照文件大小查找**

```shell
# 找出超过10M大小的文件
du -h `find . -maxdepth 2 -size +10M`
```

**查找时忽略目录**

```shell
# 跳过 ./conf.d 目录且打印 *.conf 的目录
find /etc/ -path "./conf.d" -prune -o -name "*.conf" -print
```

**根据用户组匹配**

```powershell
# 全局搜索深度为1，用户组是yu的文件
[root@localhost tmp]# find / -maxdepth 1 -group root
/
/boot
/dev
/proc
/run
/sys
/etc
/root
/var
/tmp
/usr
/bin
/sbin
/lib
/lib64
/home
/media
/mnt
/opt
/srv
```

**使用-exec或是-ok再次处理**

-ok比-exec更安全，存在用户提示确认

```powershell
# 找出以.pid结尾的文件后执行删除动作且确认
[root@localhost tmp]# find . -type f -name "*.pid" -ok rm {} \;
< rm ... ./ptyhon5.pid > ? y
< rm ... ./python.pid > ? y
< rm ... ./python1a.pid > ? y
< rm ... ./python123a.pid > ? y
[root@localhost tmp]# ls
efo.txt  file.txt  ike.txt  joker.txt  name2.zip

备注:
-exec 跟着shell命令，结尾必须以;分号结束，考虑系统差异，加上转义符\;
{}作用是替代find查阅到的结果
{}前后得有空格

# 找到目录中所有的.txt文件，且将查询结果写入到all.txt文件中
[root@localhost tmp]# find . -type f -name "*.txt" -exec cat {} \; > all.txt
cat: ./all.txt: input file is output file

# 把30天以前的日志，移动到old文件夹中
find . -type f -mtime +30 -name "*.log" -exec cp {} old \;
```

#### 4.4.0 **xargs命令**

xargs 又称管道命令，构造参数等。

是给命令传递参数的一个过滤器，也是组合多个命令的一个工具它把一个数据流分割为一些足够小的块，以方便过滤器和命令进行处理 。

简单的说就是 `把其他命令的给它的数据，传递给它后面的命令作为参数`

```shell
# 常用参数：
-d 为输入指定一个定制的分割符，默认分隔符是空格
-i 用 {} 代替 传递的数据
-I string 用string来代替传递的数据-n[数字] 设置每次传递几行数据
-n 选项限制单个命令行的参数个数
-t 显示执行详情
-p 交互模式
-P n 允许的最大线程数量为n
-s[大小] 设置传递参数的最大字节数(小于131072字节)
-x 大于 -s 设置的最大长度结束 xargs命令执行
-0，--null项用null分隔，而不是空白，禁用引号和反斜杠处理
```

示例：

```powershell
# 多行输入变单行
[root@localhost tmp]# xargs < efo.txt

# 限制每行输出格式
[root@localhost tmp]# xargs -n 3 < efo.txt

# 定义分隔符后，限制每行参数个数
[root@localhost tmp]# echo "iker,joker,ike,mac,windows,linux,python,java" |xargs -d "," -n 3

# 找到当前目录所有的.txt文件，然后拷贝到其他目录下
[root@localhost tmp]# find . -name "*.txt" | xargs -i cp {} heihei/
[root@localhost tmp]# find . -name "*.txt" | xargs -I data cp data  heihei/

# 找到当前目录下所有zip文件，然后删除
# -i参数的用法，用{}替换传递的数据
# -I 参数用法，用string代替数据
[root@localhost tmp]# find . -name "*.zip" | xargs -i rm -rf {}

```

**重点**

xargs识别字符串的标识是空格或是换行符，因此如果遇见文件名有空格或是换行符，xargs就会识别为两个字符串，就会报错

- -print0在find中表示每一个结果之后加一个NULL字符，而不是换行符（find默认在结果后加上\n，因此结果是换行输出的）
- Xargs -0 表示xargs用NULL作为分隔符

#### 4.4.1 sed命令

sed是linux强大的三剑客之一，它是一个行(流)编辑器，非交互式的对文件内容进行增删改查的操作。

##### 4.4.1.1 sed基本适用

sed是linux中提供的一个外部命令，它是一个行(流)编辑器，非交互式的对文件内容进行增删改查的操作，使用者只能在命令行输入编辑命令、指定文件名，然后在屏幕上查看输出，它和文本编辑器有本质的区别。 

```powershell
区别是：

文本编辑器: 编辑对象是文件

行编辑器：编辑对象是文件中的行
```

也就是前者一次处理一个文本，而后者是一次处理一个文本中的一行。这个是我们应该弄清楚且必须牢记的，否者可能无法理解sed的运行原理和使用精髓。 

 **sed数据处理原理** 

 ![sed处理数据原理.png](J:\homework\Python学习笔记\Python_notes.assets\1601961286169.png) 

**1. sed语法**

>  sed [options]  ‘{command}[flags]’  [filename] 

```powershell
# 命令选项
-e script  # 将脚本中指定的命令添加到处理输入时执行的命令中  多条件，一行中要有多个操作
-f script  # 将文件中指定的命令添加到处理输入时执行的命令中
-n         # 抑制自动输出
-i         # 编辑文件内容
-i.bak     # 修改时同时创建.bak备份文件。
-r         # 使用扩展的正则表达式
!          # 取反 （跟在模式条件后与shell有所区别）

# command   对文件干什么
sed 		# 常用内部命令
a   		# 在匹配后面添加
i   		# 在匹配前面添加
d   		# 删除
s   		# 查找替换 字符串
c   		# 更改
y   		# 转换   N D P 
p   		# 打印



# flags
数字             # 表示新文本替换的模式
g：             # 表示用新文本替换现有文本的全部实例
p：             # 表示打印原始的内容
w filename:     # 将替换的结果写入文件
```

**2. sed内部命令示例**

- 文件内容增加操作，将数据追加到某个位置之后，使用命令 <font color="red">a</font> 。 

```powershell
# 1.在test.txt的每行后追加一行新数据内容: append data "haha"
[root@localhost opt]# sed 'a\append data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
append data "haha"
2 the quick brown fox jumps over the lazy cat . dog
append data "haha"
3 the quick brown fox jumps over the lazy cat . dog
append data "haha"
4 the quick brown fox jumps over the lazy cat . dog
append data "haha"
5 the quick brown fox jumps over the lazy cat . dog
append data "haha"

# 2.在第二行后新开一行追加数据: append data "haha"
[root@localhost opt]# sed '2a\append data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
append data "haha"
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 3.在第二行到第四行后新开一行追加数据: append data "haha"
[root@localhost opt]# sed '2,4a\append data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
append data "haha"
3 the quick brown fox jumps over the lazy cat . dog
append data "haha"
4 the quick brown fox jumps over the lazy cat . dog
append data "haha"
5 the quick brown fox jumps over the lazy cat . dog
[root@localhost opt]#

# 4.匹配字符串追加: 找到包含"3 the"的行，在其后新开一行追加内容: append data "haha"
[root@localhost opt]# sed '/3 the/a\append data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
append data "haha"
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

注：//开启匹配模式  /要匹配的字符串/
```

- 文件内容增加操作，将数据插入到某个位置之前，使用命令 <font color="red">**i**</font>。 

```powershell
# 1.在test.txt的每行前插入一行新数据内容: insert data "haha"
[root@localhost opt]# sed 'i\insert data "haha"' test.txt
insert data "haha"
1 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
2 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
3 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
4 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
5 the quick brown fox jumps over the lazy cat . dog

# 2.在第二行前新开一行插入数据: insert data "haha"
[root@localhost opt]# sed '2i\insert data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 3.在第二到四行每行前新开一行插入数据: insert data "haha"
[root@localhost opt]# sed '2,4i\insert data "haha"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
2 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
3 the quick brown fox jumps over the lazy cat . dog
insert data "haha"
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 4.匹配字符串插入: 找到包含"3 the"的行，在其前新开一行插入内容: insert data "haha"
[root@localhost opt]# sed '/3 the/i\insert data "haha' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
insert data "haha
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
```

- 文件内容修改操作–替换，将一行中匹配的内容替换为新的数据，使用命令<font color="red">s</font>。 

```powershell
# 1.从标准输出流中做替换，将test替换为text
[root@localhost opt]# echo "this is a test" |sed 's/test/text/'
this is a text

# 2.将test.txt中每行的dog替换为cat
[root@localhost opt]# sed 's/dog/cat/' test.txt
1 the quick brown fox jumps over the lazy cat . cat
2 the quick brown fox jumps over the lazy cat . cat
3 the quick brown fox jumps over the lazy cat . cat
4 the quick brown fox jumps over the lazy cat . cat
5 the quick brown fox jumps over the lazy cat . cat

# 3.将test.txt中第二行的dog替换为cat
[root@localhost opt]# sed '2s/dog/cat/' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . cat
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 4.将test.txt中第二到第四行的dog替换为cat
[root@localhost opt]# sed '2,4s/dog/cat/' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . cat
3 the quick brown fox jumps over the lazy cat . cat
4 the quick brown fox jumps over the lazy cat . cat
5 the quick brown fox jumps over the lazy cat . dog

# 5.匹配字符串替换:将包含字符串"3 the"的行中的dog替换为cat
[root@localhost opt]# sed '/3 the/s/dog/cat/' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . cat
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

```

-  文件内容修改操作–更改，将一行中匹配的内容替换为新的数据，使用命令 <font color="red">c</font>。 

```powershell
# 1.将test.txt文件中的所有行的内容更改为: change data "data"
[root@localhost opt]# sed 'c\change data "hehe"' test.txt
change data "hehe"
change data "hehe"
change data "hehe"
change data "hehe"
change data "hehe"

# 2.将test.txt文件第二行的内容更改为: change data "haha"
[root@localhost opt]# sed '2c\change data "hehe"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
change data "hehe"
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 3.将test.txt文件中的第二、三、四行的内容更改为：change data "haha"
[root@localhost opt]# sed '2,4c\change data "hehe"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
change data "hehe"
5 the quick brown fox jumps over the lazy cat . dog

# 4.将data1文件中包含"3 the"的行内容更改为: change data "haha"
[root@localhost opt]# sed '/3 the/c\change data "hehe"' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
change data "hehe"
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

```

- 文件内容修改操作–字符转换，将一行中匹配的内容替换为新的数据，使用命令<font color="red">y</font>。 

```powershell
# 1.将test.txt中的a b c字符转换为对应的 A  B  C字符
[root@localhost opt]# sed 'y/abc/ABC/' test.txt
1 the quiCk Brown fox jumps over the lAzy CAt . dog
2 the quiCk Brown fox jumps over the lAzy CAt . dog
3 the quiCk Brown fox jumps over the lAzy CAt . dog
4 the quiCk Brown fox jumps over the lAzy CAt . dog
5 the quiCk Brown fox jumps over the lAzy CAt . dog
```

- 文件内容删除，将文件中的指定数据删除，使用命令<font color="red">d</font>。 

```powershell
# 1.删除文件test.txt中的所有数据
[root@localhost opt]# sed 'd' test.txt

# 2.删除文件test.txt中的第三行数据
[root@localhost opt]# sed '3d' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 3.删除文件test.txt中包含字符串"3 the"的行
[root@localhost opt]# sed '/3 the/d' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

```

- 文件内容查看，将文件内容输出到屏幕，使用命令<font color="red">p</font>。 

```powershell
# 1.打印test.txt文件内容
[root@localhost opt]# sed 'p' test.txt
1 the quick brown fox jumps over the lazy cat . dog
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 2.打印test.txt文件第3行内容
[root@localhost opt]# sed '3p' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 3.打印test.txt文件第2行到第5行内容
[root@localhost opt]# sed '2,5p' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 4.打印匹配到 3 the 的内容
[root@localhost opt]# sed '/3 the/p' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

可以看得出，打印内容是重复的行，原因是打印了指定文件内容一次，又将读入缓存的所有数据打印了一次，所以会看到这样的效果，如果不想看到这样的结果，可以加命令选项-n 抑制内存输出即可。
```

##### 4.4.1.2 命令选项说明

**在sed命令中，命令选项是对sed中的命令的增强** 

- 在命令行中使用多个命令 -e 

```powershell
# 1.将brown替换为green dog替换为cat
[root@localhost opt]# sed -e 's/brown/green/;s/dog/cat/' test.txt
1 the quick green fox jumps over the lazy cat . cat
2 the quick green fox jumps over the lazy cat . cat
3 the quick green fox jumps over the lazy cat . cat
4 the quick green fox jumps over the lazy cat . cat
5 the quick green fox jumps over the lazy cat . cat

# 2.从文件读取编辑器命令 -f 适用于日常重复执行的场景
[root@localhost opt]# vim abc
[root@localhost opt]# cat abc
s/brown/green/
s/dog/cat/
s/fox/elephant/
[root@localhost opt]# sed -f abc test.txt
1 the quick green elephant jumps over the lazy cat . cat
2 the quick green elephant jumps over the lazy cat . cat
3 the quick green elephant jumps over the lazy cat . cat
4 the quick green elephant jumps over the lazy cat . cat
5 the quick green elephant jumps over the lazy cat . cat

```

- 抑制内存输出 -n 

```powershell
# 1.打印test.txt文件的第二行到最后一行内容  $最后的意思 
[root@localhost opt]# sed -n '2,$p' test.txt
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
```

- 使用正则表达式 -r 

```powershell
# 1.打印text.txt中以字符串"3 the"开头的行内容
[root@localhost opt]# sed -n -r '/^3 the/p' test.txt
3 the quick brown fox jumps over the lazy cat . dog
```

 从上述的演示中，大家可以看出，数据处理只是在缓存中完成的，并没有实际修改文件内容，如果需要修改文件内容可以直接使用-i命令选项。在这里我需要说明的是-i是一个不可逆的操作，一旦修改，如果想复原就很困难，几乎不可能，所以建议大家在操作的时候可以备份一下源文件。-i命令选项提供了备份功能，比如参数使用-i.bak，那么在修改源文件的同时会先备份一个以.bak结尾的源文件，然后再进行修改操作。 

示例：

```powershell
# 1.查看文件列表，没有发现test.txt.bak

# 2.执行替换命令并修改文件
[root@localhost opt]# sed -i.bak 's/brown/green/' test.txt

# 3.发现文件夹中多了一个data1.bak文件
[root@localhost opt]# find ./test.txt.bak
./test.txt.bak
[root@localhost opt]# cat test.txt.bak
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
[root@localhost opt]# cat test.txt
1 the quick green fox jumps over the lazy cat . dog
2 the quick green fox jumps over the lazy cat . dog
3 the quick green fox jumps over the lazy cat . dog
4 the quick green fox jumps over the lazy cat . dog
5 the quick green fox jumps over the lazy cat . dog
```

##### 4.4.1.3 标志

**在sed命令中，标志是对sed中的内部命令做补充说明** 

数字标志：此标志是一个非零正数，默认情况下，执行替换的时候，如果一行中有多个符合的字符串，如果没有标志位定义，那么只会替换第一个字符串，其他的就被忽略掉了，为了能精确替换，可以使用数字位做定义。 

```powershell
[root@localhost opt]# cat test.txt
1 the quick brown fox jumps over the lazy dog . dog
2 the quick brown fox jumps over the lazy dog . dog
3 the quick brown fox jumps over the lazy dog . dog
4 the quick brown fox jumps over the lazy dog . dog
5 the quick brown fox jumps over the lazy dog . dog
```

- 替换一行中的第二处dog为cat 

```powershell
[root@localhost opt]# sed 's/dog/cat/2' test.txt
1 the quick brown fox jumps over the lazy dog . cat
2 the quick brown fox jumps over the lazy dog . cat
3 the quick brown fox jumps over the lazy dog . cat
4 the quick brown fox jumps over the lazy dog . cat
5 the quick brown fox jumps over the lazy dog . cat

```

- g标志:将一行中的所有符合的字符串全部执行替换 

```powershell
[root@localhost opt]# sed 's/dog/cat/g' test.txt
1 the quick brown fox jumps over the lazy cat . cat
2 the quick brown fox jumps over the lazy cat . cat
3 the quick brown fox jumps over the lazy cat . cat
4 the quick brown fox jumps over the lazy cat . cat
5 the quick brown fox jumps over the lazy cat . cat

```

- p标志：打印文本内容，类似于-p命令选项 

```powershell
[root@localhost opt]# sed -n '3s/dog/cat/p' test.txt
3 the quick brown fox jumps over the lazy cat . dog
```

- w filename标志：将修改的内容存入filename文件中 

```powershell
[root@localhost opt]# sed '3s/dog/cat/w txt' test.txt
1 the quick brown fox jumps over the lazy dog . dog
2 the quick brown fox jumps over the lazy dog . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy dog . dog
5 the quick brown fox jumps over the lazy dog . dog

可以看出，将修改的第三行内容存在了text文件中
[root@localhost opt]# cat txt
3 the quick brown fox jumps over the lazy cat . dog
```

#### 4.4.2 awk命令

awk是linux强大的三剑客之一，awk是一种可以处理数据、产生格式化报表的语言，功能十分强大。

##### 4.4.2.1 awk基本适用

在日常计算机管理中，总会有很多数据输出到屏幕或者文件，这些输出包含了标准输出、标准错误输出。默认情况下，这些信息全部输出到默认输出设备—屏幕。然而，大量的数据输出中，只有一小部分是我们需要重点关注的，我们需要把我们需要的或者关注的这些信息过滤或者提取以备后续需要时调用。早先的学习中，我们学过使用grep来过滤这些数据，使用cut、tr命令提出某些字段，但是他们都不具备提取并处理数据的能力，都必须先过滤，再提取转存到变量，然后在通过变量提取去处理，比如： 

```powershell
内存使用率的统计步骤

1) 通过free -m提取出内存总量，赋值给变量 memory_totle

2）通过free -m提取出n内存使用量，赋值给变量 memory_use

3）通过数学运算计算内存使用率
```

需要执行多步才能得到内存使用率，那么有没有一个命令能够集过滤、提取、运算为一体呢？当然，就是今天我要给大家介绍的命令：awk

平行命令还有gawk、pgawk、dgawk

awk是一种可以处理数据、产生格式化报表的语言，功能十分强大。awk 认为文件中的每一行是一条记录 记录与记录的分隔符为换行符,每一列是一个字段 字段与字段的分隔符默认是一个或多个空格或tab制表符。

awk的工作方式是读取数据，将每一行数据视为一条记录（record）每条记录以字段分隔符分成若干字段，然后输出各个字段的值。

**1. awk语法**

>  awk [options] ‘[BEGIN]{program}[END]’ [FILENAME] 

```powershell
常用命令选项
-F fs 	指定描绘一行中数据字段的文件分隔符  默认为空格
-f file 指定读取程序的文件名
-v var=value 定义awk程序中使用的变量和默认值

注意：awk 程序由左大括号和右大括号定义。 程序命令必须放置在两个大括号之间。由于awk命令行假定程序是单文本字符串，所以必须将程序包括在单引号内。
1）程序必须放在大括号内
2）程序必须要用单引号引起来

awk程序运行优先级是:
    1)BEGIN: 在开始处理数据流之前执行，可选项
    2)program: 如何处理数据流，必选项
    3)END: 处理完数据流后执行，可选项
```

**2. awk基本使用**

能够熟练使用awk对标准输出的行、列、字符串截取 

```powershell
# 1.演示文稿
[root@localhost opt]# cat test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
```

- awk对字段(列)的提取

字段提取：提取一个文本中的一列数据并打印输出字段相关内置变量

$0 表示整行文本

$1 表示文本行中的第一个数据字段

$2 表示文本行中的第二个数据字段

$N 表示文本行中的第N个数据字段

$NF 表示文本行中的最后一个数据字段

```powershell
# 1.读入test每行数据并把每行数据打印出来
[root@localhost opt]# awk '{print $0}' test.txt
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

# 2.打印test第六个字段
[root@localhost opt]# awk '{print $6}' test.txt
jumps
jumps
jumps
jumps
jumps

# 3.打印test最后一个字段
[root@localhost opt]# awk '{print $NF}' test.txt
dog
dog
dog
dog
dog

```

- 命令选项详解

 <font color="red"> -F</font>：指定字段与字段的分隔符

当输出的数据流字段格式不是awk默认的字段格式时，我们可以使用-F命令选项来重新定义数据流字段分隔符。比如：处理的文件是/etc/passwd，希望打印第一列、第三列、最后一列

```powershell
[root@localhost opt]# awk -F ':' '{print $1,$3,$NF}' /etc/passwd
root 0 /bin/bash
bin 1 /sbin/nologin
daemon 2 /sbin/nologin
adm 3 /sbin/nologin
lp 4 /sbin/nologin
sync 5 /bin/sync
shutdown 6 /sbin/shutdown
halt 7 /sbin/halt
mail 8 /sbin/nologin
operator 11 /sbin/nologin
games 12 /sbin/nologin
ftp 14 /sbin/nologin
nobody 99 /sbin/nologin
systemd-network 192 /sbin/nologin
dbus 81 /sbin/nologin
polkitd 999 /sbin/nologin
abrt 173 /sbin/nologin
sshd 74 /sbin/nologin
postfix 89 /sbin/nologin
tcpdump 72 /sbin/nologin
mysql 998 /sbin/nologin
joker 1000 /bin/bash

可以看的出，awk输出字段默认的分隔符也是空格
```

<font color="red"> -f file</font>：如果awk命令是日常重复工作，而又没有太多变化，可以将程序写入文件，每次使用-f调用程序文件就好，方便，高效。 

```powershell
[root@localhost opt]# cat abc
{print $1,$3,$NF}
[root@localhost opt]# awk -f abc test.txt
1 quick dog
2 quick dog
3 quick dog
4 quick dog
5 quick dog

```

<font color="red">-v</font> 定义变量，既然作者写awk的时候就是按着语言去写的，那么语言中最重要的要素—变量肯定不能缺席，所以可以使用-v命令选项定义变量 

```powershell
[root@localhost opt]# awk -v name="joker" 'BEGIN{print name}'
joker
```

- awk对记录(行)的提取

记录提取：提取一个文本中的一行并打印输出

记录的提取方法有两种：1. 通过行号  2. 通过正则匹配

记录相关内置变量

NR：指定行号 number row 

```powershell
提取test第三行数据

# 1.指定行号为3
[root@localhost opt]# awk 'NR==3{print $0}' test.txt
3 the quick brown fox jumps over the lazy cat . dog

# 2.指定行的第一个字段精确匹配字符串为5
[root@localhost opt]# awk '$1=="5" {print $0}' test.txt
5 the quick brown fox jumps over the lazy cat . dog
```

- awk对字符串提取

 记录和字段的汇合点就是字符串 

```powershell
# 1.打印test第三行的第六个字段
[root@localhost opt]# awk 'NR==3{print $6}' test.txt
jumps

```

##### 4.4.2.2 awk程序的优先级

awk代码块的优先级

关于awk程序的执行优先级，BEGIN是优先级最高的代码块，是在执行PROGRAM之前执行的，不需要提供数据源，因为不涉及到任何数据的处理，也不依赖与PROGRAM代码块；PROGRAM是对数据流干什么，是必选代码块，也是默认代码块。所以在执行时必须提供数据源；END是处理完数据流后的操作，如果需要执行END代码块，就必须需要PROGRAM的支持，单个无法执行。

BEGIN：处理数据源之前干什么----不需要数据源就可以执行

PROGRAM： 对数据源干什么----【默认必须有】 需要数据源

END：处理完数据源后干什么----需要program 需要数据源

```powershell
# 1.优先级展示
[root@localhost opt]# awk 'BEGIN{print "hello world"} {print $0} END{print "bye world"}' test.txt
hello world
1 the quick brown fox jumps over the lazy cat . dog
2 the quick brown fox jumps over the lazy cat . dog
3 the quick brown fox jumps over the lazy cat . dog
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog
bye world

# 2.不需要数据源，可以直接执行
[root@localhost opt]# awk 'BEGIN{print "hello world"}'
hello world

# 3.没有提供数据流，所以无法执行成功
[root@localhost opt]# awk '{print "hello world"}'
^C
[root@localhost opt]# awk 'END{print "hello world"}'
^C

```

##### 4.4.2.3 awk高级应用

awk是一门语言，那么就会符合语言的特性，除了可以定义变量外，还可以定义数组，还可以进行运算，流程控制，我们接下来看看吧。 

- awk定义变量和数组

定义变量

```powershell
[root@localhost opt]# awk -v name="joker" 'BEGIN{print name}'
joker
[root@localhost opt]# awk 'BEGIN{name="JOKER";print name}'
JOKER
```

 数组定义方式：数组名[索引]=值 

```powershell
# 1.定义数组array，有两个元素，分别是100，200，打印数组元素。
[root@localhost opt]# awk 'BEGIN{arry[0]=100;arry[1]=200;print arry[0],arry[1]}'
100 200
[root@localhost opt]# awk 'BEGIN{a[0]=100;a[1]=200;print a[1]}'
200
[root@localhost opt]# awk 'BEGIN{a[0]=100;a[1]=200;print a[0]}'
100

```

- awk运算

1. 赋值运算 =
2. 比较运算 >  >=   ==  <   <=   !=
3. 数学运算 +  -  *  /  %  **  ++  –
4. 逻辑运算  &&  ||  ！
5. 匹配运算 ~ !~ 精确匹配 == !=

<font color="red">赋值运算：主要是对变量或者数组赋值</font>

如：变量赋值：name=‘joker'' school=‘hello school’ 			数组赋值 array[0]=100

```powershell
[root@localhost ~]# awk -v name="joker" 'BEGIN{print name}'
joker
[root@localhost ~]# awk 'BEGIN{school="hello school";print school}'
hello school
[root@localhost ~]# awk 'BEGIN{arry[0]=100;print arry[0]}'
100

```

<font color="red"> 比较运算：如果比较的是字符串则按ascii编码顺序表比较。如果结果返回为真则用1表示，如果返回为假则用0表示 </font>

```powershell
[root@localhost ~]# awk 'BEGIN{print "ab" == "b"}'
0
[root@localhost ~]# awk 'BEGIN{print "b" == "b"}'
1

[root@localhost opt]# awk '$1>3{print $0}' test.txt
4 the quick brown fox jumps over the lazy cat . dog
5 the quick brown fox jumps over the lazy cat . dog

[root@localhost opt]# awk 'BEGIN{print 100 >= 1 }'
1
[root@localhost opt]# awk 'BEGIN{print 100 >= 101 }'
0
[root@localhost opt]# awk 'BEGIN{print 100 == 101 }'
0
[root@localhost opt]# awk 'BEGIN{print 100 < 101 }'
1

```

<font color="red">数学运算</font>

```powershell
[root@localhost opt]# awk 'BEGIN{print 100+3 }'
103
[root@localhost opt]# awk 'BEGIN{print 100-3 }'
97
[root@localhost opt]# awk 'BEGIN{print 100*3 }'
300
[root@localhost opt]# awk 'BEGIN{print 100/3 }'
33.3333
[root@localhost opt]# awk 'BEGIN{print 100**3 }'
1000000
[root@localhost opt]# awk 'BEGIN{print 100%3 }'
1
[root@localhost opt]# awk -v 'count=0' 'BEGIN{count++;print count}'
1
[root@localhost opt]# awk -v 'count=0' 'BEGIN{count--;print count}'
-1

```

<font color="red"> 逻辑运算 </font>

```powershell
# 1.与运算:真真为真，真假为假，假假为假
[root@localhost opt]# awk 'BEGIN{print 100>=2 && 100>=3 }'
1
[root@localhost opt]# awk 'BEGIN{print 100>=2 && 1>=100}'
0

# 2.或运算:真真为真，真假为真，假假为假
[root@localhost opt]# awk 'BEGIN{print 100>=2 || 1>=100}'
1
[root@localhost opt]# awk 'BEGIN{print 100>=200 || 1>=100}'
0

# 3.非运算
[root@localhost opt]# awk 'BEGIN{print !(100>=2)}'
0
[root@localhost opt]# awk 'BEGIN{print !(100<=2)}'
1

```

<font color="red">匹配运算</font>

```powershell
# 1.匹配以：分割，第一列以ro开头的一行数据
[root@localhost opt]# awk -F ":" '$1 ~ "^ro" {print $0}' /etc/passwd
root:x:0:0:root:/root:/bin/bash

# 2.匹配以：分割，第一行以ro开头的第一列数据
[root@localhost opt]# awk -F ":" '$0 ~ "^ro" {print $1}' /etc/passwd
root

# 3.匹配以：分割，第一行不以ro开头的的整行数据
[root@localhost opt]# awk -F ":" '$0 !~ "ro" {print $0}' /etc/passwd
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
sync:x:5:0:sync:/sbin:/bin/sync
shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
halt:x:7:0:halt:/sbin:/sbin/halt
mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
games:x:12:100:games:/usr/games:/sbin/nologin
ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
nobody:x:99:99:Nobody:/:/sbin/nologin
systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
dbus:x:81:81:System message bus:/:/sbin/nologin
polkitd:x:999:998:User for polkitd:/:/sbin/nologin
abrt:x:173:173::/etc/abrt:/sbin/nologin
sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
postfix:x:89:89::/var/spool/postfix:/sbin/nologin
tcpdump:x:72:72::/:/sbin/nologin
mysql:x:998:996::/home/mysql:/sbin/nologin
joker:x:1000:1000::/home/joker:/bin/bash

```

##### 4.4.2.4 awk环境变量

| 变量        | 描述                                                   |
| :---------- | :----------------------------------------------------- |
| FIELDWIDTHS | 以空格分隔的数字列表，用空格定义每个数据字段的精确宽度 |
| FS          | 输入字段分隔符号 数据源的字段分隔符 -F                 |
| OFS         | 输出字段分隔符号                                       |
| RS          | 输入记录分隔符                                         |
| ORS         | 输出记录分隔符号                                       |

```powershell
# 1.FIELDWIDTHS:重定义列宽并打印，注意不可以使用$0打印所有，因为$0是打印本行全内容，不会打印你定义的字段
[root@localhost opt]# awk 'BEGIN{FIELDWIDTHS="5 2 8"} NR==1{print $1,$2,$3}' /etc/passwd
root: x: 0:0:root

# 2.FS:指定数据源中字段分隔符，类似命令选项-F
[root@localhost opt]# awk 'BEGIN{FS=":"} NR==1{print $1,$3,$NF}' /etc/passwd
root 0 /bin/bash

# 3.OFS:指定输出到屏幕后字段的分隔符
[root@localhost opt]# awk 'BEGIN{FS=":";OFS="-"}NR==1{print $1,$3,$NF}' /etc/passwd
root-0-/bin/bash

# 4.RS:指定记录的分隔符:将记录的分隔符修改为空行后，所有的行会变成一行，所以所有字段就在一行了。
[root@localhost opt]# awk 'BEGIN{RS=""}{print $1,$13,$25,$37,$49}' test.txt
1 2 3 4 5

# 5.ORS:输出到屏幕后记录的分隔符，默认为回车，可以看出，提示符和输出在一行了，因为默认回车换成了*
[root@localhost opt]# awk 'BEGIN{RS="";ORS="*"}{print $1,$13,$25,$37,$49}' test.txt
1 2 3 4 5*[root@localhost opt]#

```

##### 4.4.2.5 流程控制

- if判断语句

```powershell
# 模板
[root@zutuanxue ~]# cat num
1
2
3
4
5
6
7
8
9

# 1.打印$1大于5的行
[root@localhost opt]# awk '{if($1>5)print $0}' num
6
7
8
9

# 2.假如$1大于5则除以2输出，否则乘以2输出
[root@localhost opt]# awk '{if($1>5)print $1/2;else print $1*2}' num
2
4
6
8
10
3
3.5
4
4.5

```

- for循环语句

```powershell
# 模板
[root@localhost opt]# cat num2
60 50 100
150 30 10
70 100 40

# 1.将一行中的数据都加起来  $1+$2+$3
[root@localhost opt]# awk '{sum=0;for (i=1;i<4;i++){sum+=$i}print sum}' num2
210
190
210

[root@localhost opt]# awk '{
sum=0
for (i=1;i<4;i++){
sum+=$i}
print sum}' num2
210
190
210

```

-  while循环语句–先判断后执行 

```powershell
# 1.将文件中的每行的数值累加，和大于或等于150就停止累加
[root@localhost opt]# awk '{sum=0;i=1;while(sum<150){sum+=$i;i++}print sum}' num2
210
150
170

[root@localhost opt]# awk '{
> sum=0
> i=1
> while (sum<150) {
>    sum+=$i
>    i++
> }
> print sum
> }' num2
210
150
170

```

-  do…while循环语句–先执行后判断 

```powershell
# 1.将文件中的每行的数值累加，和大于或等于150就停止累加
[root@localhost opt]# awk '{sum=0;i=1;do{sum+=$i;i++}while(sum<150);print sum}' num2
210
150
170

[root@localhost opt]# awk '{
> sum=0
> i=1
> do {
> sum+=$i
> i++}
> while (sum<150)
> print sum}' num2
210
150
170

```

##### 4.4.2.6  **循环控制语句** 

- break 跳出循环,继续执行后续语句 

```powershell
# 1.累加每行数值，和大于150停止累加
[root@localhost opt]# awk '{
> sum=0
> i=1
> while (i<4){
> sum+=$i
> if (sum>150){
> break}
> i++}
> print sum
> }' num2
210
180
170

```

---

### 4.3 Linux文件属性与管理

文件或目录属性主要包括：

- 索引节点，inode
- 文件类型
- 文件权限
- 硬链接个数
- 归属的用户和用户组
- 最新修改时间

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\297.jpg)

图解：

> 1. Inode索引节点号，（人的身份证，家庭地址等信息，唯一），系统寻找文件名 > Inode号 > 文件内容
> 2. 文件权限，第一个字符是文件类型，随后9个字符是文件权限，最后一个. 有关selinux
> 3. 文件硬链接数量，与ln命令配合
> 4. 文件所属用户
> 5. 文件所属用户组
> 6. 文件目录大小
> 7. 文件修改时间
> 8. 文件名

**文件扩展名：**

windows下的文件扩展名

- docx
- pptx
- pdf
- jpg
- avi
- mp4
- gif
- rar
- Zip

对于windows系统，文件名后缀有问题则会影响使用

Linux文件的扩展名只是方便阅读，对文件类型不影响

Linux通过文件属性区分文件类型

- .txt文本类型
- .conf .cfg .configure 配置文件
- .sh .bash 脚本后缀
- .py 脚本后缀
- .rpm 红帽系统二进制软件包名
- .tar .gz .zip 压缩后缀

**文件类型：**

可以通过ls -F 给文件结尾加上特殊标识。

| 格式              | 类型                                                |
| ----------------- | --------------------------------------------------- |
| ls -l看第一个字符 |                                                     |
| -                 | 普通文件regular file，（二进制，图片，日志，txt等） |
| d                 | 文件夹directory                                     |
| b                 | 块设备文件，/dev/sda1，硬盘，光驱                   |
| c                 | 设备文件，终端/dev/tty1,网络串口文件                |
| s                 | 套接字文件，进程间通信（socket）文件                |
| p                 | 管道文件pipe                                        |
| l                 | 链接文件，link类型，快捷方式                        |

**普通文件**

通过如下命令生成都是普通文件(windows中各种扩展名的文件，放入linux也是普通文件类型)

- echo
- touch
- cp
- cat
- 重定向符号 >

普通文件特征就是文件类型是，"**-**"开头，以内容区分一般分为

- 纯文本，可以用cat命令读取内容，如字符、数字、特殊符号等
- 二进制文件（binary），Linux中命令属于这种格式，例如ls、cat等命令

**文件夹**

文件权限开头带有**d**字符的文件表示文件夹，是一种特殊的Linux文件

- mkdir
- cp拷贝文件夹

**链接类型**

- ln命令创建

类似windows的快捷方式

#### 4.3.1 **file命令**

```powershell
# 二进制解释器类型
[root@localhost tmp]# file /usr/bin/python2.7
/usr/bin/python2.7: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=8d75b23c27b98a6fc5656327f915409f6f1fba5b, stripped

# yum是python的脚本文件
[root@localhost tmp]# file /usr/bin/yum
/usr/bin/yum: Python script, ASCII text executable

# shell脚本，内置命令
[root@localhost tmp]# file /usr/bin/cd
/usr/bin/cd: POSIX shell script, ASCII text executable

# txt文件
[root@localhost tmp]# file efo.txt
efo.txt: ASCII text

# 文件目录
[root@localhost tmp]# file heihei/
heihei/: directory

# 软链接类型
[root@localhost tmp]# file /usr/bin/python2
[root@localhost tmp]# ln -s test.txt /opt/t.txt
[root@localhost tmp]# cd /opt/
[root@localhost opt]# ll
total 0
lrwxrwxrwx. 1 root root 8 Oct 10 20:48 t.txt -> test.txt
[root@localhost opt]# file /opt/t.txt
/opt/t.txt: broken symbolic link to `test.txt'
```

#### 4.3.2 **软链接**

windows的一个快捷方式而已

```powershell
# ln  -s  目标文件绝对路径     软连接绝对路径
[root@localhost tmp]# ln -s test.txt /opt/t.txt

# 删除快捷方式，删除软连接是不会影响源文件的
```

#### 4.3.3 **which命令**

查找PATH环境变量中的文件，linux内置命令不在path中。

```powershell
[root@localhost tmp]# which python
/usr/bin/python
```

#### 4.3.4 **whereis命令**

whereis命令用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。

```powershell
[root@localhost tmp]# whereis python
python: /usr/bin/python /usr/bin/python2.7 /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz
```

#### 4.3.5 **tar命令**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\304.jpg)

tar命令在linux系统里，可以实现对多个文件进行，压缩、打包、解包

```shell
# tar命令语法：
tar(选项)(参数)

# 常用参数：
-A或--catenate：  # 新增文件到以存在的备份文件；
-B：				 # 设置区块大小；
-c或--create：	# 建立新的备份文件；
-C <目录>：		# 这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。
-d：				# 记录文件的差别；
-x或--extract或--get：# 从备份文件中还原文件；
-t或--list：		# 列出备份文件的内容；
-z或--gzip或--ungzip：# 通过gzip指令处理备份文件；
-Z或--compress或--uncompress：# 通过compress指令处理备份文件；
-f<备份文件>或--file=<备份文件>：# 指定备份文件；
-v或--verbose：		# 显示指令执行过程；
-r：					# 添加文件到已经压缩的文件；
-u：					# 添加改变了和现有的文件到已经存在的压缩文件；
-j：					# 支持bzip2解压文件；
-v：					# 显示操作过程；
-l：					# 文件系统边界设置；
-k：					# 保留原有文件不覆盖；
-m：					# 保留文件不被覆盖；
-w：					# 确认压缩文件的正确性；
-p或--same-permissions：# 用原来的文件权限还原文件；
-P或--absolute-names：# 文件名使用绝对名称，不移除文件名称前的“/”号；不建议使用
-N <日期格式> 或 --newer=<日期时间>：# 只将较指定日期更新的文件保存到备份文件里；
--exclude=<范本样式>：# 排除符合范本样式的文件。
-h, --dereference	# 跟踪符号链接；将它们所指向的文件归档并输出

*.gz		# gzip命令解压
*.tar		# tar命令解压
*.xz		# xz命令解压
*.zip		# unzip命令解压
```

**案例1：打包/opt/目录下所有的内容，打包生成tar包allopt.tar**

```powershell
# 第一步：创建一个大文件
[root@localhost tmp]# echo afafa{1..800000} >> file.txt
[root@localhost tmp]# ls
file.txt  quanquan.txt
[root@localhost tmp]# du -h ./*
9.1M    ./file.txt
1.2M    ./quanquan.txt

# 第二步：打包opt下所有文件
[root@localhost tmp]# tar -cvf allopt.tar ./*
./file.txt
./quanquan.txt
[root@localhost tmp]# du -h ./*
11M     ./allopt.tar
9.1M    ./file.txt
1.2M    ./quanquan.txt

# 第三步：解包这个tar包
[root@localhost tmp]# tar -xvf allopt.tar
./file.txt
./quanquan.txt
[root@localhost tmp]# ls
allopt.tar  file.txt  quanquan.txt
```

**案例2：打包，且压缩/opt目录下所有内容，生成tar.gz包`allopt.tar.gz`**

```powershell
# 第一步：打包，且压缩，就是加一个-z参数即可
[root@localhost tmp]# tar -zcvf 1012allopt.tar.gz ./*
[root@localhost tmp]# mv 1012allopt.tar.gz ../
[root@localhost tmp]# rm -rf *

# 第二步：解压缩，常见的*.tar.gz，也有人会缩写成  *.tgz ，都可以如此的去解压缩
[root@localhost tmp]# mv ../1012allopt.tar.gz ./
[root@localhost tmp]# du -h ./*
[root@localhost tmp]# tar -zxvf 1012allopt.tar.gz ./
```

**注意**

- f参数必须写在最后，后面紧跟压缩文件名
- tar命令仅打包，习惯用.tar作为后缀
- tar命令加上z参数，文件以.tar.gz或.tgz表示

**列出tar包内的文件**

```powershell
# 根据tar包文件后缀，决定是否添加z参数，调用gzip
[root@localhost tmp]# tar -ztvf allopt.tar.gz
```

**拆开tar包**

```shell
[root@localhost tmp]# tar -xf heihei.tar
-rw-r--r-- root/root  21370880 2021-10-11 23:04 ./1011allopt.tar
-rw-r--r-- root/root  10680320 2021-10-10 21:57 ./allopt.tar
-rw-r--r-- root/root        31 2021-10-11 16:28 ./eco.txt
-rw-r--r-- root/root   9488895 2021-10-10 21:56 ./file.txt
-rw-r--r-- root/root   1188894 2021-10-10 21:55 ./quanquan.txt
drwx------ root/root         0 2021-10-11 22:57 ./vmware-root_940-2689209484/
drwx------ root/root         0 2021-10-11 12:28 ./vmware-root_950-2697008400/
```

**拆开tar的包**

```powershell
# 拆开tar包
[root@localhost tmp]# tar -xf allopt.tar
```

**指定目录解tar包**

```powershell
[root@localhost tmp]# tar -xf allopt.tar -C /opt/
```

**排除文件解包**

```powershell
# 注意--exclude 跟着文件名或是文件夹，不得加斜杠，排除多个文件，就写多个--exclude
[root@localhost tmp]# tar -ztvf 1012allopt.tar.gz
-rw-r--r-- root/root  21370880 2021-10-11 23:04 ./1011allopt.tar
-rw-r--r-- root/root  10680320 2021-10-10 21:57 ./allopt.tar
-rw-r--r-- root/root   8340359 2021-10-11 23:26 ./allopt.tar.gz
-rw-r--r-- root/root        31 2021-10-11 16:28 ./eco.txt
-rw-r--r-- root/root   9488895 2021-10-10 21:56 ./file.txt
-rw-r--r-- root/root   1188894 2021-10-10 21:55 ./quanquan.txt
[root@localhost tmp]# tar -zxvf 1012allopt.tar.gz --exclude quanquan.txt
[root@localhost tmp]# ls
1011allopt.tar  1012allopt.tar.gz  allopt.tar  allopt.tar.gz  eco.txt  file.txt
```

**打包/etc下所有普通文件**

```powershell
# 找到etc目录下所有的普通文件进行打包
[root@localhost tmp]# tar -zcvf etc_file.tar.gz `find /etc -type f`

# 查看已打包内容
[root@localhost tmp]# tar -tzvf etc_file.tar.gz
```

#### 4.3.6 **gzip命令**

要说tar命令是个纸箱子用于打包，gzip命令就是压缩机器

gzip通过压缩算法lempel-ziv 算法(lz77) 将文件压缩为较小文件，节省60%以上的存储空间，以及网络传输速率

```powershell
# gzip命令语法：
gzip(选项)(参数)

# 常用参数：
-a或——ascii# 使用ASCII文字模式；
-c或--stdout或--to-stdout 　# 把解压后的文件输出到标准输出设备。 
-d或--decompress或----uncompress # 解开压缩文件；
-f或——force # 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；
-h或——help # 在线帮助；
-l或——list # 列出压缩文件的相关信息；
-L或——license # 显示版本与版权信息；
-n或--no-name # 压缩文件时，不保存原来的文件名称及时间戳记；
-N或——name # 压缩文件时，保存原来的文件名称及时间戳记；
-q或——quiet # 不显示警告信息；
-r或——recursive # 递归处理，将指定目录下的所有文件及子目录一并处理；
-S或< # 压缩字尾字符串>或----suffix<压缩字尾字符串>：更改压缩字尾字符串；
-t或——test # 测试压缩文件是否正确无误；
-v或——verbose # 显示指令执行过程；
-V或——version # 显示版本信息；
-<压缩效率> # 压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；
--best # 此参数的效果和指定“-9”参数相同；
--fast # 此参数的效果和指定“-1”参数相同。
```

示例：

```powershell
# 压缩目录中每一个txt文件为.gz,文件夹无法压缩，必须先tar打包
gzip *.txt       # gzip压缩，解压都会删除源文件

# 列出压缩文件中信息
[root@localhost tmp]# gzip -l *.gz
         compressed        uncompressed  ratio uncompressed_name
            8498103            51077120  83.4% 1012allopt.tar
            
# 解压缩且显示过程
[root@localhost tmp]# gzip -dv *.gz
1012allopt.tar.gz:       83.4% -- replaced with 1012allopt.tar
[root@localhost tmp]# ls
1012allopt.tar
[root@localhost tmp]# du -h ./*
49M     ./1012allopt.tar

# 压缩保留源文件
[root@localhost tmp]# gzip -c 1012allopt.tar > all1012oopt.tar.gz
[root@localhost tmp]# du -h ./*
49M     ./1012allopt.tar
8.2M    ./all1012oopt.tar.gz
[root@localhost tmp]# ls
1012allopt.tar  all1012oopt.tar.gz
```

**gzip套件提供了许多方便的工具命令，可以直接操作压缩文件内容**

- zcat，直接读取压缩文件内容 `zcat hehe.txt.gz`。
- zgrep，直接过滤压缩文件内容。
- zless，分页显示压缩文件内容。
- zdiff，比较压缩文件的不同。

#### 4.3.7 **zip命令**

zip 命令：是一个应用广泛的跨平台的压缩工具，压缩文件的后缀为 zip文件，还可以压缩文件夹。

```powershell
# zip命令语法：
zip 压缩文件名  要压缩的内容

# 常用参数：
-A # 自动解压文件
-c # 给压缩文件加注释
-d # 删除文件
-F # 修复损坏文件
-k # 兼容 DOS
-m # 压缩完毕后，删除源文件
-q # 运行时不显示信息处理信息
-r # 处理指定目录和指定目录下的使用子目录
-v # 显示信息的处理信息
-x # “文件列表” 压缩时排除文件列表中指定的文件
-y # 保留符号链接
-b<目录> # 指定压缩到的目录
-i<格式> # 匹配格式进行压缩
-L # 显示版权信息
-t<日期> # 指定压缩文件的日期
-<压缩率> # 指定压缩率
```

示例：

```powershell
# 压缩当前目录下所有内容为alltmp.zip文件
[root@localhost tmp]# zip 1013zip_all.zip ./*

# 压缩多个文件夹
[root@localhost tmp]# zip -r data.zip ./data ./data2
```

#### 4.3.8 **unzip命令**

用于解压zip压缩包。

```powershell
# 常用参数：
-l # 显示压缩文件内所包含的文件；
-d<目录>  # 指定文件解压缩后所要存储的目录。
```

```powershell
# 查看压缩文件内容
[root@localhost tmp]# unzip -l vim_zip.zip

# 解压缩zip文件
[root@localhost tmp]# unzip vim_zip.zip
Archive:  vim_zip.zip
   creating: vmware-root_940-2689209484/
   creating: vmware-root_941-4022177618/
   creating: vmware-root_950-2697008400/
```

#### 4.3.9 **date命令**

date命令用于显示当前系统时间，或者修改系统时间。

```shell
# date命令语法：
date  参数   时间格式

# 常用参数:
-d, --date=STRING # 显示由 STRING 指定的时间, 而不是当前时间 

-s, --set=STRING  # 根据 STRING 设置时间 

-u, --utc, --universal # 显示或设置全球时间(格林威治时间)
```

时间格式

```powershell
%%	# 文本的 % 

%a	# 当前区域的星期几的简写 (Sun..Sat)

%A	# 当前区域的星期几的全称 (不同长度) (Sunday..Saturday) 

%b	# 当前区域的月份的简写 (Jan..Dec)  

%B	# 当前区域的月份的全称(变长) (January..December) 

%c  # 当前区域的日期和时间 (Sat Nov 04 12:02:33 EST 1989) 

%d  # (月份中的)几号(用两位表示) (01..31) 

%D	# 日期(按照 月/日期/年 格式显示) (mm/dd/yy) 

%e	# (月份中的)几号(去零表示) ( 1..31) 
    
%h	# 同 %b 
    
%H	# 小时(按 24 小时制显示，用两位表示) (00..23)
     
%I	# 小时(按 12 小时制显示，用两位表示) (01..12)
     
%j	# (一年中的)第几天(用三位表示) (001..366) 
    
%k	# 小时(按 24 小时制显示，去零显示) ( 0..23)
     
%l	# 小时(按 12 小时制显示，去零表示) ( 1..12)
     
%m	# 月份(用两位表示) (01..12) 
    
%M	# 分钟数(用两位表示) (00..59) 
    
%n	# 换行 
    
%p # 当前时间是上午 AM 还是下午 PM 
    
%r	# 时间,按 12 小时制显示 (hh:mm:ss [A/P]M) 
    
%s	# 从 1970年1月1日0点0分0秒到现在历经的秒数 (GNU扩充) 
    
%S	# 秒数(用两位表示)(00..60) 
    
%t	# 水平方向的 tab 制表符 
    
%T	# 时间,按 24 小时制显示(hh:mm:ss) 
    
%U	# (一年中的)第几个星期，以星期天作为一周的开始(用两位表示) (00..53) 
    
%V	# (一年中的)第几个星期，以星期一作为一周的开始(用两位表示) (01..52) 
    
%w	# 用数字表示星期几 (0..6); 0 代表星期天 
    
%W	# (一年中的)第几个星期，以星期一作为一周的开始(用两位表示) (00..53) 
    
%x	# 按照 (mm/dd/yy) 格式显示当前日期 
    
%X	# 按照 (%H:%M:%S) 格式显示当前时间 
    
%y	# 年的后两位数字 (00..99) 
    
%Y	# 年(用 4 位表示) (1970...) 
    
%z	#  按照 RFC-822 中指定的数字时区显示(如, -0500) (为非标准扩充) 
   
%Z	# 时区(例如, EDT (美国东部时区)), 如果不能决定是哪个时区则为空 
    
# 默认情况下,用 0 填充数据的空缺部分. GNU 的 date 命令能分辨在 `%'和数字指示之间的以下修改.

    `-' (连接号) 不进行填充 `_' (下划线) 用空格进行填充
```

示例：显示当前系统部分时间

```powershell
# 1.显示短年份
date +%y

# 2.显示长年份
date +%Y

# 3.显示月份
date +%m

# 4.显示几号
date +%d

# 5.显示几时
date +%H

# 6.显示几分
date +%M

# 7.显示整秒
date +%S

# 8.显示时间如，年-月-日
date +%F

# 9.显示时间如，时：分：秒
date +%T
```

示例：-d参数指定时间显示，仅仅是显示

```powershell
# 1.显示昨天
 date +%F -d "-1day"

# 2.显示昨天
date +%F -d "yesterday"

# 3.显示前天
date +%F -d "-2day"

# 4.显示明天日期
date +%F -d "+1day"

# 5.显示明天，英文表示
date +%F -d "tomorrow"

# 6.显示一个月之后
date +%F -d "1month"

# 6.显示一个月之前
date +%F -d "-1month"

# 7.显示一年后
date +%F -d "1year"

# 8.显示60分钟后
date +%T -d "60min"


#  +表示未来
#  -表示过去
#  day表示日
#  month表示月份
#  year表示年
#  min表示分钟
```

示例：-s设置时间

```shell
# 设置时间较少，一般配置ntp时间服务器

# 1.设置时间
[root@localhost tmp]# date -s "20211011"
2021年 10月 11日 星期一 00:00:00 CST
[root@localhost tmp]# date
2021年 10月 11日 星期一 00:00:27 CST

# 2.修改分钟
[root@localhost tmp]# date -s "17:01:26"
2021年 10月 11日 星期一 17:01:26 CST
[root@localhost tmp]# date
2021年 10月 11日 星期一 17:01:27 CST
[root@localhost tmp]#


# 3.修改日期和分钟
[root@localhost tmp]# date -s "20211013 17:08:25"
2021年 10月 13日 星期三 17:08:25 CST
[root@localhost tmp]# date
2021年 10月 13日 星期三 17:08:28 CST

# 4.可设置不同格式的时间
date -s "2018-06-06 05:30:30"
date -s "2018/07/07 05:30:30"
```

#### 4.4.0 **shred命令**

文件粉碎工具

```powershell
# shred命令语法：
用法：shred [选项]... 文件...

# 多次覆盖文件，使得即使是昂贵的硬件探测仪器也难以将数据复原。

# 常用参数：
-u, --remove 覆盖后截断并删除文件
shred heihei.txt  随机覆盖文件内容，不删除源文件
```

示例：彻底粉碎且删除文件

```powershell
[root@localhost tmp]# ls -lh
[root@localhost tmp]# shred -u quanquan.txt
```

### 4.4 Linux用户管理

Root用户登录系统后可以做很多事。

- 写文档
- 看电影
- 听音乐
- 聊微信
- 撸代码
- ......

当然了，完全可以坐在办公室，远程连接服务器工作。

多用户多任务：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\300.jpg)

多个用户使用同一个操作系统，每个人做自己的事；

每个人都有自己的账号密码，权限也不一样，好比老板权限最大，员工权限较低；

多用户大多都是远程登录去控制服务器；

Linux系统不同用户权限不一样，好比小张想用我的服务器，我为了保护隐私与资料安全，开通普通用户(useradd xiaozhang)，普通用户权限较低，随便他折腾了；

还有计算机程序默认创建的用户，如：ftp，nobody等等；

用户信息存放在/etc/passwd文件中；

用户角色划分：root、普通、虚拟用户

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\301.jpg)

现代操作系统一般属于多用户的操作系统，也就是说，同一台机器可以为多个用户建立账户，一般这些用户都是为普通用户，这些普通用户能同时登录这台计算机，计算机对这些用户分配一定的资源。

普通用户在所分配到的资源内进行各自的操作，相互之间不受影响。但是这些普通用户的权限是有限制的，且用户太多的话，管理就不便，从而引入root用户。

此用户是唯一的，且拥有系统的所有权限。root用户所在的组称为root组。

“组”是具有相似权限的多个用户的集合。

**root的权利**

Linux系统的特性就是可以满足多个用户，同时工作，因此Linux系统必须具备很好的安全性。

在安装RHEL7时设置的root管理员密码，这个root管理员就是所有UNIX系统中的超级用户，它拥有最高的系统所有权，能够管理系统的各项功能，如添加/删除用户，启动/关闭进程，开启/禁用硬件设备等等。

因此“能力越大，责任越大”，root权限必须很好的掌握，否则一个错误的命令可能会摧毁整个系统。

Linux/unix是一个多用户、多任务的操作系统。

root：默认在Unix/linux操作系统中拥有最高的管理权限。

普通用户：是管理员或者具备管理权限的用户所创建的，只能读、看，不能增、删、改。

**root的背景**

- UID，user Identify，好比身份证号
- GID，group Identify，好比户口本的家庭编号
- 在Linux系统中，用户也有自己的UID身份账号且唯一
- 在Linux中UID为0，就是超级用户，如要设置管理员用户，可以改UID为0，建议用sudo
- 系统用户UID为1~999 Linux安装的服务程序都会`创建独有的用户`负责运行。
- 普通用户UID从1000开始：由管理员创建（centos7），最大值1000~60000范围
- centos6创建普通用户是500开始

```shell
UID # 用户id号，身份证号
GID # 用户组id号，部门编号
root # 用户、组、id都为0，属于老板
```

**group组**

为了方便管理属于同一组的用户，Linux 系统中还引入了用户组的概念。通过使用用户组号码(GID，Group IDentification)，我们可以把多个用户加入到同一个组中，从而方便为组中的用户统一规划权限或指定任务。

假设有一个公司中有多个部门，每个部门中又 有很多员工。

如果只想让员工访问本部门内的资源，则可以针对部门而非具体的员工来设置权限。

例如：可以通过对技术部门设置权限，使得只有技术部门的员工可以访问公司的 数据库信息等。

Linux管理员在创建用户时，将自动创建一个与其同名的用户组，这个用户组只有该用户一个人。

**用户和用户组的关系**

- 一对一：一个用户可以存在一个组里，组里就一个成员
- 一对多：一个用户呆在多个组里面
- 多对一：多个用户在一个组里，这些用户和组有相同的权限
- 多对多：多个用户存在多个组里

**常用命令解释器**

```shell
/bin/sh # 默认 
/bin/bash # 默认
/sbin/nologin # 虚拟用户
/dash ubuntu 
csh unix
tsh unix
```

**用户信息配置文件**

/etc/passwd 文件内容

![image-20211013184552966](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211013184552966.png)

| 字段名      | 解释                                                         |
| ----------- | ------------------------------------------------------------ |
| 用户名      | 对应UID，是用户登录系统的名字，系统中唯一不得重复            |
| 用户密码    | 存放在/etc/shadow文件中进行保护密码                          |
| 用户UID     | 用户ID号，由一个整数表示                                     |
| 用户组GID   | 范围、最大值和UID一样，默认创建用户会创建用户组              |
| 用户说明    | 对此用户描述                                                 |
| 用户家目录  | 用户登录后默认进去的家目录，一般是【/home/用户名】           |
| shell解释器 | 当前登录用户使用的解释器。centos/redhat系统中，默认的都是bash。若是禁止此用户登录机器，改为/sbin/nologin即可 |

```shell
/etc/passwd  # 用户信息
/etc/shadow  # 用户密码信息
/etc/group   # 用户组信息
/etc/gshadow # 用户组密码信息
/etc/skel
```

**用户/密码文件权限**

```shell
# 用户信息文件，权限是644，所有人可读，有一定安全隐患
[root@localhost etc]# ll /etc/passwd
-rw-r--r--. 1 root root 979 10月 10 19:52 /etc/passwd

# 用户密码文件，除了root用户，其他用户默认是没有任何权限
[root@localhost etc]# ll /etc/shadow
----------. 1 root root 630 10月 10 19:52 /etc/shadow

# 用户密码文件
[root@localhost etc]# tail -5 /etc/shadow
tss:!!:18908::::::
abrt:!!:18908::::::
sshd:!!:18908::::::
postfix:!!:18908::::::
joker:!!:18910:0:99999:7:::
```

**用户组/组密码文件**

```shell
# 用户组件文件
[root@localhost etc]# tail -3 /etc/group
postdrop:x:90:
postfix:x:89:
joker:x:1000:

# 用户组密码文件
[root@localhost etc]# tail -3 /etc/gshadow
postdrop:!::
postfix:!::
joker:!::

# 对于大型服务器，用户和用户组数量较多，需要定制复杂的权限控制，会用到组密码
```

**用户管理的命令**

| 命令     | 作用                     |
| -------- | ------------------------ |
| useradd  | 创建用户                 |
| usermod  | 修改用户信息             |
| userdel  | 删除用户及配置文件       |
| passwd   | 更改用户密码             |
| chpasswd | 批量更新用户密码         |
| chage    | 修改用户密码属性         |
|          |                          |
| id       | 查看用户UID、GID、组信息 |
| su       | 切换用户                 |
| sudo     | 用root身份执行命令       |
| visudo   | 编辑sudoers配置文件      |

#### 4.4.1 **创建用户流程**

> - 1.useradd chaoge
>
> - 2.系统读取/etc/login.defs（用户定义文件），和/etc/default/useradd（用户默认配置文件）俩文件中定义的规则创建新用户
> - 3.向/etc/passwd和/etc/group文件中添加用户和组信息，向/etc/shadow和/etc/gshadow中添加密码信息
> - 4.根据/etc/default/useradd文件中配置的信息创建用户家目录
> - 5.把/etc/skel中所有的文件复制到新用户家目录中

文件/etc/login.defs

```shell
[root@localhost skel]# grep -v "^#" /etc/login.defs |grep -v "^$"
MAIL_DIR    /var/spool/mail        #用户的邮件存放位置
PASS_MAX_DAYS    99999                    #密码最长使用天数
PASS_MIN_DAYS    0                            #更换密码最短时间
PASS_MIN_LEN    8                        #密码最小长度
PASS_WARN_AGE    7                            #密码失效前几天开始报警
UID_MIN                  1000        #UID开始位置
UID_MAX                 60000        #UID结束位置
SYS_UID_MIN               201        
SYS_UID_MAX               999
GID_MIN                  1000
GID_MAX                 60000
SYS_GID_MIN               201
SYS_GID_MAX               999
CREATE_HOME    yes                    #是否创建家目录
UMASK           077            #家目录的umask值
USERGROUPS_ENAB yes
ENCRYPT_METHOD MD5            #密码加密算法
MD5_CRYPT_ENAB yes
```

文件/etc/default/useradd

```shell
[root@localhost skel]# grep -v "^#" /etc/default/useradd | grep -v "^$"
GROUP=100        
HOME=/home                    #在/home目录下创建家目录
INACTIVE=-1                    #开启用户过期
EXPIRE=                            #用户终止日期
SHELL=/bin/bash            #新用户默认解释器
SKEL=/etc/skel            #用户环境变量文件存放目录
CREATE_MAIL_SPOOL=yes
```

用户创建家目录过程

```shell
# 1.命令创建
useradd joker

# 2.系统把/etc/skel目录下的内容复制到创建的用户家目录
[root@localhost skel]# pwd
/etc/skel
[root@localhost skel]# ls -la
总用量 24
drwxr-xr-x.  2 root root   62 10月  8 11:54 .
drwxr-xr-x. 79 root root 8192 10月 13 18:46 ..
-rw-r--r--.  1 root root   18 4月   1 2020 .bash_logout
-rw-r--r--.  1 root root  193 4月   1 2020 .bash_profile
-rw-r--r--.  1 root root  231 4月   1 2020 .bashrc


# 3.修改权限拥有者
[root@localhost skel]# ls /home/joker/ -la
总用量 12
drwx------. 2 joker joker  62 10月 10 19:52 .
drwxr-xr-x. 3 root  root   19 10月 10 19:52 ..
-rw-r--r--. 1 joker joker  18 4月   1 2020 .bash_logout
-rw-r--r--. 1 joker joker 193 4月   1 2020 .bash_profile
-rw-r--r--. 1 joker joker 231 4月   1 2020 .bashrc
```

创建用户有关的目录/etc/skel

此目录存放新用户需要的基础环境变量文件，添加新用户的时候，这个目录下所有文件自动被复制到新家目录下，且默认是隐藏文件，以点开头。

```shell
[root@localhost skel]# ls -la /etc/skel/
总用量 24
drwxr-xr-x.  2 root root   62 10月  8 11:54 .
drwxr-xr-x. 79 root root 8192 10月 13 18:46 ..
-rw-r--r--.  1 root root   18 4月   1 2020 .bash_logout
-rw-r--r--.  1 root root  193 4月   1 2020 .bash_profile
-rw-r--r--.  1 root root  231 4月   1 2020 .bashrc
```

**面试题：如何恢复用户命令提示符**

```shell
# 1.正常切换用户
su - joker

[root@localhost skel]# su - joker
[joker@localhost ~]$

# 2.删除用户家目录下环境变量文件
[joker@localhost ~]$ rm -rf .bash*

# 3.退出登录，再次登录，发现命令提示符故障
[joker@localhost ~]$ exit
登出
[root@localhost skel]# su - joker
上一次登录：三 10月 13 23:52:35 CST 2021pts/0 上
-bash-4.2$

# 4.恢复方法
-bash-4.2$ cp /etc/skel/.bash* ~/
-bash-4.2$ exit
登出

[root@localhost skel]# su - joker
上一次登录：三 10月 13 23:54:38 CST 2021pts/0 上
[joker@localhost ~]$
```

#### 4.4.2 **useradd命令**

useradd命令用于Linux中创建的新的系统用户。

useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码，而可用userdel删除帐号。

使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。

```shell
# 常用参数：
-c<备注> # 加上备注文字。备注文字会保存在passwd的备注栏位中；
-d<登入目录> # 指定用户登入时的启始目录；
-D # 变更预设值；
-e<有效期限> # 指定帐号的有效期限；
-f<缓冲天数> # 指定在密码过期后多少天即关闭该帐号；
-g<群组> # 指定用户所属的群组；
-j<群组> # 指定用户所属的附加群组；
-m # 自动建立用户的登入目录；
-M # 不要自动建立用户的登入目录；
-n # 取消建立以用户名称为名的群组；
-r # 建立系统帐号；
-s<shell> # 指定用户登入后所使用的shell；
-u<uid> # 指定用户id。
```

示例： 创建普通用户

```shell
# 创建ike用户
[root@localhost ~]# useradd ike

# 查看家目录
[root@localhost ~]# ls -l /home/
总用量 0
drwx------. 2 ike   ike   62 10月 14 00:05 ike
drwx------. 2 joker joker 83 10月 13 23:57 joker

# grep过滤 -w必须全字符串匹配
[root@localhost ~]# grep -w ike /etc/passwd
ike:x:1001:1001::/home/ike:/bin/bash

# 查看用户组密码文件
[root@luffycity ~]# grep -w chaoge  /etc/shadow 
chaoge:!!:18186:0:99999:7:::

# 查看用户组文件
[root@localhost ~]# grep -w ike /etc/group
ike:x:1001:

# 默认没密码，通过passwd设置密码
[root@localhost ~]# passwd ike

# 查看用户信息
[root@localhost ~]# id ike
uid=1001(ike) gid=1001(ike) 组=1001(ike)
[root@localhost ~]#

# 查看用户组密码文件
[root@localhost ~]# grep -w ike /etc/shadow
ike:$6$lkwqHByJ$Z3FZQRRSPXgvCOmMB3a7E3RFP5Z5V2SuC98Pu73TV3Ok6E/VW4Zw8dSlyKvTFY25/nPpyLVpVz4k9ZVjRHdfY1:18913:0:99999:7:::
```

**用户指定uid和属组**

```shell
# 1.创建用户组
[root@localhost ~]# groupadd -g 1014 iker

# 2.创建新用户
root@localhost ~]# useradd -g iker -u 20211014 like

# 3.检查用户信息
[root@localhost ~]# id like
uid=20211014(like) gid=1014(iker) 组=1014(iker)
```

**-M -s参数用法**

```shell
# 创建用户禁止登陆，且不创建家目录
[root@localhost ~]# useradd -M -s /sbin/nologin test1
[root@localhost ~]# grep -w test1 /etc/passwd
test1:x:1002:1002::/home/test1:/sbin/nologin
[root@localhost ~]# ls /home/  # 没有test1用户
ike  joker  like
```

**多个参数用法，指定用户信息**

```shell
[root@localhost ~]# useradd -u 789 -s /bin/sh -c learn_linux -j root,ike -e "2021-10-15" -f 2 -d /tmp/learn_linux_test2  test2
[root@localhost ~]# id test2
uid=789(test2) gid=1015(test2) 组=1015(test2),0(root),1001(ike)
```

![image-20211014004622193](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211014004622193.png)

**-D参数用来修改配置文件 `/etc/default/useradd` 文件的默认值**

```shell
# 语法：
useradd -D 参数选项

# 参数：
-e default_expire_date 用户停止日期
-s default_shell  用户登录后使用的解释器
```

示例：

```shell
# 检查默认创建用户的登录解释器
[root@localhost bin]# grep -i "shell" /etc/default/useradd
SHELL=/bin/bash

# 修改默认解释为禁止登录
[root@localhost ~]# useradd -D -s /sbin/nologin
[root@localhost ~]#

# 检查配置文件参数
[root@localhost ~]# grep -i "shell" /etc/default/useradd
SHELL=/sbin/nologin

# 创建新用户
[root@localhost ~]# useradd testnolog

# 检查信息，发现是禁止登录
[root@localhost ~]# grep "testnolog" /etc/passwd
testnolog:x:1003:1003::/home/testnolog:/sbin/nologin

# 改回去，防止后边会出错
[root@localhost ~]# useradd -D -s /bin/bash

# 验证是否修改回来
[root@localhost ~]# grep -i "shell" /etc/default/useradd
SHELL=/bin/bash
```

#### 4.4.3 **usermod命令**

usermod命令用于修改系统已存在的用户信息，只能修改未使用中的用户。

```shell
# 命令语法：
usermod(选项)(参数)

# 常用参数：
-c<备注>  # 修改用户帐号的备注文字；
-d<登入目录>  # 修改用户登入时的目录；
-e<有效期限>  # 修改帐号的有效期限；
-f<缓冲天数>  # 修改在密码过期后多少天即关闭该帐号；
-g<群组>  # 修改用户所属的群组；
-j<群组>  # 修改用户所属的附加群组；
-l<帐号名称>  # 修改用户帐号名称；
-L  # 锁定用户密码，使密码无效；
-s<shell>  # 修改用户登入后所使用的shell；
-u<uid>  # 修改用户ID；
-U  # 解除密码锁定。
```

示例：

```shell
[root@localhost ~]# usermod -u 788 -s /sbin/nologin -c changuser -j testnolog -e "2021/10/15" -f 2 -d /home/testnolog/ testnolog
[root@localhost ~]#
[root@localhost ~]# id testnolog
uid=788(testnolog) gid=1003(testnolog) 组=1003(testnolog)
[root@localhost ~]#
[root@localhost ~]# grep -w testnolog /etc/passwd
testnolog:x:788:1003:changuser:/home/testnolog/:/sbin/nologin
```

#### 4.4.4 **userdel命令**

删除用户与相关组件

- 建议注释 `/ect/passwod` 用户信息而非直接删除用户

```shell
# userdel命令语法：
userdel(选项)(参数)

# 常用参数：
-f：强制删除用户，即使用户当前已登录；
-r：删除用户的同时，删除与用户相关的所有文件。
```

示例：

```shell
# 保留家目录
[root@localhost ~]# userdel testnolog

# 强制删除用户与家目录
[root@localhost home]# rm -rf like
```

#### 4.4.5 **groupadd命令**

此命令用于创建一个新的工作组，新工作组的信息将被添加到系统文件中。

```shell
# groupadd命令语法：
groupadd - 建立新群组
groupadd [ -ggid [ -o ]] [ -r ] [ -f ] group [[ ]]

# 常用参数：
-g # 指定新建工作组的id；
-r # 创建系统工作组，系统工作组的组ID小于500；
-K # 覆盖配置文件“/ect/login.defs”；
-o # 允许添加组ID号不唯一的工作组。
```

示例：

```shell
# 创建用户组，指定id801
[root@localhost home]# groupadd -g 801 old

# 查看你组信息
[root@localhost home]# cat /etc/group | grep "old"
old:x:801:
```

#### 4.4.6 **groupdel命令**

删除用户组

```shell
[root@localhost home]# groupdel old
[root@localhost home]# cat /etc/group | grep "old"
```

#### 4.4.7 passwd命令

passwd命令修改用户密码和过期时间等，root可以改普通用户，反之不可以

```shell
# passwd命令语法：
passwd(选项)(参数)

# 常用参数
-d  # 删除密码，仅有系统管理者才能使用；
-f  # 强制执行；
-k  # 设置只有在密码过期失效后，方能更新；
-l  # 锁住密码；
-s  # 列出密码的相关信息，仅有系统管理者才能使用；
-u  # 解开已上锁的帐号。
-i  # 密码过期多少天后禁用账户
-x  # 设置x天后可以修改密码
-n  # 设置n天内不得改密码
-e  # 密码立即过期，强制用户修改密码
-w  # 用户在密码过期前收到警告信息的天数
```

示例：改自己的密码

```shell
# 修改当前登录用户的密码
[root@localhost home]# passwd
更改用户 root 的密码 。
新的 密码：
无效的密码： 密码少于 8 个字符
重新输入新的 密码：
passwd：所有的身份验证令牌已经成功更新。
```

示例：修改普通用户密码

```shell
# 修改普通用户的密码
[root@localhost home]# passwd ike
更改用户 ike 的密码 。
新的 密码：
无效的密码： 密码未通过字典检查 - 过于简单化/系统化
重新输入新的 密码：
passwd：所有的身份验证令牌已经成功更新。
```

示例：列出用户密码信息

```shell
[root@localhost home]# passwd -S ike
ike PS 2021-10-14 0 99999 7 -1 (密码已设置，使用 SHA512 算法。)
```

示例：一条命令设置密码，企业常用

```shell
# --stdin从标准输入中获取123123
[root@localhost home]# echo "123321" | passwd --stdin ike
更改用户 ike 的密码 。
passwd：所有的身份验证令牌已经成功更新。
```

示例：实际场景

```shell
# 7天内用户不得改密码，60天后可以修改，过期前10天通知用户，过期30天后禁止用户登录
[root@localhost home]# passwd -n 7 -x 60 -w 10 -i 30 ike
调整用户密码老化数据ike。
passwd: 操作成功
[root@localhost home]#
[root@localhost home]# passwd -S ike
ike PS 2021-10-14 7 60 10 30 (密码已设置，使用 SHA512 算法。)
```

示例：批量更新新密码

```shell
# 1.查看当前机器的用户信息
tail /etc/passwd 

# 2.批量改密码，ctrl+d结束输入
[root@localhost home]# chpasswd
joker:123456
ike:456789
like:123456
```

#### 4.4.8 **id命令**

id命令用于检查用户和组以及对应的UID，GID等信息。

```shell
[root@localhost home]# id joker
uid=1000(joker) gid=1000(joker) 组=1000(joker)

# 显示用户id
[root@localhost home]# id -u joker
1000
# 显示组id
[root@localhost home]# id -g joker
1000
# 显示用户名
[root@localhost home]# id -un joker
joker
# 显示组名
[root@localhost home]# id -gn joker
joker
```

#### 4.4.9 **whoami命令**

whoami显示可用于查看当前登录的用户。

```shell
[root@localhost home]# whoami
root
```

w命令显示当前已登录的用户。

```shell
[root@localhost home]# w
 16:59:57 up  4:29,  2 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     tty1                      12:30    4:29m  0.11s  0.11s -bash
root     pts/0    192.168.254.1    12:30    5.00s  0.79s  0.05s w

# 1.显示当前系统时间、系统从启动到运行的时间、系统运行中的用户数量和平均负载（1、5、15分钟平均负载）
# 2.第二行信息
    user：用户名
    tty:用户使用的终端号
    from：表示用户从哪来，远程主机的ip信息
    login：用户登录的时间和日期
    IDLE：显示终端空闲时间
    JCPU：该终端所有进程以及子进程使用系统的总时间
    PCPU：活动进程使用的系统时间
    WHAT：用户执行的进程名称
```

who命令

```shell
[root@localhost home]# who
root     tty1         2021-10-14 12:30
root     pts/0        2021-10-14 12:30 (192.168.254.1)

# 用户名称  用户终端     登录时间			从哪来的机器ip
```

#### 4.5.0 **last&lastlog命令**

查看用户详细的登录信息。

```shell
# last命令显示已登录的用户列表和登录时间
[root@localhost home]# last
root     pts/0        192.168.254.1    Thu Oct 14 12:30   still logged in
root     tty1                          Thu Oct 14 12:30   still logged in
reboot   system boot  3.10.0-1160.el7. Thu Oct 14 12:30 - 17:03  (04:33)
root     pts/0        192.168.254.1    Thu Oct 14 00:05 - 00:58  (00:53)
```

```shell
# lastlog命令显示当前机器所有用户最近的登录信息
[root@localhost home]# lastlog
用户名           端口     来自             最后登陆时间
root             pts/0    192.168.254.1    四 10月 14 12:30:58 +0800 2021
joker            pts/0                     四 10月 14 15:30:34 +0800 2021
ike              pts/0                     四 10月 14 15:30:14 +0800 2021
like             pts/0                     四 10月 14 14:40:20 +0800 2021
```

#### 4.5.1 Linux用户身份切换命令

**su命令**

su命令用于切换到指定用户。

```shell
# su命令语法：
su(选项)(参数)

# 常用参数：
-c<指令>或--command=<指令> # 执行完指定的指令后，即恢复原来的身份；
-f或——fast # 适用于csh与tsch，使shell不用去读取启动文件；
-l或——login # 改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,logname。此外，也会变更PATH变量；
-m,-p或--preserve-environment # 变更身份时，不要变更环境变量；
-s<shell>或--shell=<shell> # 指定要执行的shell；
--help # 显示帮助；
--version # 显示版本信息。
```

示例：不推荐版

```shell
# root用户切换，直接切换，没有完全切换环境变量
[root@localhost home]# su joker
[joker@localhost home]$ env|egrep "USER|MAIL|PWD|LOGNAME"
USER=joker
MAIL=/var/spool/mail/root
PWD=/home
LOGNAME=joker

```

示例：标准版

```shell
# root用户切换普通用户
[root@localhost ~]# su - joker
上一次登录：四 10月 14 17:15:51 CST 2021pts/0 上
[joker@localhost ~]$ pwd
/home/joker
[joker@localhost ~]$ env|egrep "USER|MAIL|PWD|LOGNAME"
USER=joker
MAIL=/var/spool/mail/joker
PWD=/home/joker
LOGNAME=joker

# 普通用户切换到其他用户，需要哦输入密码
[joker@localhost ~]$ su - ike
密码：
上一次登录：四 10月 14 15:30:14 CST 2021pts/0 上
最后一次失败的登录：四 10月 14 17:19:34 CST 2021pts/0 上
最有一次成功登录后有 1 次失败的登录尝试。
```

#### 4.5.1 **sudo命令**

sudo命令用来以其他身份来执行命令，预设的身份为root。在`/etc/sudoers`中设置了可执行sudo指令的用户。

普通用户不需要root密码即可用root权限执行命令。

```shell
# sudo命令语法：
sudo(选项)(参数)

# 常用参数：
-b # 在后台执行指令；
-h # 显示帮助；
-H # 将HOME环境变量设为新身份的HOME环境变量；
-k # 结束密码的有效期限，也就是下次再执行sudo时便需要输入密码；。
-l # 列出目前用户可执行与无法执行的指令；
-p # 改变询问密码的提示符号；
-s<shell> # 执行指定的shell；
-u<用户> # 以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份；
-v # 延长密码有效期限5分钟；
-V # 显示版本信息。
```

示例：

```shell
# 使用sudo命令
[ike@localhost ~]$su - joker
[joker@localhost ~]$ ls /root
ls: 无法打开目录/root: 权限不够
[joker@localhost ~]$ sudo ls /root
anaconda-ks.cfg
```

- 配置了/etc/sudoers文件后，可以对用户命令提权，sudo 命令
- 想要切换root执行操作，可以sudo su - ，需要输入当前用户密码

#### 4.5.2 **visudo命令**

visudo用于编辑/etc/sudoers文件，且提供语法检测，用于配置sudo命令。

示例：给joker用户配置sudo使用权

```shell
# 直接输入visudo命令，相当于打开vim /etc/sudoers
97 ## The COMMANDS section may have other options added to it.
98 ##
99 ## Allow root to run any commands anywhere
100 root    ALL=(ALL)       ALL

# 2.添加你想让执行sudo命令的用户
101 joker   ALL=(ALL)       ALL
```

sudo配置文件

| 用户或组 | 机器=（角色） | 允许执行命令                         |
| -------- | ------------- | ------------------------------------ |
| User     | machine=      | Commands                             |
| oldboy   | ALL=(ALL)     | /usr/sbin/useradd、/usr/sbin/userdel |

**配置sudo目的在于即能让运维方便干活（权限不足问题），又不威胁系统安全（权限把控）**

### 4.5 Linux文件/夹权限

![image-20211014180130276](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211014180130276.png)

**文件权限**

> ```shell
> r   # read可读，可以用cat等命令查看
> w   # write写入，可以编辑或者删除这个文件
> x   # executable    可以执行
> -   # 没有权限
> ```

**文件夹权限**

> ```shell
> r    # 可以对此目录执行ls列出所有文件
> w    # 可以在这个目录创建文件
> x    # 可以cd进入这个目录，或者查看详细信息
> ```

**文件权限与数字转化**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\315.png)

```shell
# rwx权限表示
r read 	 	# 读取 4
w  write 	# 写  2
x  execute  # 执行 1
-           # 无权限  0常见权限对应
```

示例：常见权限对应

```shell
444 r--r--r--
600 rw-------
644 rw-r--r--
666 rw-rw-rw-
700 rwx------
744 rwxr--r--
755 rwxr-xr-x
777 rwxrwxrwx
```

#### 4.5.1 **chmod命令**

chmod命令用来变更文件或目录的权限。

在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。

用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。

符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\316.jpg" alt="img" style="zoom:50%;" />

```shell
# 权限范围：
# 1.操作对像
   u # 文件属主权限
   g # 同组用户权限
   o # 其它用户权限
   a # 所有用户（包括以上三种）

# 2.权限设定
   + # 增加权限
   - # 取消权限
   = # 唯一设定权限

# 3.权限类别
   r # 读权限
   w # 写权限
   x # 执行权限
   X # 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。
   s # 文件属主和组id
   i # 给文件加锁，使其它用户无法访问
   
# 对应数字
r-->4
w-->2
x-->1
```

示例：

```shell
# 1.设置文件所有人可读
# 第一种写法
[root@localhost tmp]#chmod ugo+r vim_zip.zip
# 第二种写法
[root@localhost tmp]#chmod a+r vim_zip.zip

# 2.文件属主和属组内成员可写，其他人不可写
[root@localhost tmp]chmod ug+w,o-w vim_zip.zip

# 3.文件属主可以执行
[root@localhost tmp]chmod u+x vim_zip.zip

# 4.设置data目录所有资料，任何人可读取
[root@localhost /]# chmod  -R  a+w /opt/  # -R 以递归方式更改所有的文件及子目录

# 5.所有人都可以读、写、执行
chmod 777 filename.txt
```

**使用数字化权限转换**

```shell
# u：读写执行  g：写执行  o：写执行
chmod 755 alex.txt

# u：读写 g：读  o：读
chmod 644 alex.txt

# ugo：读写执行
chmod 777 alex.txt

# 所有人无权限
chmod 0 alex.txt

# u是0权限，group权限6，其他人权限5
chmod 65  alex.txt  

# 其实是004，只有其他人可读
chmod 4    alex.txt        
```

**实际效果，rwx对于文件的操作**

| 权限 | 效果                                                         |
| ---- | ------------------------------------------------------------ |
| r    | 可以读取、阅读文件内容                                       |
| w    | 可以新增、修改文件内容，删除文件的权限，由父目录权限控制，删除权限由父目录的w控制 |
| x    | 1.可读取文件内容并执行。如：<br />     . luffy.sh <br />     source luffy.sh <br />     sh luffy.sh <br />2.普通用户必须对文件有r权限才可执行 <br />3.root只需要任意u,g,o存在x权限即可执行 |

**rwx对于目录的权限**

| 权限 | 效果                                                         |
| ---- | ------------------------------------------------------------ |
| r    | 1.可以浏览目录下内容，如：ls<br />2.没有x无法进入目录<br />3.可以看到目录下文件名，无法访问文件信息（内容，权限） |
| w    | 【有x权限前提下】文件夹有增加、删除、修改文件夹中文件名的权限 |
| x    | 1.进入文件夹，如：cd <br />2.没有r无法查看列表 3.没有w无法创建文件 |

#### 4.5.2 **chown命令**

修改文件属主、属组信息

```shell
# chown命令语法：

chown alex   test.txt    # 文件属于alex
chown :组     test.txt   # 修改文件属组
chown 用户:组             # 修改

# 常用参数
-R, --recursive        	 # 递归处理所有的文件及子目录
-v, --verbose            # 为处理的所有文件显示诊断信息
```

示例：

```shell
# 1.修改 1011allopt.tar 文件的所属主，所属组为ike
[root@localhost tmp]# chown ike:ike 1011allopt.tar

# 2.将t3目录下所有内容属主和属组改为ike
[root@localhost tmp]# mkdir -p t1/t2/t3
[root@localhost tmp]# mv 1012allopt.tar ./t1/t2/t3/
[root@localhost tmp]# chown -R ike:ike t1/t2/t3/
[root@localhost tmp]# ll /tmp/t1/t2/t3/
总用量 49880
-rw-r--r--. 1 ike ike 51077120 10月 12 09:43 1012allopt.tar

# 3.修改 all1012oopt.tar.gz 文件的所属组为ike
[root@localhost tmp]# chown .ike all1012oopt.tar.gz
[root@localhost tmp]# ll
-rw-r--r--. 1 root ike   8498118 10月 13 09:28 all1012oopt.tar.gz
```

#### 4.5.3  **chgrp命令**

chgrp命令用来改变文件或目录所属的用户组。

该命令用来改变指定文件所属的用户组。

其中，组名可以是用户组的id，也可以是用户组的组名。

```shell
# 常用参数：
-c或——changes # 效果类似“-v”参数，但仅回报更改的部分；
-f或--quiet或——silent # 不显示错误信息；
-h或--no-dereference # 只对符号连接的文件作修改，而不是该其他任何相关文件；
-R或——recursive # 递归处理，将指令目录下的所有文件及子目录一并处理；
-v或——verbose # 显示指令执行过程；
--reference=<参考文件或目录> # 把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同；
```

示例：

```shell
# 把/data目录下所有文件的属组改为alex
chgrp -R  alex  /data   
```

#### 4.5.4 umask命令

umask 命令用来限制新文件权限的掩码。

也称之为遮罩码，防止文件、文件夹创建的时候，权限过大

当新文件被创建时，其最初的权限由文件创建掩码决定。

用户每次注册进入系统时，umask命令都被执行，并自动设置掩码改变默认值，新的权限将会把旧的覆盖。

**Linux默认权限**

> - 文件：644 - rw- r-- r--
> - 目录：655 -rwx r-x r-x

- umask默认配置在/etc/profile 61-64行

> ```shell
> umask值就是指“Linux文件的默认属性需要减掉的权限”。比如：
> 1.Linux普通文件的最大默认属性是666，目录文件的最大属性是777。
> 2.但是我们不想要用户在新建立文件时，文件的属性是666或777，那么我们就要设置umask值。
> 3.Linux系统预置的umask值是022，那么用户在新建立普通文件时，普通文件的属性就是 666-022=644，新建立目录文件时，目录文件的属性就是 777-022=755。
> ```

```shell
# 语法参数

-S # 以字符的形势显示当前的掩码。
-p # 带umask开头以数字的形势显示当前掩码
```

**计算umask文件权限**

系统默认umask数值对应的权限。

> 公式：
>
> 默认的文件、文件夹权限，减去umaks的值等于最终的权限值
>
> 文件、文件夹最大权限是777

示例：默认umask值，root和普通用户不一样

```shell
# root用户umask
# 查看当前用户预设权限
[root@localhost tmp]# umask
0022

# 以字母的形势显示权限，7-0 7-2 7-2
[root@localhost tmp]# umask -S
u=rwx,g=rx,o=rx

[root@localhost tmp]# umask -p
umask 0022

# 普通用户umask
[joker@localhost ~]$ umask
0002

[joker@localhost ~]$ umask -S
u=rwx,g=rwx,o=rx

[joker@localhost ~]$ umask -p
umask 0002
```

示例：root登录系统默认创建文件，文件夹的权限

```shell
[root@localhost tmp]# touch 1015file.txt
[root@localhost tmp]# mkdir test
[root@localhost tmp]# ll
总用量 0
-rw-r--r--. 1 root root 0 10月 15 10:43 1015file.txt		# 644
drwxr-xr-x. 2 root root 6 10月 15 10:44 test				# 755

```

示例：普通登录系统默认创建文件，文件夹的权限

```shell
[joker@localhost tmp]$ touch filemin.txt
[joker@localhost tmp]$ mkdir tsetmin
[joker@localhost tmp]$ ll
-rw-rw-r--. 1 joker joker 0 10月 15 10:49 filemin.txt  # 664
drwxrwxr-x. 2 joker joker 6 10月 15 10:49 tsetmin	  # 775
```

**计算文件umask权限**

- 操作系统创建文件，默认最大权限是666(-rw-rw-rw-)
- 创建普通文件权限是644(-rw-r--r--)

```shell
# 文件默认起始权限666，减去umask默认值022，等于创建文件的权限644
666
022
-----
644   # 系统创建文件权限默认是644   -  rw-  r-- r--
```

- umask值不得大于6
- umask计算出的文件权限不得拥有执行权限x，如果计算结果中有执行权限，则需要将其加一

```shell
# 文件默认权限666，假设umask值设置为045
666
045
-----
621   # 存在执行权限，必须+1，默认文件不得执行
----
622   # 最终文件权限
```

示例：

```shell
# 临时修改默认umask变量值
[joker@localhost tmp]$ umask 045
[joker@localhost tmp]$ umask
0045

# 创建文件，查看权限，此时默认已经是622的权限
[joker@localhost tmp]$ touch 045file.txt
[joker@localhost tmp]$ ll
总用量 0
-rw--w--w-. 1 joker joker 0 10月 15 11:45 045file.txt
```

**计算目录umask权限**

- 操作系统创建文件夹，默认最大权限是777(-rwxrwxrwx)

```shell
# 目录最大权限777，减去umask值，得到目录创建后的权限
777
022
------
755
```

示例：

```shell
[joker@localhost tmp]$ umask
0002
[joker@localhost tmp]$ mkdir joker_test
[joker@localhost tmp]$ ll
drwxrwxr-x. 2 joker joker 6 10月 15 12:06 joker_test
```

#### 4.5.5 **lsattr命令**

lsattr命令用于查看文件的第二扩展文件系统属性，结合chattr一起用。

```shell
# 常用参数
-R  # 递归地列出目录以及其下内容的属性.  
-V  # 显示程序版本.
-a  # 列出目录中的所有文件,包括以`.'开头的文件的属性.
-d  # 以列出其它文件的方式那样列出目录的属性, 而不列出其下的内容.
-v  # 显示文件版本
```

#### 4.5.6 **chattr命令**

chattr命令用于更改文件的扩展属性，比chmod更改的rwx权限更底层。

```shell
# 常用参数：

a  # 只能向文件中添加数据，不得删除
-R # 递归更改目录属性
-V # 显示命令执行过程

# 常用模式：
+ # 增加参数
- # 移除参数
= # 更新为指定参数、

A # 不让系统修改文件最后访问时间
a # 只能追加文件数据，不得删除
i # 文件不能被删除、改名、修改内容
```

示例：文件设置i的权限

```shell
# 加i后不能修改内容，不能删除文件
[root@localhost tmp]# chattr +i 1015file.txt
[root@localhost tmp]# lsattr 1015file.txt
----i----------- 1015file.txt
[root@localhost tmp]# rm -rf 1015file.txt
rm: 无法删除"1015file.txt": 不允许的操作
[root@localhost tmp]# echo abc > 1015file.txt
-bash: 1015file.txt: 权限不够

# 减去i后可以修改内容，可以删除
[root@localhost tmp]# echo "abc" > 1015file.txt
[root@localhost tmp]#
[root@localhost tmp]# cat 1015file.txt
abc
[root@localhost tmp]# rm -rf 1015file.txt
```

示例：a参数，只能追加内容

```shell
# 加a后不能删除文件，只能追加内容
[root@localhost tmp]# chattr +a 022file.txt
[root@localhost tmp]# lsattr  022file.txt
-----a---------- 022file.txt
[root@localhost tmp]# rm -rf 022file.txt
rm: 无法删除"022file.txt": 不允许的操作
[root@localhost tmp]# echo "123456" > 022file.txt
-bash: 022file.txt: 不允许的操作
[root@localhost tmp]# echo "123456" >> 022file.txt
s
# 减去a后可以删除文件
[root@localhost tmp]# lsattr  022file.txt
---------------- 022file.txt
[root@localhost tmp]# rm -f 022file.txt
```

#### 4.5.7 **防火墙**

用于控制服务器的出/入流量。

防止恶意流量攻击服务器，保护服务器的端口等服务。

在学习阶段是直接关闭的，专业的运维人员需要学习iptables软件的一些指令。

```shell
# 云服务器，默认都有一个硬件防火墙，以及软件防火墙（iptables、firewalld）
# 我在服务器上，运行了django服务，如果开启了防火墙服务器，且没有自定义规则，默认是拒绝所有外来流量 ，导致我们windows无法访问到linux运行的django等程序

# 1.清空防火墙规则
[root@localhost tmp]# iptables -F
iptables -F

# 2.关闭防火墙的服务
[root@localhost tmp]# systemctl status firewalld  # 对防火墙状态进行检车确认关闭
[root@localhost tmp]# systemctl stop firewalld  # 关闭防火墙服务
[root@localhost tmp]# systemctl disable firewalld  # 禁止防火墙开机自启
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
```

#### 4.5.8 **DNS域名解析**

什么是dns解析？

```shell
# dns服务器，存储了公网注册的所有（域名----ip）对应的解析关系
# linux的dns客户端配置文件 /etc/resolv.conf 
[root@localhost tmp]# cat /etc/resolv.conf     # 全部注释后再ping baidu.com
; generated by /usr/sbin/dhclient-script
#search localdomain
#nameserver 192.168.254.2

#nameserver 119.29.29.29
#nameserver 233.5.5.5
[root@localhost tmp]# ping baidu.com  # 百度ping不通,无法进行域名解析
ping: baidu.com: 未知的名称或服务
[root@localhost tmp]# ping 123.206.16.61  # ping公网可以通
PING 123.206.16.61 (123.206.16.61) 56(84) bytes of data.
64 bytes from 123.206.16.61: icmp_seq=1 ttl=128 time=11.5 ms

[root@localhost tmp]# cat /etc/resolv.conf     # 把定义的两个dns服务地址放开后可以进行解析域名
; generated by /usr/sbin/dhclient-script
#search localdomain
#nameserver 192.168.254.2

nameserver 119.29.29.29
nameserver 233.5.5.5
```

示例：**/ect/hosts 比 /etc/resolv.conf 域名解析优先级更高**

```shell
# linux的本地dns强制解析文件  /etc/hosts，可以写入一些测试的域名，用于本地机器使用。
[root@localhost tmp]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

127.0.0.1 baidu.com
[root@localhost tmp]# ping baidu.com
PING baidu.com (127.0.0.1) 56(84) bytes of data.
64 bytes from localhost (127.0.0.1): icmp_seq=1 ttl=64 time=0.015 ms
```

#### 4.5.9 **nslookup命令**

域名查找命令，寻找域名的对应关系

>  安装命令：yum -y install bind-utils

```powershell
[root@localhost tmp]# nslookup
> baidu.com
Server:         119.29.29.29
Address:        119.29.29.29#53

Non-authoritative answer:
Name:   baidu.com
Address: 220.181.38.251
Name:   baidu.com
Address: 220.181.38.148

```

**浏览器输入了url之后，发生了什么事，解析过程是什么？**

- 浏览器进行dns查找，解析域名对应的ip机器，找到之后浏览器访问此ip地址
- 用户请求，发送到了服务器之后，优先是发给了ngix（web服务器）
  - 用户请求的是静态资源，如：（jpg、html、css、jquery）nginx直接从磁盘上找到资料发给前端
  - 如果nginx检测到的是动态请求，如：登录、注册、读取数据库等操作，通过url匹配发现是动态请求，发给后端的应用服务器（php，tomcat，django）
- django处理完用户的动态请求，如果发现需要读取数据库，再同故宫opymysql向mysql读取数据
- 如果djago处理请求发现读取的redis，再通过pyredis向redis拿数据
- django处理完毕后，返回给nginx
- nginx返回给前端
- 浏览器渲染数据后，刚给用户查看页面

> 推荐书籍：大型网站技术架构

#### 4.6.0 **crontab定时任务**

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211016184727254.png" alt="image-20211016184727254" style="zoom: 25%;" />

crond定时任务服务，提供了一个客户端管理命令crontab

```shell
# 编辑定时任务配置文件
crontab  -e  

# 查看定时任务的规则
crontab -l  

# 定时任务，注意的是 ，几号，和星期几不得共用
```

示例：

```shell
# 1.每分钟，将一句话，追加写入到一个文件中
# 打开配置文件
crontab -e
# 写入以下内容
*  *  *  *  *  /usr/bin/echo  "一直在写文件，每分钟写入一条" >>  /tmp/wsc.txt

# 2.检查定时任务
[root@localhost ~]# crontab -l
* * * * * /usr/bin/echo "一直在写文件，每分钟写入一条" >> /tmp/wsc.txt
```

**定时语法规则**

```shell
# 语法规则：
* * * * * 命令的绝对路径
# 五个星分别代表：分 时 日 月 周

# 1. 每小时的第3，第5分钟支执行
3,5 * * * * 

# 2.每天的2点一刻，3点一刻，4点一刻，5点一刻，执行命令
15 2-5 * * *

# 3.每天8点半上班
30 8 * * *

# 4.每天12点下班回家
00 00 * * *

# 5.每分钟执行一次命令
* * * * * 

# 6.每小时的3,15分钟执行命令
3,15 * * * * 

# 7.在上午8-11点的第3和第15分钟执行
3,15 8-11 * * *

# 8.每晚9:30执行命令
30 21 * * * 

# 9.每周六、日的下午1：30执行命令
30 13 * * 6-7

# 10.每周一到周五的凌晨1点，清空/tmp目录的所有文件，注意执行的命令请用绝对路径，否则会执行失败
00 1 * * 1-5 /usr/bib/rm -rf /tmp/*

# 11.每晚的零点重启nginx
00 00 * * * /usr/bin/systemctl restart nginx

# 12.每月的1,10,22日的4:45重启nginx
45 4 1,10,22 * * /usr/bin/systemctl start nginx

# 13.每个星期一的上午8点到11点的第3到15分钟执行命令
3-15 8-11 * * 1
```

### 4.6 Linux软件包管理

> - indows的软件管理，安装文件的后缀  *.exe
> - macos的应用程序安装 后缀  *.dmg
> - linux的二进制软件包 都是  *.rpm

软件的依赖关系

```shell
pip install flask # 仅仅就安装了flask模块吗？肯定不是，会安装一堆依赖的模块，比如jinja2等模块
# 在linux平台，以恶搞软件想要正确的运行，也得解决系统的环境，解决依赖关系
```

linux平台的软件安装形式，有三个：

- 源代码编译安装，此方式较为麻烦，但是可以自由选择软件的版本（因为是去官网下载最新版本代码），也可以扩展第三方额外的功能---->推荐：五星
  - 扩展第三方功能
  - 指定软件安装目录

- rpm包手动安装，此方式拒绝，需要手动解决依赖关系，贼恶心---->推荐：两星
- yum自动化安装软件，需要配置好yum源，能够自动搜索依赖关系，下载，安装，处理依赖关系----->五星
  - 不好的地方在于，yum源仓库的软件，版本可能较低
  - 无法指定安装路径，机器数量较多的时候，不容易控制

**更换yum源**

![image-20211016191224707](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211016191224707-16343827464062.png)

yum源的默认仓库文件夹是  `/etc/yum.repos.d/` ，只有在这个目录 `第一层` 的*.repo结尾的文件，才会被yum读取

```shell
# 1.下载wget命令
yum install wget -y   # wget命令就是在线下载一个url的静态资源

# 2.备份旧的yum仓库源
cd  /etc/yum.repos.d
mkdir  repobak
mv *.repo   repobak  # 备份repo文件

# 3.下载新的阿里的yum源仓库，阿里的开源镜像站https://developer.aliyun.com/mirror/
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

# 4.继续下载第二个仓库 epel仓库
wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

# 5.此时已经配置完毕，2个新的yum仓库，可以自由的嗨皮，下载软件了
[root@localhost yum.repos.d]# ls
CentOS-Base.repo  epel.repo  repobak

# 6.下载一个redis玩一玩
[root@localhost yum.repos.d]# yum install redis -y  # 就能够自动的下载redis，且安装redis

# 7.此时可以启动redis软件了，通过yum安装的redis，这么启动
systemctl  start redis   

# 8.使用redis的客户端命令，连接redis数据库
[root@localhost yum.repos.d]# systemctl start redis
[root@localhost yum.repos.d]# redis-cli
127.0.0.1:6379> ping
PONG

# 注：查找自己需要安装的包的名字
yum search 需要安装包的名字
```

### 4.7 inux文件管理练习题

1. linux命令格式是什么样？
2. linux命令必须添加参数才能执行吗
3. 解释linux的命令提示符，如何用linux命令解析 `[root@pylinux opt]#`
4. linux的目录分隔符是？
5. 简述Linux的目录结构
6. 切换到上一级目录
7. 切换到上一次目录
8. 切换到用户家目录
9. 查看当前工作目录
10. 当前目录是/home，以绝对、相对两种路径方式进入/opt目录
11. 列出/opt/下所有文件，包含隐藏文件，且显示详细信息（时间、文件大小）
12. 以树状结构显示文件夹内容
13. 一条命令创建/tmp/chaoge/linux文件夹
14. 创建python脚本first.py
15. 一条命令创建/tmp/chaoge1.txt 、 /tmp/chaoge2.txt 、/tmp/chaoge3.txt
16. 复制/opt/下所有内容到/tmp/下
17. 更改文件名first.txt为second.txt
18. 把/opt/下所有内容移动到/tmp下
19. 强制删除/tmp下所有内容
20. 查看ls命令的帮助信息
21. 立即重启的命令
22. 如何永久设置linux环境变量
23. linux的常见配置文件目录是
24. vim三种模式是?使用流程是?
25. 查看文件且显示行号
26. 如何清空文件内容，注意是空内容，而非空格
27. 显示文件前30行
28. 显示文件后50行
29. 实时刷新文件内容
30. 读取文件内容且倒序排序
31. 读取文件内容进行排序后去重
32. 读取文件内容且排序后，统计重复行的次数
33. 统计文件一共有多少行
34. 如何查看文件的详细信息（inode号，访问、修改时间、链接数）
35. 找出服务器上所有以".py"结尾的文件
36. 找出服务器2天内被访问过的文件
37. 找出服务器大于50M的文件
38. 找出/tmp/目录下所有的txt文件，然后删除
39. 把/data/html/文件夹打包压缩成data_html.tgz文件
40. 如何解压缩alltmp.gz文件
41. 如何解压缩data.zip文件
42. 显示具体的系统时间，时：分：秒
43. 如何彻底粉碎文件

```shell
# 1.默写常用命令与其意思

# 2.如何查看Linux的环境变量，路径以什么符号分割？如何修改PATH变量，以及永久修改PATH？

# 3.反复练习绝对路径，相对路径

# 在/home/目录创建文件夹oldboy,进入oldboy创建helloLinux.txt
# 创建/tmp/chaogeLinux.txt，用绝对路径方式与相对路径两种方法
# 在/tmp/目录下创建chaogeDir目录

# 4. 解释 linux下 > >> 符号是什么意思?

# 5.找到服务器上所有 以.txt结尾的普通文件

# 6.查看进程，并且过滤出有关python的进程

# 7.如何树状图显示django的项目文件内容

# 8.如何查看rm命令的详细使用信息？

# 9.读取/etc/passwd文件内容，写入到/tmp/pwd.txt文件中，（用两种方式）

# 10.在linux编写二个socket脚本，能够运行一个服务端，一个客户端，能send，recv接收消息即可

# 11.读取第十题的socket服务端脚本，且显示行号

# 12.拷贝/opt下所有的内容，放到/optbak文件夹下

# 13.给第十题的socket客户端脚本，改个名字

# 14.移动第十题的2个socket脚本，放入到/Learn_linux目录下

# 15.给启动django的命令做一个简单的别名
# python3 manage.py runserver 0.0.0.0:8000


# 16.搜索linux中所有超过100M的 压缩文件（后缀是*.gz）

# 17.搜索linux中名字有关django的文件夹

# 18.过滤出settings.py配置文件中的有益信息（非空，非注释行）

# 19.过滤出 /etc/passwd文件中有关root的行，注意大小写

# 20.实时刷新文件/var/log/nginx/access.log日志的内容变化

# 21.查看settings.py文件前10行

# 22.查看/etc/passwd文件的 10~20行的内容

# 23.传输本地的/tmp/pwd.txt到192.168.1.11服务器的/tmp目录

# 24. 如何在/home.python目录，执行/home/my_first.py文件?

# 25. 向my_first.py文件中写入"Lift is short,I use python"

# 26. 找一下python命令在哪

# 27. 查看/var/log/文件大小

# 28. 如何查看服务器的负载信息？

# 29.如何查看服务器是否运行了mysql?有哪些验证方式

# 30.如何杀死django进程？

# 31.预习博客https://www.cnblogs.com/pyyu/articles/9355477.html


# 1.怎么查看系统发行版？

# 2.Linux下UID，GID是什么？

# 3.怎么切换用户?

# 4.创建/删除用户？

# 5.pyyu用户无法执行此命令怎么办？ ls /root
# sudo command

# 6.linux文件权限有哪些?顺序是？

# 7.请说出 755, 740分别是什么权限?

# 8.修改文件权限为只有属主可读可写可执行？

# 9.请解压oldboy.tar.gz
# tar -zxvf oldboy.tar.gz

# 10.压缩/tmp/下所有内容到oldboy.tar.gz

# 11.如何查看linux端口？

# 12.如何杀死进程号为5888？

# 13.如何关闭iptables？

# 14.查看linux的dns文件

# 15.linux解析dns的命令是什么？

# 16.写一个定时任务，每周3的凌晨1点，压缩/var/log/下的所有文件为 log.tar.gz

# 17.如何安装redis,且启动redis可以访问?

# 18.如何配置yum epel源？用文字描述

# 19.centos7用什么命令管理服务

# 20.如何给linux添加一个dns服务器记录

# 21.每月的,5,15,25天的晚上5点50重启nginx

# 22.每周3到周5的深夜11点，备份/var/log /vmtp/

# 23.每天早上6.30清空/tmp/内容

# 24.每个星期三的下午6点到8点的第5，15分钟执行命令 command

# 25.扩展题，试一试

# 看博客，搭建python3环境，锻炼自己看文档
# https://www.cnblogs.com/pyyu/p/7402145.html
```

----

## 第五章 Linux编译python环境

### 5.1 安装python编译器

linux安装软件有哪些方式？

- rpm软件包 手动安装，拒绝此方式，需要手动解决依赖关系
- yum自动化安装，自动处理依赖关系，非常好用
- 源代码编译安装，可自定义的功能更多，指定软件安装路径
- 二进制源代码下载，此软件已经被编译安装过，生成了可执行的文件
  - 下载压缩包后，直接解压缩即可使用

```powershell
# 1.很重要，必须执行此操作，安装好编译环境，c语言也是编译后运行，需要gcc编译器golang，对代码先编译，再运行，python是直接运行
yum install gcc patch libffi-devel python-devel  zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel -y

# 2.获取python的源代码，下载且安装，下载源代码包的形式，自由选择
用windows的迅雷极速下载，下载完了之后，发送给linux机器即可
mac的同学，可以用scp或者等传输工具
windows的同学可以用lrzsz(yum install lrzsz -y )，xftp(自行去网站下载，支持断点续传，大文件传输)等文件传输工具

[root@localhost yum.repos.d] # wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz

# 3.下载完源代码包之后，进行解压缩
[root@localhost yum.repos.d]# tar -zxvf Python-3.6.9.tgz

# 4.解压缩完毕之后，生成了python369的源代码目录，进入源代码目录准备开始编译
cd Python-3.6.9

# 5.此时准备编译三部曲 ，编译的第一曲：指定python3的安装路径，以及对系统的开发环境监测，使用如下命令
	# 命令解释:configure 是一个脚本文件，用于告诉gcc编译器，python3即将安装到哪里，以及对基础的开发环境检查，检查openssl，检查sqllite，等等
	# 编译第一曲，结束后，主要是生成makefile，用于编译的
	[root@localhost Python-3.6.8]# ./configure --prefix=/opt/python369/   

	# 编译第二曲：开始进行软件编译
	直接输入 make 指令即可

	# 编译第三曲：编译安装，生成python3的可执行程序，也就是生成/opt/python36/
	make install    

	# 编译的第二曲，和第三曲，可以用 && 连起来写,代表make成功之后，继续make install 
    make && make install

# 6.等待出现如下结果，表示python3编译安装结束了
Successfully installed pip-18.1 setuptools-40.6.2

# 7.此时可以去检查python3的可执行程序目录
[root@localhost bin]# pwd
/opt/python369/bin

# 8.配置PATH环境变量 ，永久修改PATH，添加Python3的bin目录放入PATH开头位置
vim /etc/profile
PATH="/opt/python369/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin"

# 9.手动读取/etc/profile，加载文件中的所有变量
source  /etc/profile  

# 10.检查python3的目录，以及pip3的绝对路径
[root@localhost bin]# which python3
/opt/python36/bin/python3
[root@localhost bin]# which pip3
/opt/python36/bin/pip3
```

### 5.2 django项目的搭建

```powershell
# 1.安装django模块
pip3 install -i https://pypi.douban.com/simple  django==1.11.25
# 检查一下pip3的模块信息
pip3 list

# 2.升级pip3工具
pip3 install  -i https://pypi.douban.com/simple  --upgrade pip

# 3.在linux平台，使用命令创建django项目了
django-admin  startproject  dj1
 
# 4.创建一个app01
[root@localhost python369]# cd ../dj1
[root@localhost python369]# python manage.py startapp app01
 
# 5.编写一个视图函数，hello视图，修改app01，【访问hello视图，返回字符串:hello world】
	# 5.1修改django的settings.py ，注册app01 ，修改如下
     INSTALLED_APPS = [
          'django.contrib.admin',
          'django.contrib.auth',
          'django.contrib.contenttypes',
          'django.contrib.sessions',
          'django.contrib.messages',
          'django.contrib.staticfiles',
          'app01',
      ]
      # 并且修改允许访问的主机列表,写一个 * 表示允许所有的主机访问(默认只允许本地127.0.0.1访问,启动在了linux的机器上，如果不修改windows无法访问)
      ALLOWED_HOSTS = ["*"]   

    # 5.2先修改django的 urls.py,如下：
    from django.conf.urls import url
    from django.contrib import admin
    from app01 import views
    urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^hello/', views.hello),
    ]


    # 5.3编写django的app01.views，添加如下代码
    from django.shortcuts import render,HttpResponse

    def hello(requests):
    return HttpResponse("hello world!")


# 6. 进行数据库迁移
[root@localhost app01]# python3 manage.py migrate

    # 6.1.指定ip和端口，启动django
    [root@localhost app01]# python3  manage.py  runserver 0.0.0.0:9000

# 7.如何访问django项目？
访问linux的ip地址 + django的端口

# 8.注意：前提一定要关闭防火墙，否则无法访问服务器地址
```

### 5.3 virtualenv虚拟环境

使用virtualenv实现解释器环境的隔离，在系统中建立多个不同并且相互不干扰的虚拟环境。

```shell
# 1.下载虚拟环境工具
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple virtualenv

# 2.通过命令创建虚拟环境，通过pip安装的第三方库会放到虚拟环境中
--python=python3：指定venv虚拟解释器，以哪个解释器为本体
这个命令如果你用相对陆路径，就得注意你在哪执行的此命令
[root@localhost bin]# virtualenv --python=python3  venv1

# 3.创建好venv1之后，需要激活方可使用，这个激活其实就是在修改PATH而已
[root@localhost bin]# source /opt/venv1/bin/activate
(venv1) [root@s25linux bin]#

# 4.明确虚拟环境下venv1的解释器是干净隔离的
(venv1) [root@localhost bin]# which python3
/opt/venv1/bin/python3
(venv1) [root@localhost bin]# which pip3
/opt/venv1/bin/pip3
(venv1) [root@localhost bin]# pip3 list
Package    Version
---------- -------
pip        21.2.4
setuptools 58.1.0


# 5.在venv1中安装django1
(venv1) [root@s25linux opt]# pip3 install -i https://pypi.douban.com/simple django==1.11.9

# 6.再开一个ssh窗口，再创建venv2，用于运行django2 版本
virtualenv --python=python  venv2
# 激活虚拟环境venv2
[root@localhost bin]# source /opt/venv2/bin/activate
# 创建django版本2的项目
pip3 install -i https://pypi.douban.com/simple django==2.0.1
# 创建diango2项目
django-admin  startproject  venv2_dj2
# 分别启动2个版本的django，浏览器访问效果

# 7.执行此命令，退出虚拟环境，系统会自动删除venv的PATH，也就表示退出了
deactivate
```

示例：linux启动crm项目

```shell
# 1.准备好crm代码，上传至linux服务器中
scp

# 2.新建一个虚拟环境，用于运行crm
[root@localhost auto_crm]# virtualenv --python=python3  venv_crm

# 3.激活venv_crm环境
[root@localhost opt]# source /opt/auto_crm/venv_crm/bin/activate

# 4.导出crm所需要的依赖，通过ssh命令导入到linux服务器
pip3 freeze > requirements.txt

# 5.linux服务器安装所需要的依赖
(venv_crm) [root@s25linux tf_crm]# pip3 install -i https://pypi.douban.com/simple  -r  requirements.txt

# 6.缺少mysql，因此需要安装mariadb
(venv_crm) [root@s25linux tf_crm]# yum install mariadb-server  mariadb -y

# 7.启动mariadb数据库
(venv_crm) [root@s25linux tf_crm]# systemctl start mariadb

# 8.还要注意，由于数据库是空的，还得进行数据库表的导入，导出本地数据库
# 参数--all-databases能够导出所有的数据库、表，也可以指定某一个数据库、表导出
mysqldump  -uroot -p  --all-databases   >  alldb.sql 

# 导出db1、db2两个数据库的所有数据
mysqldump -uroot -p --databases db1 db2 >/tmp/user.sql

# 9.通过lrzsz、scp、xftp等方式，发送alldb.sql文件，给linux服务器，再进行数据导入
mysql -uroot -p  <  /opt/alldb.sql

# 10.此时再次尝试启动crm项目
(venv_crm) [root@localhost auto_crm]# python3 manage.py runserver 0.0.0.0:9000
```

### 5.4 uwsgi启动python

> 官方文档：https://uwsgi-docs.readthedocs.io/en/latest/

#### 5.4.1 wsgi是什么？

WSGI，全称 Web Server Gateway Interface，或者 Python Web Server Gateway Interface ，是为 Python 语言定义的 Web 服务器和 Web 应用程序或框架之间的一种简单而通用的接口。自从 WSGI 被开发出来以后，许多其它语言中也出现了类似接口。

WSGI 的官方定义是，the Python Web Server Gateway Interface。从名字就可以看出来，这东西是一个Gateway，也就是网关。网关的作用就是在协议之间进行转换。

WSGI 是作为 Web 服务器与 Web 应用程序或应用框架之间的一种低级别的接口，以提升可移植 Web 应用开发的共同点。WSGI 是基于现存的 CGI 标准而设计的。

很多框架都自带了 WSGI server ，比如 Flask，webpy，Django、CherryPy等等。当然性能都不好，自带的 web server 更多的是测试用途，发布时则使用生产环境的 WSGI server或者是联合 nginx 做 uwsgi 。

也就是说，WSGI就像是一座桥梁，一边连着web服务器，另一边连着用户的应用。但是呢，这个桥的功能很弱，有时候还需要别的桥来帮忙才能进行处理。WSGI 的作用如图所示：

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\20160509174237093.png)

**WSGI的作用**

WSGI有两方：“服务器”或“网关”一方，以及“应用程序”或“应用框架”一方。服务方调用应用方，提供环境信息，以及一个回调函数（提供给应用程序用来将消息头传递给服务器方），并接收Web内容作为返回值。

所谓的 WSGI中间件同时实现了API的两方，因此可以在WSGI服务和WSGI应用之间起调解作用：从WSGI服务器的角度来说，中间件扮演应用程序，而从应用程序的角度来说，中间件扮演服务器。“中间件”组件可以执行以下功能：

- 重写环境变量后，根据目标URL，将请求消息路由到不同的应用对象。
- 允许在一个进程中同时运行多个应用程序或应用框架。
- 负载均衡和远程处理，通过在网络上转发请求和响应消息。
- 进行内容后处理，例如应用XSLT样式表。

WSGI 的设计确实参考了 Java 的 servlet。

#### 5.4.2 uwsgi

uwsgi是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。Nginx中HttpUwsgiModule的作用是与uWSGI服务器进行交换。

要注意 WSGI / uwsgi / uWSGI 这三个概念的区分。

- WSGI看过前面小节的同学很清楚了，是一种通信协议。
- uwsgi同WSGI一样是一种通信协议。
- 而uWSGI是实现了uwsgi和WSGI两种协议的Web服务器。

uwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型（type of information），每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。

为什么有了uWSGI为什么还需要nginx？因为nginx具备优秀的静态内容处理能力，然后将动态内容转发给uWSGI服务器，这样可以达到很好的客户端响应。

接下来，我们要看看 uWSGI 的安装配置与使用。

让你的django在linux上，支持并发形式启动，支持多进程，多线程，乃至于协程的一个C语言编写的高性能工具。

```shell
# 1.安装uwsgi工具
pip3 install -i https://pypi.douban.com/simple  uwsgi

# 2.编写uwsgi.ini配置文件，以多进程形式启动tf_crm，手动创建uwsgi的配置文件
touch uwsgi.ini
# 写入如下的功能性的参数配置，用于启动项目
# 这些部署的流程，是国外的uwsgi官网，给与的用法，我们照着用即可
# 注意要根据你自己的目录，修改如下的参数

[uwsgi]
# Django-related settings
# the base directory (full path)
# 填写crm项目的第一层绝对路径
chdir           = /opt/auto_crm
# Django's wsgi file
# 填写crm项目第二层的相对路径，找到第二层目录下的wsgi.py
# 这里填写的不是路径，是以上一个参数为相对，找到主应用目录下的wsgi.py文件
module          = auto_crm.wsgi
# the virtualenv (full path)
# 填写虚拟环境解释器的第一层工作目录 
home            = /opt/tf_crm/venv_crm
# process-related settings
# master
# 主进程
master          = true
# maximum number of worker processes
# 代表定义uwsgi运行的多进程数量，官网给出的优化建议是 2*cpu核数+1 ，单核的cpu填写几?
# 如果是单进程，十万个请求，都丢给一个进程去处理
# 3个工作进程，十万个请求，就分给了3个进程去分摊处理
processes       = 3

# the socket (use the full path to be safe
# 这里的socket参数，是用于和nginx结合部署的unix-socket参数，这里临时先暂停使用
# 线上不会用http参数，因为对后端是不安全的，使用socket参数是安全的连接，用nginx反向代理去访问
# 后端程序是运行在防火墙内部，外网是无法直接访问的
# 临时使用http参数，便于我们用浏览器调试访问
# socket          = 0.0.0.0:8000
http 			  = 0.0.0.0:8000

# ... with appropriate permissions - may be needed
# chmod-socket    = 664
# clear environment on exit
vacuum            = true


# 3.此时可以用uwsgi命令，基于uwsgi协议的一个高性能web后端启动了
uwsgi  --ini  ./uwsgi.ini  # 指定配置文件启动后端

# 4.此时crm项目，已经用uwsgi支持了3个进程的启动了，但是由于uwsgi对静态文件的解析性能很弱，线上是丢给nginx去处理的

# 5.浏览器访问
uwsgi：http://192.168.254.130:8000/login
wsgi:http://192.168.254.130:9000/login
```

### 5.5 supervisor工具

**\- supervisord**

运行 Supervisor 时会启动一个进程 supervisord，它负责启动所管理的进程，并将所管理的进程作为自己的子进程来启动，而且可以在所管理的进程出现崩溃时自动重启。

**\- supervisorctl**

是命令行进程管理工具，可以用来执行 stop、start、restart 等命令，来对这些子进程进行管理。

supervisor是所有进程的父进程，管理着启动的子进展，supervisor以子进程的PID来管理子进程，当子进程异常退出时supervisor可以收到相应的信号

**安装supervisor工具**

```shell
# 1.使用yum命令即可直接安装
[root@localhost ~]# yum install supervisor -y

# 2.生成supervisor的配置文件
[root@localhost ~]# echo_supervisord_conf >  /etc/supervisord.conf

# 3.修改supervisor的配置文件，添加管理crm的任务
[root@localhost etc]# vim  /etc/supervisor.conf

# 4.写入以下内容，[program:xx]是被管理的进程配置参数，xx是进程的名称
[program:s25crm]
command=写入启动uwsgi的命令  # supervisor其实就是在帮你执行命令而已！
autostart=true       	  # 在supervisord启动的时候也自动启动
startsecs=10         	  # 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒
autorestart=true     	  # 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启
stopasgroup=true     	  # 默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程
killasgroup=true     	  # 默认为false，向进程组发送kill信号，包括子进程
```

示例：

```shell
# 1.uwsgi和uwsgi.ini都配置完毕之后，开始使用supervisor工具进行管理了
# 先明确，启动uwsgi的绝对路径命令是什么：
	8.1  找到虚拟环境uwsgi的绝对路径  /opt/venv_crm_uwsgi/bin/uwsgi
	8.2  找到创建uwsgi.ini配置文件的绝对路径  /opt/venv_crm_uwsgi/uwsgi.ini
	8.3  因此启动crm项目的 完整绝对路径命令是
			/opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini

# 2.修改supervisor的配置文件
vim  /etc/supervisord.conf

# 3.写入以下内容
[program:supervisor]
command=/opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini   ;supervisor其实就是在帮你执行命令而已！
autostart=true       ; 在supervisord启动的时候也自动启动
startsecs=10         ; 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒
autorestart=true     ; 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启
stopasgroup=true     ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程
killasgroup=true     ;默认为false，向进程组发送kill信号，包括子进程

# 4.启动supervisor，默认就会直接启动uwsgi了 
supervisord -c /etc/supervisord.conf   # 启动supervisor服务端，指定配置文件启动

# 5.启动完毕supervisor之后，检查进程信息
ps -ef|grep  supervisor # 检查supervisor是否存在了进程，是否启动
ps -ef|grep  uwsgi  # 检查uwsgi是否启动

(venv_crm_uwsgi) [root@localhost venv_crm_uwsgi]# ps -ef |grep supervisor
root       3460      1  0 06:55 ?        00:00:00 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord.conf
root       3468   1655  0 06:56 pts/0    00:00:00 grep --color=auto supervisor
(venv_crm_uwsgi) [root@localhost venv_crm_uwsgi]# ps -ef | grep uwsgi
root       3461   3460  0 06:55 ?        00:00:00 /opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini
root       3463   3461  0 06:55 ?        00:00:00 /opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini
root       3464   3461  0 06:55 ?        00:00:00 /opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini
root       3465   3461  0 06:55 ?        00:00:00 /opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini
root       3466   3461  0 06:55 ?        00:00:00 /opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini
root       3470   1655  0 06:56 pts/0    00:00:00 grep --color=auto uwsgi

# 6.进入supervisor任务管理终端
supervisorctl -c /etc/supervisord.conf

# 7.访问url，看到了没有静态文件的crm界面，就是正确的了

# 8.supervisorctl的管理命令
	# 提供了如下命令：
    (venv_crm_uwsgi) [root@localhost venv_crm_uwsgi]# supervisorctl -c /etc/supervisord.conf
    supervisor                       RUNNING   pid 3514, uptime 0:05:39
    supervisor> status
    supervisor                       RUNNING   pid 3514, uptime 0:05:41
    supervisor> stop supervisor
    supervisor: stopped
    supervisor> status
    supervisor                       STOPPED   Oct 20 07:07 AM
    supervisor> start supervisor
    supervisor: started
    supervisor> status
    supervisor                       RUNNING   pid 3547, uptime 0:00:31
    supervisor> restart all

# 9.uwsgi异常崩溃的话，supervisor会立即重启uwsgi

# 10.如果要运行多个uwsgi项目，在supervisor中定义多个任务即可
```

6.supervisorctl的管理命令

![image-20211019231158128](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211019231158128.png)

7.uwsgi异常崩溃的话，supervisor会立即重启uwsgi

![image-20211019230609875](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211019230609875.png)

### 5.6 nginx

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20181014124743540-1325157612.png" alt="img"  />

 想必我们大多数人都是通过访问网站而开始接触互联网的吧。我们平时访问的网站服务 就是 Web 网络服务，一般是指允许用户通过浏览器访问到互联网中各种资源的服务。

Web 网络服务是一种被动访问的服务程序，即只有接收到互联网中其他主机发出的 请求后才会响应，最终用于提供服务程序的 Web 服务器会通过 HTTP(超文本传输协议)或 HTTPS(安全超文本传输协议)把请求的内容传送给用户。

目前能够提供 Web 网络服务的程序有 IIS、Nginx 和 Apache 等。其中，IIS(Internet Information Services，互联网信息服务)是 Windows 系统中默认的 Web 服务程序

2004 年 10 月 4 日，为俄罗斯知名门户站点而开发的 Web 服务程序 Nginx 横空出世。 Nginx 程序作为一款轻量级的网站服务软件，因其稳定性和丰富的功能而快速占领服务器市 场，但 Nginx 最被认可的还当是系统资源消耗低且并发能力强，因此得到了国内诸如新浪、 网易、腾讯等门户站的青睐。

nginx**是一个开源的**，支持高性能，高并发的www服务和代理服务软件。它是一个俄罗斯人lgor sysoev开发的，作者将源代码开源出来供全球使用。
nginx比它大哥apache性能改进许多，nginx占用的系统资源更少，支持更高的并发连接，有更高的访问效率。nginx不但是一个优秀的web服务软件，还可以作为反向代理，负载均衡，以及缓存服务使用。安装更为简单，方便，灵活。nginx可以说是非常nb了。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180813155012063-443835808.png" alt="img" style="zoom:50%;" />

Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。

中文文档：http://www.nginx.cn/doc/index.html

官网：http://nginx.org

**为什么选择nginx**

Nginx 是一个高性能的 Web 和反向代理服务器, 它具有有很多非常优越的特性：

- 作为 Web 服务器：相比 Apache，Nginx 使用更少的资源，支持更多的并发连接，体现更高的效率，这点使 Nginx 尤其受到虚拟主机提供商的欢迎。能够支持高达 50,000 个并发连接数的响应，感谢 nginx 为我们选择了 epoll and kqueue 作为开发模型。

- 作为负载均衡服务器：nginx 既可以在内部直接支持 Rails 和 PHP，也可以支持作为 HTTP代理服务器 对外进行服务。nginx 用 C 编写, 不论是系统资源开销还是 CPU 使用效率都比 Perlbal 要好的多。

- 作为邮件代理服务器: nginx 同时也是一个非常优秀的邮件代理服务器（最早开发这个产品的目的之一也是作为邮件代理服务器），Last.fm 描述了成功并且美妙的使用经验。

- nginx 安装非常的简单，配置文件 非常简洁（还能够支持perl语法），Bugs非常少的服务器: nginx 启动特别容易，并且几乎可以做到7*24不间断运行，即使运行数个月也不需要重新启动。你还能够在 不间断服务的情况下进行软件版本的升级。

**nginx 与 apache 对比**

- 静态文件处理能力：nginx高于apache
- 资源消耗：nginx优于apache,因为nginx是异步处理模型，只需要几个进程就能够处理大量在线请求，而apache 2.4仍然是进程模型或者线程模型，即仍然采用大量线程来处理大量在线请求。
- Apache支持的模块很多，而且也比较稳定。而nginx由于出现的比较晚，所以在这方面可能比不上Apache。
- nginx本身就是一个反向代理服务器，而且支持7层负载均衡。
- nginx处理动态页面很鸡肋，一般只用与处理静态页面和反向代理。

**面试回答nginx**

支持高并发，能支持几万并发连接
资源消耗少，在3万并发连接下开启10个nginx线程消耗的内存不到200M
可以做http反向代理和负载均衡
支持异步网络i/o事件模型epoll

**web服务器和web框架的关系**

web服务器（nginx）：接收HTTP请求（例如www.baidu.com）并返回数据

web框架（django，flask）：开发web应用程序，处理接收到的数据

#### 5.6.1 环境搭建

**tenginx搭建**

小提示： 如果你想删除编译安装的软件（1.清空PATH  2.删除文件夹即可）

```shell
# 1.注意，编译软件之前，还是需要解决系统的开发环境，例如如下
yum install gcc patch libffi-devel python-devel  zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel openssl openssl-devel -y

# 2.进入淘宝tenginx官网，下载源代码，进行编译安装
http://tengine.taobao.org/index_cn.html

# 3.在linux的opt目录下，下载nginx源代码
[root@localhost opt]# wget http://tengine.taobao.org/download/tengine-2.3.2.tar.gz

# 4.解压缩源代码，准备编译三部曲
[root@localhost opt]# tar -zxvf  tengine-2.3.2.tar.gz

# 5.进入源码目录，指定nginx的安装位置
[root@localhost tengine-2.3.2]# ./configure --prefix=/opt/tngx232/

# 6.编译且编译安装，生成nginx的可执行命令目录
make && make install

# 7.安装完毕后，会生成/opt/tngx232/文件夹，nginx可以使用的配置都在这里了
[root@localhost tngx232]# ls
conf  html  logs  sbin
 
	# 目录解释
    conf  # 存放nginx所有配置文件的目录,主要nginx.conf
    html  # 存放nginx默认站点的目录，如index.html、error.html等
    logs  # 存放nginx默认日志的目录，如error.log access.log
    sbin  # 存放nginx主命令的目录，sbin/nginx
    
# 8.添加nginx到PATH中，可以快捷执行命令
[root@localhost tngx232]# vim /etc/profile
PATH="/opt/tng232/sbin:/opt/python369/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin"

# 9.手动读取/etc/profile，加载文件中的所有变量
[root@localhost tngx232]# source  /etc/profile 

# 10.首次启动nginx，注意要关闭防火墙
# 直接输入nginx命令即可启动，没有返回说明已经启动成功
[root@localhost sbin]# nginx
	# 有关nginx的命令
    nginx			 # 首次输入是直接启动，不得再次输入 
    nginx -s reload  # 平滑重启，重新读取nginx的配置文件，而不重启进程
    nginx -s stop  	 # 停止nginx进程 
    nginx -t   		 # 检测nginx.conf语法是否正确
    
# 11.默认访问nginx的首页站点url是
http://192.168.254.130:80
```

**nginx搭建**

```shell
# 1.注意，编译软件之前，还是需要解决系统的开发环境，例如如下
yum install gcc patch libffi-devel python-devel  zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel openssl openssl-devel -y

# 2.进入nginx官网，下载源代码，进行编译安装
[root@localhost opt]# wget http://nginx.org/download/nginx-1.16.0.tar.gz -P /usr/src/

# 3.解压缩源代码，准备编译三部曲
[root@localhost src]# cd /usr/src
[root@localhost src]# tar -zxvf nginx-1.16.0.tar.gz
[root@localhost nginx-1.16.0]# cd /usr/src/nginx-1.16.0

# 4.检查环境是否满足安装条件，指定安装路径（配置文件、命令文件、开启模块功能）
[root@localhost nginx-1.16.0]# ./configure --prefix=/usr/local/nginx

# 5.编译，使用gcc将源码生成可执行程序
[root@localhost nginx-1.16.0]# make -j4

# 6.安装
[root@localhost nginx-1.16.0]# make install

# 7.nginx相关目录
 nginx path prefix: "/usr/local/nginx"
 nginx binary file: "/usr/local/nginx/sbin/nginx"
 nginx modules path: "/usr/local/nginx/modules"
 nginx configuration prefix: "/usr/local/nginx/conf"
 nginx configuration file: "/usr/local/nginx/conf/nginx.conf"
 nginx pid file: "/usr/local/nginx/logs/nginx.pid"
 nginx error log file: "/usr/local/nginx/logs/error.log"
 nginx http access log file: "/usr/local/nginx/logs/access.log"
 
# 8.添加环境变量
[root@localhost sbin]# vim /etc/profile
PATH="/usr/local/nginx/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin"
[root@localhost sbin]# source /etc/profile

# 9.启动nginx并验证
[root@localhost sbin]# yum install lsof -y
[root@localhost sbin]# yum install elinks -y
[root@localhost sbin]# netstat -ntlp
[root@localhost sbin]# lsof -i :80
[root@localhost sbin]# nginx
[root@localhost sbin]# elinks http://127.0.0.1:80 -dump
```

#### 5.6.2 主配置文件解析

Nginx配置文件由三部分组成：

- 全局块：主要设置一些影响Nginx服务器整体运行的配置指定，比如：worker_processes，值越大，可以支持并发处理两就越多。

- Events块：涉及的指令主要影响nginx服务器于用户的网络连接，比如：worker_connections，支持的最大连接数。

- HTTP块：包括http全局块和server块，是服务器配置中最频繁的部分，包括配置代理，缓存，日志定义等绝大多数功能。

  server块：配置虚拟主机的相关参数。

  location块：配置请求路由，以及各种页面的处理情况。

```powershell
详见：https://www.runoob.com/w3cnote/nginx-setup-intro.html
# 1.定义Nginx运行的用户和用户组
# user  nobody;
# 2.nginx进程数，建议设置为等于CPU总核心数。以cpu核数为准
worker_processes  5;
# 3.想用哪个用哪个，直接打开注释，或者写进来即可
error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;
#error_log  "pipe:rollback logs/error_log interval=1d baknum=7 maxsize=2G";
# 4.pid文件的作用是用于启停进程的号码
	# 平时获取nginx进程id：ps -ef去获取nginx的进程id
	# 把pid写入到此 nginx.pid 文件中，
pid        logs/nginx.pid;

events {
    worker_connections  1024;
}

# 5.这个http区域，是nginx的核心功能区域
http {
    include       mime.types;	# 文件扩展名与文件类型映射表
    default_type  application/octet-stream;	# 默认文件类型，默认为text/plain
	# 6.打开此nginx的访问日志功能，即可查看日志
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  logs/access.log  main;	# combined为日志格式的默认值

    sendfile        on;	# 允许sendfile方式传输文件，默认为off，可以在http块，server块，location块
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;	# 连接超时时间，默认为75s，可以在http，server，location块。
	# 7.nginx开启静态资源压缩，比如nginx返回磁盘的html文件特别大，里面包含了诸多的js css，图片引用
    #  一个html文件 达到4m大小
    #  传输图片 等等都是高清的 1080p图片
    #  打开此功能，能够极大的提升网站访问，以及静态资源压缩
    gzip  on;

    # 提供静态资源缓存功能，第一次访问过网页之后，nginx能够让图片js等静态资源，缓存到浏览器上
    # 浏览器下次访问网站，速度就几乎是秒开了
    # 想要用这些功能，只需要在nginx里打开某些配置即可，作者都已经写好了该功能
    
	# 8.这里的server区域配置，就是虚拟主机的核心配置
    # nginx支持编写多个server{} 区域块，以达到多虚拟主机，多个站点的功能
    # server{} 区域块，可以存在多个，且默认是自上而下去加载，去匹配的
    # 目前这里是第一个server {} 区域块，端口是85
    server {
        # 9.定义该网站的端口
        listen       85;
        # 10.填写域名，没有就默认即可
        server_name  localhost;
        # 11.更改nginx的编码支持
        charset utf-8;
        # 12.如此添加一行参数，当用户请求出错，出现404的时候，就返回 root 定义的目录去寻找40x.html文件
        # 也就是去配置 /1021python/ 这个目录下 寻找 40x.html
        error_page  404  /40x.html;
        #access_log  logs/host.access.log  main;
        
       #access_log  "pipe:rollback logs/host.access_log interval=1d baknum=7 maxsize=2G"  main;
        # 13.nginx的域名匹配，所有的请求，都会进入到这里
        # 例如：192.168.178.140:85/lubenwei.jpg
        #      192.168.178.140:85/menggededianhua.txt
        location / {
            # 14.这个root参数，是定义该虚拟主机，资料存放路径的，可以自由修改
            # 当用户访问  192.168.178.140:85的时候，就返回该目录的资料
            root   /1021python/;
           # 15.index参数，用于定义nginx的首页文件名字  ，只要在/s25nginx目录下存在index.html文件即可
            index  index.html index.htm;
        }
    }
# 多站点
# 这里就是上一个Server{}的标签闭合处了，，可以写入第二个server{}
# 注意 ，注意，server{} 标签快，是平级的关系，不得嵌套，检查好你的花括号
# 这里是第二个虚拟主机的配置了
    server  {
    listen 80;
    server_name  _;
    #nginx的域名匹配
    # 当用户访问 192.168.178.140:89的时候，返回该目录的内容
    location  / {
            root   /1021linux/;
            index  index.html;
    	}
    }
}
```

##### 5.6.2.1 默认网站

```powershell
server {
listen       80;
server_name  localhost;

location / {
root   html;
index  index.html index.htm;
}

error_page   500 502 503 504  /50x.html;
location = /50x.html {
root   html;
}
}


# 1.在html目录中创建测试目录
mkdir {a..c}
echo aaaa > a/index.html
echo bbbb > b/index.html
echo cccc > c/index.html

# 2.访问网站
elinks http:127.0.0.1 -dump
elinks http:127.0.0.1/a -dump
elinks http:127.0.0.1/b -dump
elinks http:127.0.0.1/c -dump
```

##### 5.6.2.2 目录访问权限

```powershell
# 只允许本机访问，其他人拒绝访问
location /a {
                allow 127.0.0.1;   # 允许此ip的用户可以访问
                allow 192.168.254.131;  # 允许此ip的用户可以访问
                deny all;
                # return 502;  # 打开return后，所有人访问都返回502
        }

# 1.修改配置文件如上
[root@localhost html]# vim ../conf/nginx.conf

# 2.修改后平滑重启
[root@localhost html]# nginx -t
[root@localhost html]# nginx -s reload

# 3.测试 or chrome浏览器访问，看效果
[root@localhost html]# elinks http://192.168.254.131/a -dump
```

##### 5.6.2.3 登录验证

```powershell
location /c {
auth_basic "登陆验证";
auth_basic_user_file /etc/nginx/htpasswd;
}

# 1.安装加密工具
[root@localhost conf]# rpm -qf `which htpasswd`
httpd-tools-2.4.6-97.el7.centos.2.x86_64
[root@localhost conf]# yum install httpd-tools -y

# 2.清除配置文件内的注释行和空行
[root@localhost conf]# sed -i "/#/d" nginx.conf.default1
[root@localhost conf]# sed -i "/^$/d" nginx.conf.default1

# 3.对sky进行加密 -m 已有目录加密，-c 目录不存在创建目录
htpasswd -m /etc/nginx/htpasswd sky

# 4.编辑配置文件，写入以上内容
# 5.访问http://192.168.254.131/c/
需要输入用户名和密码
```

##### 5.6.2.4 日志管理

Nginx访问日志主要有两个参数控制：

> ```powershell
> - log_format  # 用来定义记录日志的格式（可以定义多种日志格式，取不同名字即可）
> 
> - access_log  # 用来指定日至文件的路径及使用的何种日志格式记录日志
> 
> - access_log logs/access.log main;
> 
> 
> 
> log_format格式变量：
> 
> $remote_addr #记录访问网站的客户端地址
> 
> $remote_user #远程客户端用户名
> 
> $time_local #记录访问时间与时区
> 
> $request #用户的http请求起始行信息
> 
> $status #http状态码，记录请求返回的状态码，例如：200、301、404等 $body_bytes_sent #服务器发送给客户端的响应body字节数
> 
> $http_referer #记录此次请求是从哪个连接访问过来的，可以根据该参数进行防盗链设置。
> 
> $http_user_agent #记录客户端访问信息，例如：浏览器、手机客户端等
> 
> $http_x_forwarded_for #当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_fo设置
> ```

```powershell
# 自定义一个json格式的访问地址
log_format main_json '{"@timestamp":"$time_local",'
'"client_ip": "$remote_addr",'
'"request": "$request",'
'"status": "$status",'
'"bytes": "$body_bytes_sent",'
'"x_forwarded": "$http_x_forwarded_for",'
'"referer": "$http_referer"'
'}';
access_log logs/access_json.log main_json;

# 1.编辑配置文件，新增以上内容
[root@localhost conf]# vim nginx.conf

# 2.监测实时日志
[root@localhost conf]# tailf ../logs/access.log
```

##### 5.6.2.5 防盗链

```powershell
# 防盗链配置，盗链：访问地址重定向到另一个地址的内容
location /c {
# location ~* \.(png|gif|bpm)$ {
valid_referers none blocked *.ayitula.com;
if ($invalid_referer) {
return 403;
}
}

# 1.机器133安装httpd
[root@localhost ~]# yum install httpd -y

# 2.启动httpd
[root@localhost ~]# systemctl start httpd

# 3.把index.html文件拷贝到 /var/www/html/路径下
[root@localhost html]# cat index.html
<html>
<head>
<title>fang dao lian ce shi</title>
</head>
<body>
<font size="5">
<a href="http://192.168.254.131/a/0.png">dao lian</a>
<br></br>
</font>
</body>
</html>

# 4.把0.png图片拷贝到131的机器上/usr/local/nginx/html/目录下
# 5.访问192.168.254.133
```

#### 5.6.3 nginx虚拟机

虚拟主机：就是把⼀台物理服务器划分成多个“虚拟”的服务器，每⼀个虚拟主机都可以有独⽴的域名和独⽴的⽬录

也称之为是nginx的虚拟主机站点配置，指的就是在nginx中，能够通过文件目录的不同，可以定义多个不同的网站

```powershell
# 1.如何修改nginx的首页地址，进入html目录下，找到index.html文件，默认读取的是这个文件
[root@localhost html]# pwd
/opt/tngx232/html
[root@localhost html]# ls
50x.html  index.html

# 2.在自己的站点下，存放一些静态资料，如gif，jpg等
[root@localhost html]# ls
50x.html  index.html  tu2.jpg

# 3.浏览器访问图片地址 比如：http://192.168.254.130/tu2.jpg
```

**nginx多站点功能**

- 基于IP的多虚拟机主机

```powershell
http {
    server {
        listen       80;
        server_name  192.168.254.131;
        location / {
            root   html/web1;
            index  index.html index.htm;
        }
    }

 server {
        listen       80;
        server_name  192.168.254.132;
        location / {
            root   html/web2;
            index  index.html index.htm;
        }
    }
}


# 1.添加一个子网卡
[root@localhost conf]# ifconfig ens33:1 192.168.254.131/24 up

# 2.html文件夹下创建站点

# 3.写入内容
echo web1 > nginx/html/web1/index.html
echo web2 > nginx/html/web2/index.html

# 4.测试完关闭子网卡
[root@localhost conf]# ifconfig ens33:1 down

# 5.访问配置好的链接
[root@web131 conf]# elinks http:192.168.254.131 -dump
   web1111
[root@web131 conf]# elinks http:192.168.254.132 -dump
   web2222


# 缺点
每个网站都需要一个IP
缺点：需要多个IP，如果是公网IP，每个IP都需要付费
```

- 基于域名的多虚拟主机

```powershell
# 1.修改hosts文件，强制写入域名对应关系
vim /etc/hostes
192.168.254.130   www.jd.com
192.168.254.131   www.baidu.com

# 2.修改nginx配置文件
http {
    include       mime.types;
    default_type  application/octet-stream;


    sendfile        on;
    keepalive_timeout  65;


    server {
        listen       80;
        server_name  www.jd.com;

        location / {
            root   html/web1;
            index  index.html index.htm;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }

    }

 server {
        listen       80;
        server_name  www.baidu.com;

        location / {
            root   html/web2;
            index  index.html index.htm;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}


# 3.平滑重启，访问域名
[root@web131 conf]# nginx -s reload
[root@web131 conf]# elinks www.jd.com -dump
   web1111
[root@web131 conf]# elinks www.baidu.com -dump
   web2222
```

- 基于端口的多虚拟主机

```powershell
# 1.修改nginx.conf配置如下，定义2个server{} 区域块即可，如下
http {
    server {
        listen       80;
        server_name  192.168.254.131;
        location / {
            root   html/web1;
            index  index.html index.htm;
        }
    }

 server {
        listen       8080;
        server_name  192.168.254.131;
        location / {
            root   html/web2;
            index  index.html index.htm;
        }
    }
}

# 2.访问链接
[root@web131 conf]# elinks http:192.168.254.131 -dump
   web1111
[root@web131 conf]# elinks http:192.168.254.131:8080 -dump
   web2222


# 缺点
只需要一个IP
缺点：断开你是无法告诉公网用户  无法适用于公网   适合内部用户
```

改完配置文件后，分别创建2个站点的资源目录

```powershell
[root@localhost conf]# mkdir /1021linux  /1021python
[root@localhost conf]# echo "this is python" > /1021python/index.html
[root@localhost conf]#
[root@localhost conf]# echo "this is linux" > /1021linux/index.html
[root@localhost conf]# nginx -t
nginx: the configuration file /opt/tng232//conf/nginx.conf syntax is ok
nginx: configuration file /opt/tng232//conf/nginx.conf test is successful
# 注意，改了配置文件，一定要平滑重启，否则不生效
[root@localhost conf]# nginx -s reload
# 此时分别访问2个站点，即可看到2个站点的资料
192.168.254.130:85
192.168.254.130:89
```

- nginx的404页面优化

```powershell
# 修改nginx.conf，修改一行参数即可
error_page  404  /40x.html;
```

- nginx的访客日志

```shell
# 日志变量解释
$remote_addr    	 # 记录客户端ip
$remote_user    	 # 远程用户，没有就是 “-”
$time_local 　　 		# 对应[14/Aug/2018:18:46:52 +0800]
$request　　　 　 	   # 对应请求信息"GET /favicon.ico HTTP/1.1"
$status　　　  　	   # 状态码
$body_bytes_sent　　	# 571字节 请求体的大小
$http_referer　　		# 对应“-”　　由于是直接输入浏览器就是 -
$http_user_agent　　  # 客户端身份信息，以此可以nginx判断，用户客户端是手机浏览器，就转发移动端页面给与用户，如果是pc的客户端，就转发给pc页面给与用查看
$http_x_forwarded_for　　# 记录客户端的来源真实ip 97.64.34.118，机器A用机器B的ip去访问，可以抓出机器A的地址，这个参数不是万能的，爬虫和反扒是相互的

# 1.监听日志
[root@localhost tng232]# pwd
/opt/tng232
[root@localhost tng232]# tail -f logs/access.log
192.168.254.1 - - [22/Oct/2021:01:00:19 +0800] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36" "-"
192.168.254.1 - - [22/Oct/2021:01:02:30 +0800] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1" "-"

# 2.浏览器访问
192.168.254.130:85
```

**注意：注意！改了配置文件，一定要平滑重启，否则不生效！**

#### 5.6.4 反向代理

**正向代理**

<img src="https://img2018.cnblogs.com/blog/1132884/201811/1132884-20181118222300086-1972487571.png" alt="img" style="zoom: 50%;" />

正向代理，也就是传说中的代理,他的工作原理就像一个跳板（VPN），简单的说：

我是一个用户，我访问不了某网站，但是我能访问一个代理服务器，这个代理服务器呢，他能访问那个我不能访问的网站，于是我先连上代理服务器，告诉他我需要那个无法访问网站的内容，代理服务器去取回来，然后返回给我。正向代理，代理的是客户端。

作用：

（1）访问原来无法访问的资源，如google

（2） 可以做缓存，加速访问资源

（3）对客户端访问授权，上网进行认证

（4）代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息

**反向代理**

代理服务器，客户机在发送请求时，不会直接发送给⽬的主机，⽽是先发送给代理服务器，代理服务接受客户机请求之后，再向主机发出，并接收⽬的主机返回的数据，存放在代理服务器的硬盘中，再发送给客户机。

作用：

（1）保证内网的安全，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网

（2）负载均衡，通过反向代理服务器来优化网站的负载

<font color="red">正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端. </font>
<font color="red">反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端.</font>

**应用场景**

- 堡垒机场景
- 内网服务器发布场景
- 缓存场景

![1639293901196](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639293901196.png)

![1639293922891](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639293922891.png)

![1639293934357](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639293934357.png)

![1639293943870](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639293943870.png)

**nginx实现反向代理**

![image-20211021220239420](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211021220239420-16348249609553.png)

**反向代理原理**

> 1. 客户端通过浏览器向代理服务器发起请求 
> 2. 代理服务器接受请求
> 3. 代理服务器向业务服务器发起请求
> 4. 业务服务器接收请求
> 5. 业务服务器处理请求
> 6. 业务服务器向代理服务器发送响应
> 7. 代理服务器向客户端发送响应
> 8. 客户端浏览器渲染请求并展示给用户

```shell
location / {
	index index.php index.html index.htm;  # 定义⾸⻚索引⽂件的名称
	proxy_pass http://mysvr ; # 请求转向mysvr 定义的服务器列表
	proxy_set_header Host $host;
	proxy_set_header X-Real-IP $remote_addr;
	proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
	client_max_body_size 10m;  # 允许客户端请求的最⼤单⽂件字节数
	client_body_buffer_size 128k;  # 缓冲区代理缓冲⽤户端请求的最⼤字节数，
	proxy_connect_timeout 90;  # nginx跟后端服务器连接超时时间(代理连接超时)
	proxy_send_timeout 90;  # 后端服务器数据回传时间(代理发送超时)
	proxy_read_timeout 90;  # 连接成功后，后端服务器响应时间(代理接收超时)
	proxy_buffer_size 4k; #  设置代理服务器（nginx）保存⽤户头信息的缓冲区⼤⼩
	proxy_buffers 4 32k;  # proxy_buffers缓冲区，⽹⻚平均在32k以下的话，这样设置
	proxy_busy_buffers_size 64k;  # ⾼负荷下缓冲⼤⼩（proxy_buffers*2）
	proxy_temp_file_write_size 64k;  # 设定缓存⽂件夹⼤⼩，⼤于这个值，将从upstream服务器传
}
```

第一个server{}标签，用于反向代理的作用，修改nginx.conf 如下：

```shell
# 第一个虚拟主机的配置，作用是反向代理了
server {
# 1.定义该网站的端口
listen       80;
# 2.填写域名，没有就默认即可
server_name  localhost;
# 3.更改nginx的编码支持
charset utf-8;
# 4.如此添加一行参数，当用户请求出错，出现404的时候，就返回 root 定义的目录去寻找40x.html文件
# 也就是去配置 /1021python/ 这个目录下 寻找 40x.html
error_page  404  /40x.html;
#access_log  logs/host.access.log  main;

#access_log  "pipe:rollback logs/host.access_log interval=1d baknum=7 maxsize=2G"  main;
# 5.nginx的域名匹配，所有的请求，都会进入到这里
    location / {
    # 6.这里的locaiton 路径匹配，如果你写的是root参数，就是一个web站点功能
    # 如果你写的是proxy_pass参数，就是一个请求转发，反向代理功能
    proxy_pass http://192.168.254.130:90;
    }
}
```

第二个server{}标签，作用是返回机器上的资料，也就是一个web站点的功能

```shell
# 1.第二个虚拟主机，作用是web站点功能，资源服务器，提供页面的
server  {
listen 90;
server_name  _;

# 2.当请求来到192.168.178.140:90的时候，就返回/s25proxy目录下的index.html
    location  / {
    root   /1021proxy/;
    index  index.html;
    }
}

# 3.创建资源文件夹，以及html页面内容
[root@localhost conf]# cat /1021proxy/index.html
我是资源服务器  192.168.254.130 端口是90

# 4.平滑重启
[root@localhost conf]# nginx -s reload

# 5.测试访问代理服务器，查看页面效果
```

![image-20211021215953107](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211021215953107.png)

#### 5.6.5 限速

限流（rate limiting）是NGINX众多特性中最有用的，也是经常容易被误解和错误配置的，特性之一。该特性可以限制某个用户在一个给定时间段内能够产生的HTTP请求数。请求可以简单到就是一个对于主页的GET请求或者一个登陆表格的POST请求。

限速也可以⽤于安全⽬的上，⽐如暴⼒密码破解攻击。通过限制进来的请求速率，并且（结合⽇志）标记出⽬标URLs来帮助防范DDoS攻击。⼀般地说，限流是⽤在保护上游应⽤服务器不被在同⼀时刻的⼤量⽤户请求湮没。

**应用场景**

- DDOS防御
- 下载场景保护IO

**限速原理**

![1639293988653](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639293988653.png)

算法思想：

> ⽔（请求）从上⽅倒⼊⽔桶，从⽔桶下⽅流出（被处理）； 来不及流出的⽔存在⽔桶中（缓冲），以固定速率流出； ⽔桶满后⽔溢出（丢弃）。
>
>  这个算法的核⼼是：缓存请求、匀速处理、多余的请求直接丢弃。
>
> 相比漏桶算法，令牌桶算法不同之处在于它不但有一只“桶”，还有个队列，这个桶是用来存放令牌的，队列才是用来存放请求的。

**实现方式**

Nginx官⽅版本限制IP的连接和并发分别有两个模块：

- limit_req_zone ⽤来限制单位时间内的请求数，即速率限制。采用的漏桶算法"leaky bucket"。
- limit_req_conn ⽤来限制同⼀时间连接数，即并发限制。

**limit_req_zone 参数配置**

- Syntax: limit_req zone=name [burst=number] [nodelay];

- Default: —

- Context: http, server, location

限速案例一：

> ```
> 说明：区域名称为joker，大小为10m，平均处理的请求频率不能超过每秒一次,键值是客户端IP。
> 使用$binary_remote_addr变量， 可以将每条状态记录的大小减少到64个字节，这样1M的内存可以保存大约1万6千个64字节的记录。
> 如果限制域的存储空间耗尽了，对于后续所有请求，服务器都会返回 503 (Service Temporarily Unavailable)错误。
> 速度可以设置为每秒处理请求数和每分钟处理请求数，其值必须是整数，
> 所以如果你需要指定每秒处理少于1个的请求，2秒处理一个请求，可以使用 “30r/m”。
> 
> 说明：limit_rate_after定义当一个文件下载到指定大小（本例中为512k）之后开始限速；
> limit_rate 定义下载速度为150k/s。
> 注意：这两个参数针对每个请求限速。
> ```

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    
	# 基于IP做连接限制 限制同⼀IP并发为1 下载速度为100K
    limit_req_zone $binary_remote_addr zone=joker:10m rate=1r/s;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }

        location /xiansu/ {
             limit_req zone=joker burst=5 nodelay;  # 限制并发数量
             limit_rate 100k;  # 限制100的速度下载
         }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

limit_req_zone $binary_remote_addr zone=joker:10m rate=1r/s;
# 第⼀个参数：$binary_remote_addr 表示通过remote_addr这个标识来做限制，“binary_”的⽬的是缩写内存占⽤量，是限制同⼀客户端ip地址。
# 第⼆个参数：zone=joker:10m表示⽣成⼀个⼤⼩为10M，名字为zone的内存区域，⽤来存储访问的频次信息。
# 第三个参数：rate=1r/s表示允许相同标识的客户端的访问频次这⾥限制的是每秒1次还可以有⽐如30r/m的。

limit_req zone=joker burst=5 nodelay
# 第⼀个参数：zone=joker 设置使⽤哪个配置区域来做限制，与上⾯limit_req_zone ⾥的name对应。
# 第⼆个参数：burst=5，重点说明⼀下这个配置，burst爆发的意思，这个配置的意思是设置⼀个⼤⼩为5的缓冲区当有⼤量请求（爆发）过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。
# 第三个参数：nodelay，如果设置，超过访问频次⽽且缓冲区也满了的时候就会直接返回503，如果没有设置，则所有请求会等待排队。


# 1.以上配置好后，平滑重启，
[root@web131 html]# nginx -s reload

# 2.快速访问链接，模拟并发数量
[root@web131 html]# elinks http:192.168.254.131/xiansu -dump

# 3.下载大文件，限制下载速度
[root@web131 tmp]# wget http://127.0.0.1/xiansu/bigfile
```

限速案例二：

> ```
> 说明：首先用limit_conn_zone定义了一个内存区块索引aming，大小为10m，它以$binary_remote_addr作为key。
> 该配置只能在http里面配置，不支持在server里配置。
> 
> limit_conn 定义针对addr这个zone，并发连接为10个。在这需要注意一下，这个10指的是单个IP的并发最多为1个。
> ```

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    
	# 基于IP对下载速率做限制 限制每秒处理1次请求，对突发超过5个以后的请求放⼊缓存区
    limit_conn_zone $binary_remote_addr zone=addr:10m;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }

        location /xiansu/ {
             limit_conn addr 1;
         }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.平滑重启
[root@web131 tmp]# nginx -t
[root@web131 tmp]# nginx -s reload

# 2.打开两个终端下载文件，限制只允许一次请求
[root@web131 tmp]# wget http://127.0.0.1/xiansu/bigfile
--2021-11-28 10:55:59--  http://127.0.0.1/xiansu/bigfile
Connecting to 127.0.0.1:80... connected.
HTTP request sent, awaiting response... 503 Service Temporarily Unavailable
2021-11-28 10:55:59 ERROR 503: Service Temporarily Unavailable.
```

#### 5.6.6 URL重写

**URL模板语法**

- set：设置变量
- if：负责语句中的判断
- return：返回 响应或URL
- bread：终止后续后rewrite规则
- rewrite：重定向URL

**set指令**：⾃定义变量

> Syntax:
>
> set $variable value;
>
> Default:
>
> —
>
> Context:
>
> server, location, if

将http://192.168.254.131 重写为 http://192.168.254.131/web1

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
			set $name web1;
			rewrite ^(.*)$ http://192.168.254.131/$name;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.平滑重启
[root@web131 tmp]# nginx -t
[root@web131 tmp]# nginx -s reload

# 2.访问192.168.254.131
```

![1639294074414](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294074414.png)

**if and return 指令**：判断和返回数据	

> Syntax:
>
> if (condition) { ... }
>
> Default:
>
> —
>
> Context:
>
> server, location

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
            if ($http_user_agent ~* 'Chrome') {
            return 403;
            #return http://www.jd.com;
            }
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.平滑重启
[root@web131 tmp]# nginx -t
[root@web131 tmp]# nginx -s reload

# 2.chrome访问192.168.254.131，返回403，其他浏览器访问，正确返回
```

**break指令**：停⽌执⾏当前虚拟主机的后续rewrite指令集

> Syntax: break;
>
> Default:—
>
> Context:server, location, if

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
            if ($http_user_agent ~* 'Chrome') {
            break;
            return 403;
            #return http://www.jd.com;
            }
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.平滑重启
[root@web131 tmp]# nginx -t
[root@web131 tmp]# nginx -s reload

# 2.chrome访问192.168.254.131，没有返回403
```

**rewrite指令**

>  rewrite <regex> <replacement> [flflag];
>
> 关键字  	正则  		替代内容 		flag标记

> flag:
>
> last 
>
> \# 本条规则匹配完成后，继续向下匹配新的location URI规则
>
> break 
>
> \# 本条规则匹配完成即终⽌，不再匹配后⾯的任何规则
>
> redirect 
>
> \# 返回302临时重定向，浏览器地址会显示跳转后的URL地址
>
> permanent
>
> 返回301永久重定向，浏览器地址栏会显示跳转后的URL地址

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name localhost;
        location / {
			rewrite ^/$ http://www.jd.com permanent;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.修改hosts文件
[root@web131 conf]# cat /etc/hosts
192.168.254.131 www.ayitula.com

# 2.浏览器访问192.168.254.131
permanent：接口返回状态码301
redirect：接口返回状态码302
break:接口返回状态码302
last:见下图
```

![1639294112119](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294112119.png)

根据⽤户浏览器重写访问⽬录：如果是chrome浏览器 就将 http://192.168.254.131/$URI 重写为 http://192.168.254.131/chrome/$URI

```shell
orker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
                server_name localhost;
        location / {
                        #rewrite ^/$ http://www.jd.com redirect;
                        if ($http_user_agent ~* 'chrome'){
                                rewrite ^(.*)$ /chrome$1 last;
                                 }
                         location /chrome {
                                  root html ;
                                   index index.html;
                                    }
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# 1.修改配置文件，平滑重启
[root@web131 conf]# nginx -s load

# 2.地址栏输入 192.168.254.131/b
```

#### 5.6.7 nginx优化

标准情况下，软件默认的参数都是对安装软件的硬件标准来设置的，⽬前我们服务器的硬件资源远远⼤于要求的标准，所以为了让服务器性能更加出众，充分利⽤服务器的硬件资源，我们⼀般需要优化APP的并发数来提升服务器的性能。

1. **工作进程优化**

优化方案：

- Nginx是主进程+⼯作进程模型
- worker_processes 1； ⼯作进程数量 按CPU的总核⼼调整
- worker_cpu_affiffiffinity 0010 0100 1000; CPU的亲和⼒
- worker_connections 1024； ⼀个⼯作进程的并发数

并发优化

```shell
worker_processes  4;
worker_cpu_affinity 0001 0010 0100 1000;
events {
    worker_connections  10240;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}


# 1.平滑重启
[root@localhost conf]# nginx -t
[root@localhost conf]# nginx -s reload

# 2.查看并发数
[root@localhost conf]# ps -eo psr,pid,args | grep "nginx"
  3   4741 nginx: master process nginx
  0  34994 nginx: worker process
  1  34995 nginx: worker process
  2  34996 nginx: worker process
  3  34997 nginx: worker process
  2  34999 grep --color=auto nginx
[root@localhost conf]# lsof -i :80
COMMAND   PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
nginx    4741   root    6u  IPv4  26352      0t0  TCP *:http (LISTEN)
nginx   34994 nobody    6u  IPv4  26352      0t0  TCP *:http (LISTEN)
nginx   34995 nobody    6u  IPv4  26352      0t0  TCP *:http (LISTEN)
nginx   34996 nobody    6u  IPv4  26352      0t0  TCP *:http (LISTEN)
nginx   34997 nobody    6u  IPv4  26352      0t0  TCP *:http (LISTEN)

# 3.统计连接个数
[root@localhost conf]# netstat -antlp | grep nginx | grep ESTABLISHED -wc -l
```

2. **长连接**

http协议属于TCP协议

优化⽬标:减少三次握⼿和四次断开的次数 

优化方案：

-  keepalive_timeout 5; ⻓连接时间  
-  keepalive_requests 8192; 每个⻓连接接受最⼤请求数 

```shell
worker_processes  4;
worker_cpu_affinity 0001 0010 0100 1000;
events {
    worker_connections  10240;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout 5;
    keepalive_requests 8192;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

# timeout修改成0，代表关闭，看响应头里的keepalive
```

3. **数据压缩**

- gzip on;   （启⽤ gzip 压缩功能）
- gzip_proxied any; （nginx 做前端代理时启⽤该选项，表示⽆论后端服务器的headers头返回什么信息，都⽆条件启⽤压缩）
- gzip_min_length 1024; （最⼩压缩的⻚⾯，如果⻚⾯过于⼩，可能会越压越⼤，这⾥规定⼤于1K的⻚⾯才启⽤压缩） 
-  gzip_buffers   4 8k; （设置系统获取⼏个单位的缓存⽤于存储gzip的压缩结果数据流 按照原始数据⼤⼩以8K为单位申请4倍内存空间） 
-  gzip_comp_level 3; （压缩级别，1压缩⽐最⼩处理速度最快，9压缩⽐最⼤但处理最慢，同时也最消耗CPU,⼀般设置为3就可以了） 
-  gzip_types    text/plain text/css application/x-javascript application/javascript application/xml; （什么类型的⻚⾯或⽂档启⽤压缩）  

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    gzip  on;

   gzip_proxied any;
   gzip_min_length 1k;
   gzip_buffers 4 8k;
   gzip_comp_level 6;
   gzip_types text/plain text/css application/x-javascript application/javascript
   application/xml;


    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}


# 访问页面，打开控制台查看压缩时间
```

4. **客户端缓存**

语法： expires [time|epoch|max|off] 

默认值： expires off 

作⽤域： http, server, location 

```shell
 server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }

        location ~* \.(png|gif)$ {
             expires 1h;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 测试: 浏览器刷新，以Chrome为例：
# 1. ctrl+f5 清空本地缓存从服务器拿数据
# 2. F5或者 点击 浏览器的刷新图标 优先从本地找 然后 去找服务器核对信息
# 3. 是否一致 一致 返回304 从本地那数据
# 4. 回车 从本地缓存拿数据 
```

#### 5.6.6 nginx集群

##### 5.6.6.1 集群简介

**集群介绍**

​	简单的说，集群就是指一组（若干个）相互独立的计算机，利用高速通信网络组成的一个较大的计算机服务系统，每个集群节点（即集群中的每台计算机）都是运行各自服务的独立服务器，这些服务器之间可以彼此通信，协同向提供应用程序、系统资源和数据，并以单一系统的模式加以管理。当用户客户机请求集群系统时，集群给用户的感觉就是一个单一独立的服务器，而实际上用户请求的是一组集群服务器。

​	打开百度等互联网网站，看起来简单，也许你觉得用几分钟就可以制作出相似的网页，而实际上，这个页面的背后是由成千上万台服务器集群协同工作的结果，而这么多的服务器维护和管理，以及相互协调工作就是运维工程师的工作职责了。

​	若要用一句话描述集群，即一堆服务器合作做同一件事，这些机器可能需要整个技术团队架构、设计和统一协调管理，这些机器可以分布在一个机房，也可以分布在全国全球各个地区的多个机房。

**集群特点**

想了解为什么要使用集群，就要从集群的特点开始讲。

**1. 高性能（Performance）**

​	一些国家重要的计算密集型应用（如天气预报、核试验模拟等），需要计算机有很强的运算处理能力。以全世界现有的技术，即使是大型机，其计算能力也是有限的，很难单独完成此任务，因为计算时间可能会相当长，也许几天，甚至几年或更久。因此，对于这类复杂的计算业务，便使用了计算机集群技术，几种几十上百台，甚至成千上万台计算机进行计算。

现在大型网站，如，谷歌、百度、淘宝等，都不是几台大型机可以构建的，都是上万台服务器组成的高性能集群，分布在不同的地点。

**2. 价格有效性（Cost-effectiveness）**

​	通常一条系统集群架构，只需要几台或数十台服务器主机即可。与动辄价值上百万的专用超级计算机相比便宜了很多。在达到同样性能需求的条件下，采用计算机集群架构比采用同等运算能力的大型计算机具有更高的性价比。

​	早期的淘宝、支付宝的数据库等核心系统就是使用上百万的小型机服务器。后因使用维护成本太高以及扩展设备费用成几何级数翻倍，甚至成为扩展瓶颈，人员维护也十分困难，最终使用PC服务器集群替换，比如，把数据库系统从小结合Oracle数据库迁移到了MySQL开源数据库结合PC服务器上来。不但成本下降了，扩展和维护也更容易了。

**3. 可伸缩性（Scalability）**

​	当服务负载、压力增长时，针对集群系统进行较简单的扩展即可满足需求，且不回降低服务质量。

​	通常情况下，硬件设备若想扩展性能，不得不增加新的CPU和存储设备，如果加不上去了，就不得不购买更高性能的服务器。就拿我们现有的服务器来讲，可以增加的设备总是有限的，如果采用集群技术，则只需要将新的单个服务器加入现有集群架构中即可，从访问的客户角度来看，系统服务无论是连续性还是性能上都几乎没有变化，系统在不知不觉中完成了升级，加大了访问能力，轻松地实现了扩展。集群系统中的节点数目可以增长到几千乃至上万个，其伸缩性远超过单台超级计算机。

**4. 高可用性（Availability）**

​	单一的计算机系统总会面临设备损毁的问题，如CPU、内存、主板、电源、硬盘等，只要一个部件坏掉，这个计算机系统就可能宕机，无法正常提供服务。在集群系统中，尽管部分硬件和软件也还是会发生故障，但整个系统的服务可以是7*24 可用。

​	集群架构技术可以使得系统在若干硬件设备故障发生时仍可以继续工作，这样就将系统的停机时间减小到最小。集群系统在提高系统可靠性的同时，也大大减小了系统故障带来的业务损失，目前几乎100%的互联网网站都要求7*24小时提供服务。

**5. 透明性（Transparency）**

​	多个独立计算机组成的松耦合集群系统构成一个虚拟服务器。用户或客户端程序访问集群系统时，就像访问一台高性能、高可哦那个的服务器一样，集群中一部分服务器的上、下线不会中断整个系统服务，这对用户也时透明的。

##### 5.6.6.2 nginx集群原理

Nginx集群其实是：虚拟主机+反向代理+upstream分发模块组成的

- 虚拟主机：接受和响应请求
- 反向代理: 带⽤户去数据服务器拿数据
- upstream: 告诉Nginx去哪个数据服务器拿数据

数据⾛向：

1. 虚拟主机接受⽤户请求
2. 虚拟主机去找反向代理
3. 反向代理让去找upstream
4. upstream 告诉 ⼀个数据服务器IP
5. Nginx去找数据服务器 并发起⽤户的请求
6. 数据服务器接受请求并处理请求
7. 数据服务器响应请求给Nginx
8. Nginx响应请求给⽤户

##### 5.6.6.3 集群的实现

![1639294233000](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294233000.png)

方案一：单点故障解决方案

> 1.部署⼀台备份服务器，宕机直接切换
>
> 2.部署多台服务器,根据DNS的轮询解析机制去实现⽤户分发
>
> 问题：
>
> 1.服务器利⽤率低，成本⾼，切换不及时，服务器压⼒依然⼤
>
> 2.优势是⽤户处理速度得到了提升，但是当其中⼀台故障，就会有⼀部分⽤户访问不了⽹站

方案二：并行处理解决方案

> 1.上述的DNS轮询解析⽅案
>
> 2.多机阵列---集群模式

![1639294220464](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294220464.png)

**集群实现**

将多个物理机器组成⼀个逻辑计算机，实现负载均衡和容错；

计算机集群简称集群，是⼀种计算机系统， 它通过⼀组松散集成的计算机软件或硬件连接起来⾼度紧密地协作完成计算 ⼯作。在某种意义上，他们可以被看作是⼀台计算机。 （百度解释）

组成要素：

- VIP：⼀个IP地址
- 分发器：nginx
- 数据服务器：web服务器

**nginx集群**

- 在该集群中Nginx扮演的⻆⾊是： 分发器
- 任务：接受请求、分发请求、响应请求
- 功能模块：
  - ngx_http_upstream_module 基于应⽤层分发模块
  - ngx_stream_core_module 基于传输层分发模块 （1.9开始提供）

1. 数据服务器1：relase1

```shell
# 1.安装nginx后，index写入内容，启动nginx
[root@localhost ~]# echo relase1 > /usr/local/nginx/html/index.html
[root@localhost ~]# nginx

# 2.访问本地，确认nginx已开启
[root@localhost ~]# elinks http://localhost -dump
```

2. 数据服务器2：relase2

```shell
# 1.安装nginx后，index写入内容，启动nginx
[root@localhost ~]# echo relase2 > /usr/local/nginx/html/index.html
[root@localhost ~]# nginx

# 2.访问本地，确认nginx已开启
[root@localhost ~]# elinks http://localhost -dump
```

3. 分发器：master

```shell
# 1.nginx配置如下
[root@localhost conf]# cat nginx.conf
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web {
        server 192.168.88.129;
        server 192.168.88.130;
    }

    server {
        listen       80;
        server_name  localhost;
        location / {
            proxy_pass http://web;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

4. 数据服务器1访问：

```shell
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
```

##### 5.6.6.4 nginx服务器状态及默认算法

```shell
# nginx服务器状态
nginx的upstream⽬前⽀持4种⽅式的分配
# 1.轮询（默认）
每个请求按时间顺序逐⼀分配到不同的后端服务器，如果后端服务器down掉，能⾃动剔除。

# 2.weight
指定轮询⼏率，weight和访问⽐率成正⽐，⽤于后端服务器性能不均的情况。

# 3.ip_hash
每个请求按访问ip的hash结果分配，这样每个访客固定访问⼀个后端服务器，可以解决session的问题。

# 4.fair（第三⽅）
按后端服务器的响应时间来分配请求，响应时间短的优先分配。

# 5.url_hash（第三⽅）
按访问url的hash结果来分配请求，使每个url定向到同⼀个后端服务器，后端服务器为缓存时⽐较有效。
```

> nginx服务器算法
>
> 每个设备的状态设置为:
>
> 1. down 表示当前的server暂时不参与负载
> 2. weight 默认为1，weight越⼤，负载的权重就越⼤。
> 3. max_fails ：允许请求失败的次数默认为1，当超过最⼤次数时，返回proxy_next_upstream 模块定义的错误
> 4. fail_timeout:失败超时时间，在连接Server时，如果在超时时间之内超过max_fails指定的失败次数，会认为在fail_timeout时间内Server不可⽤，默认为10s。
> 5. backup： 其它所有的⾮backup机器down或者忙的时候，请求backup机器。所以这台机器压⼒会最轻。

1. 轮询分发

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web {
        server 192.168.88.129;
        server 192.168.88.130;
    }

    server {
        listen       80;
        server_name  localhost;
        location / {
            proxy_pass http://web;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
```

2. 基于权重算法

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web {
        server 192.168.88.129 weight=1;
        server 192.168.88.130 weight=2;
    }

    server {
        listen       80;
        server_name  localhost;
        location / {
            proxy_pass http://web;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}

[root@localhost conf]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost conf]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost conf]# elinks http://192.168.88.128 -dump
   relase2

```

3. ip_hash：ip_hash算法能够保证来⾃同样源地址的请求，都分发到同⼀台主机

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web {
        ip_hash;
        server 192.168.88.129;
        server 192.168.88.130 down;
    }

    server {
        listen       80;
        server_name  localhost;
        location / {
            proxy_pass http://web;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}


# 两个数据服务器访问，都分发到同一台主机
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
   
# 如果数据服务器用down标识
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase1
```

4. back_up

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web {
        server 192.168.88.129 down;
        server 192.168.88.130 backup;
    }

    server {
        listen       80;
        server_name  localhost;
        location / {
            proxy_pass http://web;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}


# 129加入down，模拟relase1服务器挂掉，则访问relase2服务器，relase1忙不过来的时候则分发到relase2服务器
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost ~]# elinks http://192.168.88.128 -dump
   relase2
```

##### 5.6.6.5 nginx分发

**基于host多集群分发**

通过域名分发，实现分发到不同的服务器下的host，适用于多个网站

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web1 {
        server 192.168.88.129;
    }
    upstream web2 {
        server 192.168.88.130;
    }

# relase1
    server {
        listen       80;
        server_name  www.relase1.com;
        location / {
            proxy_pass http://web1;
        }
    }
# relase2
    server {
        listen       80;
        server_name  www.relase2.com;
        location / {
            proxy_pass http://web2;
        }
    }
}

# 1.配置relase集群服务器
# 2.设置域名
# 3.在业务服务器设置域名
192.168.88.128 www.relase1.com
192.168.88.128 www.relase2.com
# 4.访问域名
[root@localhost ~]# elinks http://www.relase1.com -dump
   relase1
[root@localhost ~]# elinks http://www.relase2.com -dump
   relase2
```

**基于开发语言分发**

适用于混合开发网站

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream php {
        server 192.168.88.129;
    }
    upstream html {
        server 192.168.88.130;
    }

    server {
        listen  80;
        server_name localhost;
        location ~* \.php$ {
            proxy_pass http://php;
        }

         location ~* \.html$ {
            proxy_pass http://html;
        }
    }
}

# 1.relase1集群中的服务器安装php
[root@localhost ~]# yum -y install httpd php

# 2.停掉nginx服务器，开启httpd
[root@localhost ~]# pkill -9 nginx
[root@localhost ~]# systemctl start httpd

# 3.写入php页面
[root@localhost ~]# echo "<?php phpinfo(); ?>" > /var/www/html/index.php

# 4.浏览器访问
http://192.168.88.128/index.html
http://192.168.88.128/index.php
```

**浏览器分发**

通过不同的浏览器访问，返回的不一样，适用于不同浏览器或浏览器与移动端区分

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream elinks { server 192.168.88.129; }
    upstream chrome { server 192.168.88.130; }
    upstream any { server 192.168.88.129:81; }

    server {
        listen  80;
        server_name localhost;

        location / {
            proxy_pass http://any;

            if ( $http_user_agent ~* elinks ) {
                proxy_pass http://elinks;
            }

            if ( $http_user_agent ~* chrome ) {
                 proxy_pass http://chrome;
            }
         }
    }
}

# 1.relase1集群中的服务器停掉httpd
[root@localhost ~]# systemctl start httpd

# 2.写入页面，配置nginx
[root@localhost nginx]# mkdir web1
[root@localhost nginx]# echo web1 > web1/index.html
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

    server {
        listen       81;
        server_name  localhost;
        location / {
            root   web1;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

}

# 3.master修改配置文件
[root@localhost conf]# cat nginx.conf
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream elinks { server 192.168.88.129; }
    upstream chrome { server 192.168.88.130; }
    upstream any { server 192.168.88.129:81; }

    server {
        listen  80;
        server_name localhost;

        location / {
            proxy_pass http://any;

            if ( $http_user_agent ~* elinks ) {
                proxy_pass http://elinks;
            }

            if ( $http_user_agent ~* chrome ) {
                 proxy_pass http://chrome;
            }
         }
    }
}

# 4.适用不同的浏览器访问192.168.88.128，根据浏览器进行分发返回结果
```

**源IP分发**

```shell
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;


    upstream bj.server {
        server 192.168.88.129;
    }

    upstream sh.server {
        server 192.168.88.130;
    }

    upstream default.server {
        server 192.168.88.129:81;
    }

    geo $geo {
        default default;
        192.168.88.149/32 bj;
        192.168.88.139/32 sh;
    }

    server {
        listen  80;
        server_name localhost;

        location / {
            proxy_pass http://$geo.server$request_uri;
         }
    }
}
# 测试服务器地址192.168.88.129
# 192.168.88.129/32 bj：对应的找bj的服务器
# 192.168.88.129/32 sh：对应的找sh的服务器
# 没有匹配到129、139的机器，走的是默认的服务器129.81
[root@localhost conf]# elinks http://192.168.88.128 -dump
   relase2
[root@localhost conf]# elinks http://192.168.88.128 -dump
   relase1
[root@localhost conf]# elinks http://192.168.88.128 -dump
   web1
```

##### 5.6.6.6 构建高可用集群

**高可用介绍**

![1639294313132](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294313132.png)

- 分发器宕机怎么办
- 数据服务器宕机怎么办

Keepalived的作⽤是检测服务器的状态，如果有⼀台web服务器宕机，或⼯作出现故障， Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使⽤其他服务器代替该服务器的⼯作，当服务器⼯作正常后Keepalived⾃动将服务器加⼊到服务器群中，这些⼯作全部⾃动完成，不需要⼈⼯⼲涉，需要⼈⼯做的只是修复故障的服务器。

运⾏ vrrp 协议；主分发器的Keepalived会向⽹络中发主播，告诉自己还活着（224.0.0.18）

分发器上安装：Nginx+keepalvied来实现高可用的配置

1. 分发器安装nginx+keepalived

```shell
# 1.在主分发器获取tar包
[root@localhost src]# wget http://www.keepalived.org/software/keepalived-2.0.8.tar.gz -P /usr/src/

# 2.编写安装脚本
[root@localhost src]# cat keepalived_install.sh
#!/bin/bash
pkg=keepalived-2.0.8.tar.gz
tar xf $pkg
yum -y install kernel-devel
ln -s /usr/src/kernels/3.10.0-862.14.4.el7.x86_64/ /usr/src/linux
cd keepalived-2.0.8/
yum install openssl-* -y
./configure --prefix=/usr/local/keepalived
make
make install
mkdir -pv /etc/keepalived
cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/
keepalived/
ln -s /usr/local/keepalived/sbin/keepalived /sbin/

# 3.执行安装脚本
[root@localhost src]# sh keepalived_install.sh

# 4.安装成功后把安装包和脚本拷贝到备用分发器，安装一次
[root@localhost src]# scp keepalived_install.sh root@192.168.88.131:/usr/src/
[root@localhost src]# scp keepalived-2.0.8.tar.gz root@192.168.88.131:/usr/src/
```

2. 配置数据服务器 Nginx

```shell
# 1.安装nginx并启动
```

3. 配置Nginx分发器

```shell
# 1..在主分发器配置如下
[root@localhost conf]# cat nginx.conf
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    upstream web{
        server 192.168.88.129 max_fails=2 fail_timeout=3;
        server 192.168.88.130 max_fails=2 fail_timeout=3;
    }

    server {
        listen  80;
        server_name localhost;

        location / {
            proxy_pass http://web;
         }
    }
}

# 2.拷贝到备分发器
[root@localhost keepalived]# scp /usr/local/nginx/conf/nginx.conf root@192.168.88.131:/usr/local/nginx/conf/
```

4. 配置keepalived.conf

```shell
# 1.主分发器配置keepalived，如下
[root@localhost keepalived]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id NGINX_DEVEL
}

vrrp_script check_nginx {
    script "/etc/keepalived/nginx_pid.sh"
    interval 2
    fall 1
}
vrrp_instance nginx {
    state MASTER
    interface ens33
    mcast_src_ip 192.168.88.128
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
    auth_type PASS
    auth_pass 1111
}

track_script {
    check_nginx
}

virtual_ipaddress {
    192.168.88.213/24
}
}

# 2.编写nginx检查脚本
[root@localhost keepalived]# cat /etc/keepalived/nginx_pid.sh
#!/bin/bash
nginx_kp_check () {
nginxpid=`ps -C nginx --no-header |wc -l`
if [ $nginxpid -eq 0 ];then
    /usr/local/nginx/sbin/nginx
    sleep 1
    nginxpid=`ps -C nginx --no-header |wc -l`
    if [ $nginxpid -eq 0 ];then
    systemctl stop keepalived
    fi
fi
}
nginx_kp_check

# 3.授予执行的权限
[root@localhost keepalived]# chmod 755  nginx_pid.sh

# 4.把nginx配置文件、keepalived配置文件和nginx检测脚本拷贝到备分发器，授予执行权限
[root@localhost keepalived]# scp /etc/keepalived/keepalived.conf root@192.168.88.131:/etc/keepalived/
[root@localhost keepalived]# scp nginx_pid.sh root@192.168.88.131:/etc/keepalived/

# 5.备分发器配置keepalived，如下
[root@localhost keepalived]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id NGINX_DEVEL
}

vrrp_script check_nginx {
    script "/etc/keepalived/nginx_pid.sh"
    interval 2
    fall 1
}
vrrp_instance nginx {
    state BACKUP
    interface ens33
    mcast_src_ip 192.168.88.131
    virtual_router_id 51
    priority 90
    advert_int 1
    authentication {
    auth_type PASS
    auth_pass 1111
}

track_script {
    check_nginx
}

virtual_ipaddress {
    192.168.88.213/24
}
}
```

5. 测试：模拟分发器宕机，模拟数据服务器宕机

```shell
# 10.查看keepalived状态，启动主、备分发器keepalived，启动nginx检测脚本
[root@localhost keepalived]# systemctl status keepalived
[root@localhost keepalived]# systemctl restart keepalived
[root@localhost keepalived]# sh nginx_pid.sh
[root@localhost keepalived]# ip add show

# 11.抓包检查
[root@localhost keepalived]# tcpdump -nn -vvv -i ens33 vrrp

# 12.模拟分发器宕机测试
[root@localhost keepalived]# watch -n1 killall nginx
[root@localhost keepalived]# lsof -i :80

# 13.查看主/备分发器keepalived状态及备份ip
[root@localhost keepalived]# systemctl status keepalived
[root@localhost keepalived]# ip add show

# 14.模拟129数据服务器宕机测试，访问虚拟ip
[root@localhost html]# pkill nginx
[root@localhost ~]# elinks http://192.168.88.213 -dump
   relase2
```

##### 5.6.6.7 nginx负载均衡概述

Web服务器，直接面向用户，往往要承载大量并发请求，单台服务器难以负荷，使用多台WEB服务器组成集群，前端使用Nginx负载均衡，将请求分散的打到我们的后端服务器集群中，实现负载的分发。那么会大大提升系统的吞吐率、请求性能、高容灾。

<img src="https://img2018.cnblogs.com/blog/1132884/201811/1132884-20181122233808543-706880577.png" alt="img"  />

> Nginx要实现负载均衡需要用到proxy_pass代理模块配置
>
> Nginx负载均衡与Nginx代理不同地方在于：
>
> Nginx代理仅代理一台服务器，而Nginx负载均衡则是将客户端请求代理转发至一组upstream虚拟服务池；
>
> Nginx可以配置代理多台服务器，当一台服务器宕机之后，仍能保持系统可用。

**nginx负载均衡环境的搭建**

第一个虚拟主机server{}的作用，是反向代理，80端口

**注：负载均衡的算法，默认是轮询机制，一台服务器处理一次**

```shell
# 在nginx.conf > http 区域中
# 1.用upstream关键词定义负载均衡池，写入资源服务器的地址
upstream 1021real_server {
	server 192.168.254.130:90;
	server 192.168.254.130:95;
}

# 在nginx.conf > http 区域 >  server区域  > location配置中
# 添加proxy_pass

# 2.修改配置如下
server {
	listen       80;
	server_name  localhost;
	charset utf-8;
	error_page  404  /40x.html;
	# 这里的locaiton 路径匹配，如果你写的是root参数，就是一个web站点功能
    # 如果你写的是proxy_pass参数，就是一个请求转发，反向代理功能
    location / {
    #当请求发送给  192.168.254.130:80的时候
    #直接通过如下的参数，转发给资源池的端口
    proxy_pass http://1021real_server;
    }
}
```

第二个server{}标签的配置，作用是提供资源给用户看的，90端口

```shell
# 第二个虚拟主机，作用是web站点功能，资源服务器，提供页面的
server  {
    listen 90;
    server_name  _;
# 当请求来到 192.168.254.130:90的时候，就返回/s25lol目录下的index.html
    location  / {
    root   /1021lol/;
    index  index.html;
	}
}
```

第三个server{}标签的作用，同样是返回资源页面，查看负载均衡效果的，95端口

```shell
# 第三个server{}虚拟主机，作用是 提供资源服务器的内容的
server {
	listen 95;
	server_name _;
# 当请求来到 192.168.254.130:95的时候，就返回/1021drf目录下的index.html
    location /{
    	root /1021drf/;
    	index index.html;
    }
}
```

分别准备2个资源服务器的内容

```shell
[root@localhost conf]# mkdir /1021drf  /1021lol
[root@localhost conf]# vim /1021lol/index.html
[root@localhost conf]# vim /1021drf/index.html
[root@localhost conf]# nginx -t
[root@localhost conf]# nginx -s reload

# 访问：192.168.254.130：80端口，轮询机制访问服务器
```

#### 5.6.7 负载均衡算法

```shell
# 1.默认是轮询机制，每台服务器处理一次
# 2.加权轮询，修改nginx.conf如下，给与机器不同的权重
upstream 1021real_server {
	server 192.168.254.130:90 weight=4;
	server 192.168.254.130:95 weight=1;
}

# 访问：192.168.254.130：80端口，加权轮询，90端口访问4次切换，95端口访问一次
```

| 调度算法   | 概述                                                         |
| ---------- | ------------------------------------------------------------ |
| 轮询       | 时间顺序逐一分配到不同的后端服务器(默认)                     |
| weight     | 加权轮询,weight值越大,分配到的访问几率越高                   |
| ip_hash    | 每个请求按访问IP的hash结果分配,这样来自同一IP的固定访问一个后端服务器 |
| url_hash   | 按照访问URL的hash结果来分配请求,是每个URL定向到同一个后端服务器 |
| least_conn | 最少链接数,那个机器链接数少就分发                            |

1. 轮询(不做配置，默认轮询)

2. weight权重(优先级)

3. ip_hash配置，根据客户端ip哈希分配，不能和weight一起用

<img src="https://img2018.cnblogs.com/blog/1132884/201811/1132884-20181123121107048-1594518031.png" alt="img" style="zoom:50%;" />

### 5.7 前后端项目不分离部署

基于：**supervisor** + **nginx** + **uwsgi** + **django** + **virtualenv** + **mariadb** 部署django前后端不分离代码。

**1. 安装虚拟环境 virtualenv**

```shell
# 1.安装virtualenv
[root@localhost tng232]# pip3 install virtualenv

# 2.安装virtualenvwrapper
[root@localhost tng232]# pip3 install virtualenvwrapper

# 3.先创建一个目录放置虚拟环境
[root@localhost tng232]# mkdir ~/.virtualenvs

# 4.查看virtualenvwrapper的安装路径
[root@localhost bin]# sudo find / -name virtualenvwrapper.sh
/opt/python369/bin/virtualenvwrapper.sh

# 5.配置环境变量
[root@localhost tng232]# vim ~/.bashrc

	# 4.1 根据自己的环境修改路径追加到配置文件中
    export VIRTUALENVWRAPPER_PYTHON=/opt/python369/bin/python3.6  # python安装目录
    export WORKON_HOME=$HOME/.virtualenvs
    source /opt/python369/bin/virtualenvwrapper.sh  # virtualenvwrapper安装目录

# 6.配置完成之后需要执行如下命令，才可以让编辑后的配置文件生效 
[root@localhost bin]# source ~/.bashrc
virtualenvwrapper.user_scripts creating /root/.virtualenvs/premkproject
virtualenvwrapper.user_scripts creating /root/.virtualenvs/postmkproject
virtualenvwrapper.user_scripts creating /root/.virtualenvs/initialize
virtualenvwrapper.user_scripts creating /root/.virtualenvs/premkvirtualenv
virtualenvwrapper.user_scripts creating /root/.virtualenvs/postmkvirtualenv
virtualenvwrapper.user_scripts creating /root/.virtualenvs/prermvirtualenv
virtualenvwrapper.user_scripts creating /root/.virtualenvs/postrmvirtualenv
virtualenvwrapper.user_scripts creating /root/.virtualenvs/predeactivate
virtualenvwrapper.user_scripts creating /root/.virtualenvs/postdeactivate
virtualenvwrapper.user_scripts creating /root/.virtualenvs/preactivate
virtualenvwrapper.user_scripts creating /root/.virtualenvs/postactivate
virtualenvwrapper.user_scripts creating /root/.virtualenvs/get_env_details


# 7.mkvirtualenv -p python3 创建的虚拟环境名称
[root@localhost bin]# mkvirtualenv -p python3 crm_nginx

# 8.查看已创建的虚拟环境
(crm_nginx) [root@localhost bin]# lsvirtualenv
crm_nginx
=========

# 9.进入虚拟环境目录
[root@localhost bin]# workon crm_nginx

# 10.快捷进入虚拟环境目录
(app01) [root@localhost bin]# cdvirtualenv
bin/  lib/

# 11.快捷进入虚拟环境包安装目录
(app01) [root@localhost bin]# cdsitepackages
(app01) [root@localhost site-packages]# pwd
/root/.virtualenvs/app01/lib/python3.6/site-packages

# 12.退出当前虚拟环境
(crm_nginx) [root@localhost bin]# deactivate

# 13.删除虚拟环境
[root@localhost site-packages]# rmvirtualenv app01
Removing app01...

# 14.查看虚拟环境中已经安装的包
[root@localhost .virtualenvs]# pip3 list
[root@localhost .virtualenvs]# pip3 freeze
```

**2. 上传代码到linux中，调试项目是否能够运行**

```shell
# 1.解压代码
(crm_nginx) [root@localhost cs_book02]# unzip cs_book02.zip

# 2.安装jdanggo==2.0.1
(crm_nginx) [root@localhost cs_book02]# pip3 install -i https://pypi.douban.com/simple django==2.0.1

# 3.安装pymsql
(crm_nginx) [root@localhost cs_book02]# pip3 install -i https://pypi.douban.com/simple pymysql

# 4.缺少mysql，因此需要安装mariadb
(crm_nginx) [root@localhost cs_book02]# yum install mariadb-server mariadb -y

# 5.启动mariadb数据库
[root@localhost opt]# systemctl start mariadb

# 6.还要注意，由于数据库是空的，还得进行数据库表的导入，导出本地数据库
# 参数--all-databases能够导出所有的数据库、表，也可以指定某一个数据库、表导出
mysqldump  -uroot -p  --all-databases   >  alldb.sql 

# 7.通过lrzsz、scp、xftp等方式，发送alldb.sql文件，给linux服务器，再进行数据导入
(crm_nginx) [root@localhost cs_book02]# mysql -uroot -p < alldb.sql
ERROR 1911 (HY000) at line 1009: Unknown option 'STATS_PERSISTENT'

# 注：导入数据会出现这样的报错，这时兼容性问题，不影响数据的导入

# 8.项目就可以启动了
(crm_nginx) [root@localhost cs_book02]# python3 manage.py runserver
Performing system checks...
```

**3. 安装uwsgi及supervisor修改配置**

```shell
# 1.在线上，是使用uwsgi结合uwsgi.ini配置文件，启动crm的，因此 ，启动方式如下
(crm_nginx) [root@localhost cs_book02]# pip3 install -i https://pypi.douban.com/simple uwsgi

# 2.修改uwsgi.ini配置，关闭http参数
(crm_nginx) [root@localhost auto_crm]# vim uwsgi.ini
    # 填写crm项目的第一层绝对路径
    chdir           = /opt/cs_book02
    # 这里填写的不是路径，是以上一个参数为相对，找到主应用目录下的wsgi.py文件
	module          = cs_book02.wsgi
	# 填写虚拟环境解释器的第一层工作目录
	home            = /root/.virtualenvs/crm_nginx
	socket          = 0.0.0.0:8000

# 3.使用supervisor启动uwsgi进程，需要修改supervisord.conf配置文件
(crm_nginx) [root@localhost cs_book02]# vim /etc/supervisord.conf
	# 3.1  找到虚拟环境uwsgi的绝对路径  /root/.virtualenvs/crm_nginx/bin
	# 3.2  找到创建uwsgi.ini配置文件的绝对路径  /opt/cs_book02/uwsgi.ini
command=/root/.virtualenvs/crm_nginx/bin/uwsgi --ini /opt/cs_book02/uwsgi.ini

# 4.检查后台的状态，以及启动命令
(crm_nginx) [root@localhost auto_crm]# ps -ef | grep supervisor
root       7777   1648  0 21:35 pts/0    00:00:00 grep --color=auto supervisor
(crm_nginx) [root@localhost auto_crm]# ps -ef | grep uwsgi
root       7787   1648  0 21:35 pts/0    00:00:00 grep --color=auto uwsgi

(crm_nginx) [root@localhost auto_crm]# supervisord -c /etc/supervisord.conf
Unlinking stale socket /tmp/supervisor.sock
(crm_nginx) [root@localhost auto_crm]# supervisorctl -c /etc/supervisord.conf
supervisor                       RUNNING   pid 7820, uptime 0:00:13
supervisor>
```

**4. 修改nginx配置**

```shell
# 1.修改nginx配置
(crm_nginx) [root@localhost conf]# pwd
/opt/tng232/conf
(crm_nginx) [root@localhost conf]# vim nginx.conf

server {
# 定义网站端口
listen       80;
# 填写域名，没有就默认即可
server_name  localhost;
# 这是一个局部的变量，只对当前这个server{}代码块生效，编码设置为utf-8
charset utf-8;
error_page  404  /40x.html;
	# 这里的locaiton 路径匹配，如果你写的是root参数，就是一个web站点功能
	# 基于uwsgi协议的一个高性能的反向代理转发，新的参数
    location / {
    # 当请求发送给192.168.254.130:80的时候
    # 通过uwsgi_pass把请求发送给后端的uwsgi服务器
    uwsgi_pass 0.0.0.0:8000;
    # 这个参数是固定的，是添加一些转发请求头参数，导入到nginx的转发固定用法，在请求转发的时候，也能保留请求的信息
    include uwsgi_params;
    }
}

# 2.重启nginx
(crm_nginx) [root@localhost cs_book02]#  nginx -s reload

# 3.访问网站，发现静态文件丢失
http://192.168.254.130/books/
http://192.168.254.130/static/bs/css/bootstrap.css

# 4.配置nginx接收所有的django静态文件
(crm_nginx) [root@localhost cs_book02]#  vim settings.py
	# 4.1修改以下内容
    ALLOWED_HOSTS = ["*"]  # 提前加*否则无法访问地址
    STATIC_ROOT = '/book_static/' # 作用是定义一个统一管理的目录，收集所有crm的静态文件
    
    # 4.2用命令收集静态文件
	python3 manage.py collectstatic
	
# 5.修改配置文件后重启supervisor
(crm_nginx) [root@localhost cs_book02]# supervisorctl -c /etc/supervisord.conf
supervisor                       RUNNING   pid 14906, uptime 0:01:09
supervisor> restart supervisor
supervisor: stopped
supervisor: started

# 6.loccation下边新增加location
    # nginx的locaction{}代码块用于批片nginx的url，对于django静态文件的匹配，写入以下内容：
    
    location /static{
                alias /book_static;  # settings中STATIC_ROOT的值
            }
            
    # 这个配置的意义是：
    # 当请求django的static文件下的静态文件的时候，都告诉nginx，去linux的/book_static目录下进行匹配，就等于是重定向到/book_static目录。
```

基于：**Cnetos7.5** + **Nginx** + **python** + **Django** + **uwsgi** + **mysql**

实现部署流程：

1. 安装nginx
2. 安装python
3. 安装mysql
4. 部署发布平台
5. 测试 

- 安装nginx见以上章节
- 安装mysql

前期准备：

1.依赖包：cmake命令 2.8以上

> https://cmake.org/download/

2.boost Boost库是⼀个可移植、提供源代码的C++库，作为标准库的后备，是C++标准化进 

程的开发引擎之⼀

> https://www.boost.org/

3.mysql

> https://dev.mysql.com/downloads/mysql/5.7.html#downloads

**安装mysql**

```shell
# 1.配置开发环境
[root@localhost sbin]# yum -y install ncurses-devel gcc-* bzip2-* bison

# 2.安装cmake
[root@localhost src]# wget https://github.com/Kitware/CMake/releases/download/v3.22.0/cmake-3.22.0.tar.gz /usr/src/
[root@localhost src]# tar -zxvf cmake-3.22.0.tar.gz
[root@localhost cmake-3.22.0]# cd cmake-3.22.0
[root@localhost cmake-3.22.0]# ./configure
[root@localhost cmake-3.22.0]# make
[root@localhost cmake-3.22.0]# make install

# 3.安装boost
[root@localhost cmake-3.22.0]# wget https://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gz/download /usr/src
[root@localhost src]# tar -zxvf boost_1_59_0.tar.gz
[root@localhost src]# mv boost_1_59_0 /usr/local/boost

# 4.安装mysql，官网下载
[root@localhost src]# tar -zxvf mysql-5.7.24.tar.gz
[root@localhost src]# mkdir -pv /usr/local/mysql/data
[root@localhost mysql-5.7.24]# cd /usr/src/mysql-5.7.24
[root@localhost mysql-5.7.24]# rm -rf CMakeLists.txt  # 重新配置需要删除的文件
[root@localhost mysql-5.7.24]# cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql \
-DMYSQL_DATADIR=/usr/local/mysql/data/ \
-DMYSQL_UNIX_ADDR=/usr/local/mysql/mysql.sock \
-DWITH_INNOBASE_STORAGE_ENGINE=1 \
-DWITH_MYISAM_STORAGE_ENGINE=1 \
-DENABLED_LOCAL_INFILE=1 \
-DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci \
-DWITH_DEBUG=0 \
-DWITH_EMBEDDED_SERVER=1 \
-DDOWNLOAD_BOOST=1 -DENABLE_DOWNLOADS=1 -DWITH_BOOST=/usr/local/boost
[root@localhost mysql-5.7.24]# make
[root@localhost mysql-5.7.24]# make install
[root@localhost src]# cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql
[root@localhost init.d]# useradd -s /sbin/nologin -r mysql
[root@localhost init.d]# chown mysql.mysql /usr/local/mysql/ -R
[root@localhost init.d]# ln -sf /usr/local/mysql/bin/* /usr/bin/
[root@localhost init.d]# ln -sf /usr/local/mysql/lib/* /usr/lib/
[root@localhost init.d]# ln -sf /usr/local/mysql/libexec/* /usr/local/libexec
[root@localhost init.d]# ln -sf /usr/local/mysql/share/man/man1/* /usr/share/man/man1
[root@localhost init.d]# ln -sf /usr/local/mysql/share/man/man8/* /usr/share/man/man8

# 5.修改配置文件
[root@localhost etc]# vim /ect/my.cnf
[root@localhost etc]# cat /ect/my.cnf
[mysqld]
basedir=/usr/local/mysql
datadir=/usr/local/mysql/data
socket=/usr/local/mysql/mysql.sock
symbolic-links=0
[mysqld_safe]
log-error=/var/log/mysql/mysql.log
pid-file=/var/run/mysql/mysql.pid
!includedir /etc/my.cnf.d

# 6.初始化
[root@localhost etc]# chown -R mysql.mysql /var/log/mysql/
[root@localhost etc]# chown -R mysql.mysql /var/run/mysql/
[root@localhost etc]# /usr/local/mysql/bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/
root@localhost: UtgafXHO96/q

# 7.启动mysql
[root@localhost data]# /etc/init.d/mysql start
# 如果报错，检查权限|查看mysqld进程

# 7.修改密码
[root@localhost data]# mysql_secure_installation

# 8.登录mysql
[root@localhost data]# mysql -u root -p 123456789
```

**安装python3+虚拟环境+django**

```shell
# 1.下载安装包，解压，检测
[root@localhost src]# wget https://www.python.org/ftp/python/3.9.8/Python-3.9.8.tgz /usr/src/
[root@localhost src]# tar -zxvf Python-3.9.8.tgz
[root@localhost Python-3.9.8]# yum -y install gcc-* openssl-* libffidevel sqlite-devel
[root@localhost Python-3.9.8]# ./configure --enable-optimizations --with-openssl=/usr/bin/openssl

# 2.编译安装
[root@localhost Python-3.9.8]# make -j4
[root@localhost Python-3.9.8]# make install

# 3.升级pip3
[root@localhost Python-3.9.8]# vim Modules/Setup  # 去掉下面的注释
214 SSL=/usr/local/ssl
215 _ssl _ssl.c \
216     -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
217     -L$(SSL)/lib -lssl -lcrypto

[root@localhost Python-3.9.8]# make -j4
[root@localhost Python-3.9.8]# make install
[root@localhost Python-3.9.8]# pip3 install --upgrade pip

# 4.安装virtualenvwrapper
[root@localhost bin]# pip3 install virtualenv
[root@localhost bin]# pip3 install virtualenvwrapper

# 5.先创建一个目录放置虚拟环境
[root@localhost bin]# mkdir ~/.virtualenvs

# 6.查看virtualenvwrapper的安装路径
[root@localhost bin]# sudo find / -name virtualenvwrapper.sh
/opt/python369/bin/virtualenvwrapper.sh

# 5.配置环境变量
[root@localhost bin]# vim ~/.bashrc
	# 5.1 根据自己的环境修改路径追加到配置文件中
    export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3
	export WORKON_HOME=$HOME/.virtualenvs
	source /usr/local/bin/virtualenvwrapper.sh

# 6.配置完成之后需要执行如下命令，才可以让编辑后的配置文件生效 
[root@localhost bin]# source ~/.bashrc

# 7.mkvirtualenv -p python3 创建的虚拟环境名称
[root@localhost bin]# mkvirtualenv -p python3 nginx_python

# 8.安装django
(nginx_python) [root@localhost bin]# pip3 install django==2.1.8

# 9.创建一个项目
(nginx_python) [root@localhost bin]# django-admin.py startproject myweb

# 10.启动django，浏览器访问
(nginx_python) [root@localhost myweb]# python3 manage.py runserver 192.168.88.128:8000
```

**安装uwsgi**

```shell
# 1.下载uwsgi
(nginx_python) [root@localhost myweb]# pip3 install uwsgi

# 2. 编写uwsgi配置文件
(nginx_python) [root@localhost myweb]# mkdir /etc/uwsgi
(nginx_python) [root@localhost myweb]# /etc/uwsgi/uwsgi.ini
[uwsgi]
uid = root
gid = root
socket = 127.0.0.1:9090
master = true		# 启动主进程
vhost = true		# 多站模式
no-site = true		# 多站模式时不设置⼊⼝模块和⽂件
workers = 2			# ⼦进程数
reload-mercy = 10	# 平滑的重启
vacuum = true		# 退出、重启时清理⽂件
max-requests = 1000	# 开启10000个进程后, ⾃动respawn下
limit-as = 512		# 开启10000个进程后, ⾃动respawn下
buffer-size = 30000
pidfile = /var/run/uwsgi9090.pid	# pid⽂件，⽤于下⾯的脚本启动、停⽌该进程
daemonize = /var/log/uwsgi9090.log
pythonpath = /root/.virtualenvs/nginx_python/lib/python3.9/site-packages	# django路径

# 3.编写uwsgi启动脚本
(nginx_python) [root@localhost ~]# cat /etc/init.d/uwsgi
#!/bin/sh
 DESC="uwsgi daemon"
 NAME=uwsgi
 DAEMON=/root/.virtualenvs/nginx_python/bin/uwsgi
 CONFIGFILE=/etc/uwsgi/$NAME.ini
 PIDFILE=/var/run/${NAME}9090.pid
 SCRIPTNAME=/etc/init.d/$NAME
 FIFOFILE=/tmp/uwsgififo
 set -e
 [ -x "$DAEMON" ] || exit 0
 do_start() {
 if [ ! -f $PIDFILE ];then
 $DAEMON $CONFIGFILE || echo -n "uwsgi running"
 else
 echo "The PID is exist..."
 fi
 }
 do_stop() {
 if [ -f $PIDFILE ];then
 $DAEMON --stop $PIDFILE || echo -n "uwsgi not running"
 rm -f $PIDFILE
 echo "$DAEMON STOPED."
 else
 echo "The $PIDFILE doesn't found"
 fi
 }
 do_reload() {
 if [ -p $FIFOFILE ];then
 echo w > $FIFOFILE
 else
 $DAEMON --touch-workers-reload $PIDFILE || echo -n "uwsgi
can't reload"
 fi
 }
 do_status() {
 ps aux|grep $DAEMON
 }
 case "$1" in
 status)
 echo -en "Status $NAME: \n"
 do_status
 ;;
 start)
 echo -en "Starting $NAME: \n"
 do_start
 ;;
 stop)
 echo -en "Stopping $NAME: \n"
 do_stop
 ;;
 reload|graceful)
 echo -en "Reloading $NAME: \n"
 do_reload
 ;;
 *)
 echo "Usage: $SCRIPTNAME {start|stop|reload}" >&2
 exit 3
 ;;
 esac
 exit 0

# 4.修改nginx配置文件
(nginx_python) [root@localhost ~]# cat /usr/local/nginx/conf/nginx.conf
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    server {
        listen  80;
        server_name localhost;

        location / {
            include uwsgi_params;
            uwsgi_pass 127.0.0.1:9090;
            uwsgi_param UWSGI_SCRIPT myweb.wsgi;
            uwsgi_param UWSGI_CHDIR /usr/local/bin/myweb;
            index index.html index.htm;
            client_max_body_size 35m;
            uwsgi_cache_valid 1m;
            uwsgi_temp_file_write_size 64k;
            uwsgi_busy_buffers_size 64k;
            uwsgi_buffers 8 64k;
            uwsgi_buffer_size 64k;
            uwsgi_read_timeout 300;
            uwsgi_send_timeout 300;
            uwsgi_connect_timeout 300;
         }
    }
}

# 5.启动uwsgi
(nginx_python) [root@localhost uwsgi]# /etc/init.d/uwsgi status
(nginx_python) [root@localhost uwsgi]# /etc/init.d/uwsgi stop
(nginx_python) [root@localhost uwsgi]# /etc/init.d/uwsgi start

# 6.启动nginx，页面访问 192.168.88.128，前提关闭防火墙
(nginx_python) [root@localhost uwsgi]# systemctl disable firewalld
(nginx_python) [root@localhost uwsgi]# vim /etc/selinux/config
SELINUX=disabled
(nginx_python) [root@localhost uwsgi]# /usr/local/nginx/sbin/nginx
```

### 5.8 前后端项目分离部署

基于：**vue** + **nginx** + **uwsgi** + **django(drf)** + **mariadb** + **redis** + **virtualenv** + **supervisor** 部署前后端分离代码

思路解析：

> 1.vue打包之后，生成了dist静态文件夹 ，前端的静态文件都在这里了，静态文件都丢给nginx直接去返回即可
>
> 2.vue的dist静态文件，运行之后，能够立即看到路飞的首页内容了，此时还没有和后端的数据交互
>
> 3.当你在前端的页面向后台发送 ajax请求，提交post
>
> 4.vue向后端发送post请求，由后端drf去处理
>
> 注意：在线上部署的架构图流程中，django后台，是躲在了防火墙之后的，只能通过nginx反向代理去访问

前后端分离部署示意图：

![image-20211023183620148](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211023183620148.png)

#### 5.8.1 部署前端

先配置node的环境，用 npm run build  对前端的vue代码，进行打包。

```shell
# 1.获取前端vue的项目代码
(venv_luffy_pro) [root@localhost opt]# wget https://files.cnblogs.com/files/pyyu/07-luffy_project_01.zip

# 2.解压缩代码，进行编译打包vue的代码，注意要配置nodejs环境
(venv_luffy_pro) [root@localhost opt]# unzip 07-luffy_project_01.zip

# 3.配置nodejs环境，下载nodejs源码包
# 这里特殊的是：老师给的这个地址，是node的二进制源码包，是已经编译完成了的node解释器，直接解压缩，配置PATH即可，无需再编译了
(venv_luffy_pro) [root@localhost opt]# wget https://nodejs.org/download/release/v8.6.0/node-v8.6.0-linux-x64.tar.gz
(venv_luffy_pro) [root@localhost opt]# tar -zxvf  node-v8.6.0-linux-x64.tar.gz

# 4.添加环境变量
(venv_luffy_pro) [root@localhost bin]# vim /etc/profile
PATH="/opt/tng232/sbin:/opt/python369/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin:/opt/node-v8.6.0-linux-x64/bin"
(venv_luffy_pro) [root@localhost bin]# source /etc/profile

# 5.重新读取/etc/profile，加载node的环境，查看是否正常
(venv_luffy_pro) [root@localhost bin]# node -v
v8.6.0
(venv_luffy_pro) [root@localhost bin]# npm -v
5.3.0

# 6.进入vue的代码目录下，修改数据提交地址，这一步非常重要!
(venv_luffy_pro) [root@localhost restful]# pwd
/opt/07-luffy_project_01/src/restful
(venv_luffy_pro) [root@localhost restful]# vim api.js
:%s/127.0.0.1:8000/192.168.254.130:9000/g  # 使用vim替换命令  将127.0.0.1:8000替换成192.168.254.130:9000

# 7.明确vue的数据提交地址，正确修改之后，开始安装node的依赖，就好比 python项目要运行，需要安装pip3模块一样
#  --registry https://registry.npm.taobao.org 这是临时更换淘宝源的含义
#  结尾什么都不跟上，默认是安装当前路径的 package.json文件
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# pwd
/opt/07-luffy_project_01
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# ll
total 384
drwxr-xr-x. 2 root root    188 Oct 30  2018 build
drwxr-xr-x. 2 root root     59 Oct 30  2018 config
-rw-r--r--. 1 root root    325 Dec 14  2018 index.html
-rw-r--r--. 1 root root   1869 Nov  2  2018 package.json
-rw-r--r--. 1 root root 380551 Nov  6  2018 package-lock.json
-rw-r--r--. 1 root root    473 Oct 30  2018 README.md
drwxr-xr-x. 7 root root    127 Dec 18  2018 src
drwxr-xr-x. 4 root root     67 Nov  6  2018 static
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# npm --registry https://registry.npm.taobao.org install

# 8.安装完node的模块之后，开始编译vue
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# npm run build

# 9.此时应该会生成了dist文件夹，直接丢给nginx去返回页面即可，由于学习阶段，80端口已经给crm项目用了，因此路飞项目换成81端口
# 修改nginx.conf，添加一个server{}虚拟主机，返回路飞的页面
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# vim /opt/tng232/conf/nginx.conf
    # 添加第二个虚拟主机，给luffy使用
        server {
            listen 81;
            server_name _;
            location / {
                # 直接返回vue的打包内容即可
                root /opt/07-luffy_project_01/dist;
                index index.html;
                #这一条参数确保vue页面刷新时候，不会出现404页面
		　　　　  try_files $uri $uri/ /index.html;
             }
# 10.平滑重启nginx
(venv_luffy_pro) [root@localhost 07-luffy_project_01]# nginx -s reload

# 11.访问：192.168.254.13.：81
```

#### 5.8.2 部署后端

```shell
# 1.获取drf项目代码，本地上传，git克隆
(venv_luffy_pro) [root@localhost opt]# wget  https://files.cnblogs.com/files/pyyu/luffy_boy.zip

# 2.解压缩项目代码，之后，安装依赖，使用虚拟环境管理不同的项目
(venv_luffy_pro) [root@localhost opt]# unzip luffy_boy.zip
(venv_luffy_pro) [root@localhost luffy_boy]# cat requirements.txt
certifi==2018.11.29
chardet==3.0.4
crypto==1.4.1
Django==2.1.4
django-redis==4.10.0
django-rest-framework==0.1.0
djangorestframework==3.9.0
idna==2.8
Naked==0.1.31
pycrypto==2.6.1
pytz==2018.7
PyYAML==3.13
redis==3.0.1
requests==2.21.0
shellescape==3.4.1
urllib3==1.24.1
uWSGI==2.0.17.1

# 3.安装路飞项目的依赖
(venv_luffy_pro) [root@localhost luffy_boy]# pip3 install  -i https://pypi.douban.com/simple -r requirements.txt

# 4.测试路飞代码是否能够运行
python3 manage.py  runserver 0.0.0.0:8001

# 5.测试通过之后，在线上，是使用uwsgi结合uwsgi.ini配置文件，启动crm的，因此 ，启动方式如下
(venv_luffy_pro) [root@localhost luffy_boy]# pip3 install -i https://pypi.douban.com/simple uwsgi

# 6.配置uwsgi.ini，启动路飞后端，老师的uwsig.ini内容如下
(venv_luffy_pro) [root@localhost luffy_boy]# cp ../cs_book02/uwsgi.ini ./
(venv_luffy_pro) [root@localhost luffy_boy]# vim uwsgi.ini
================================================================================
[uwsgi]
# socket        =192.168.254.130:9001
# Django-related settings
# the base directory (full path)
# 填写crm项目的第一层绝对路径
chdir           = /opt/luffy_boy
# Django's wsgi file
# 填写crm项目第二层的相对路径，找到第二层目录下的wsgi.py
# 这里填写的不是路径，是以上一个参数为相对，找到第二层项目目录下的wsgi.py文件
module          = luffy_boy.wsgi
# the virtualenv (full path)
# 填写虚拟环境解释器的第一层工作目录
home            = /root/.virtualenvs/venv_luffy_pro/
# process-related settings
# master
# 主进程
master          = true
# maximum number of worker processes
# 代表定义uwsgi运行的多进程数量，官网给出的优化建议是 2*cpu核数+1 ，单核的cpu填写几?
# 如果是单进程，十万个请求，都丢给一个进程去处理
# 3个工作进程，十万个请求，就分给了3个进程去分摊处理
processes       = 3

# the socket (use the full path to be safe
# 这里的socket参数，是用于和nginx结合部署的unix-socket参数，这里临时先暂停使用
# 线上不会用http参数，因为对后端是不安全的，使用socket参数是安全的连接，用nginx反向代理去访问
# 后端程序是运行在防火墙内部，外网是无法直接访问的
# 临时使用http参数，便于我们用浏览器调试访问
socket          = 0.0.0.0:9005
#http        = 0.0.0.0:8000

# ... with appropriate permissions - may be needed
# chmod-socket    = 664
# clear environment on exit
vacuum            = true
================================================================================

# 7.此时给supervisor再添加一个任务，用于管理路飞
(venv_luffy_pro) [root@localhost luffy_boy]# vim /etc/supervisord.conf
[program:luffy_boy]
#command=/opt/venv_crm_uwsgi/bin/uwsgi --ini /opt/venv_crm_uwsgi/uwsgi.ini   ;supervisor其实就是在帮你执行命令而已！
command=/root/.virtualenvs/venv_luffy_pro/bin/uwsgi --ini /opt/luffy_boy/uwsgi.ini
autostart=true       ; 在supervisord启动的时候也自动启动
startsecs=10         ; 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒
autorestart=true     ; 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启
stopasgroup=true     ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程
killasgroup=true     ;默认为false，向进程组发送kill信号，包括子进程

# 8.先杀死所有的supervisor进程,此时重启supervisord进程，把新的任务，管理路飞也加进去
(venv_luffy_pro) [root@localhost luffy_boy]# pkill -9 supervisor  # 杀死supervisor进程，用于重启
(venv_luffy_pro) [root@localhost luffy_boy]# pkill -9 uwsgi		  # 杀死crm的进程，用于待会重启

# 9.此时检查2个任务的状态
(venv_luffy_pro) [root@localhost luffy_boy]# supervisord -c /etc/supervisord.conf
Unlinking stale socket /tmp/supervisor.sock
(venv_luffy_pro) [root@localhost luffy_boy]# supervisorctl -c /etc/supervisord.conf
supervisor> status
luffy_boy                        RUNNING   pid 31071, uptime 0:28:35
supervisor                       RUNNING   pid 31070, uptime 0:28:35

# 10.此时发现还差一个代理服务器的配置，修改nginx.conf如下
# 再添加一个虚拟主机，作用是给路飞后台的代理使用
    server {
        listen 9000;
        server_name _;
        # nginx的9000代理服务器，接收到任意请求之后，直接转发给后端的uwsgi
        location / {
               uwsgi_pass 0.0.0.0:9005;
               include uwsgi_params;
        }

    }

# 11.重启nginx
(venv_luffy_pro) [root@localhost luffy_boy]# nginx -s reload

# 12.登录路飞
http://192.168.254.130:81/
账号：alex
密码：alex3714

# 13.添加python和linux的课程，加入购物车，查看个人中心的购物车，发现还缺少redis数据库的运行
# 安装redis，启动redis
(venv_luffy_pro) [root@localhost views]# yum install redis -y
(venv_luffy_pro) [root@localhost views]# systemctl start redis

# 14.添加课程后查看redis
(venv_luffy_pro) [root@localhost views]# redis-cli
127.0.0.1:6379> keys *
1) "shopping_car_2_2"
2) ":1:504a36a447613fee29b2d93d5841dfb497d6d524"
3) "shopping_car_2_4"
4) ":1:da4439b1ec2d4b79935ce1af872db3b79cb6118c"
5) ":1:6b6fbde8ec35be266242cf69585b2667ce8c88ed"
127.0.0.1:6379>
```

## 第六章 redis持久化存储

`Redis` 是一种内存型数据库，一旦服务器进程退出，数据库的数据就会丢失，为了解决这个问题，`Redis` 提供了两种持久化的方案，将内存中的数据保存到磁盘中，避免数据的丢失。

### 6.1 RDB持久化

`redis` 提供了 `RDB持久化` 的功能，这个功能可以将`redis`在内存中的的状态保存到硬盘中，它可以 **手动执行**。

也可以再 `redis.conf` 中配置，**定期执行**。

RDB持久化产生的RDB文件是一个 **经过压缩** 的 **二进制文件** ，这个文件被保存在硬盘中，redis可以通过这个文件还原数据库当时的状态。

> ```
> RDB(持久化)
> 内存数据保存到磁盘
> 在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）
> 优点：速度快，适合做备份，主从复制就是基于RDB持久化功能实现
> rdb通过再redis中使用save命令触发 rdb
> ```

```shell
# 1.安装redis
yum install redis -y   # 也可以通过其他方式安装

# 2.修改redis.conf，更改默认端口，设置密码，开启安全模式等操作
# 用yum安装的redis，默认配置文件在/etc/redis.conf，我用编译安装，目录自定义,修改以下配置：
[root@localhost redis]# pwd
/usr/local/redis
[root@localhost redis]# vim redis.conf
    # 这里是绑定redis的启动地址，如果你支持远程连接，就改为0.0.0.0
    bind 0.0.0.0  
    # 更改端口
    port 6500
    # 设置redis的密码
    requirepass haohaio
    # 默认打开了安全模式
    protected-mode yes  
    # 打开一个redis后台运行的参数
    daemonize yes 

# 3.请使用如下的命令，指定配置文件启动
[root@localhost redis]# redis-server redis.conf
17214:C 30 Oct 2021 00:23:14.144 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
17214:C 30 Oct 2021 00:23:14.144 # Redis version=5.0.10, bits=64, commit=00000000, modified=0, pid=17214, just started
17214:C 30 Oct 2021 00:23:14.144 # Configuration loaded

# 4.检查redis的进程
[root@localhost redis]# ps -ef | grep redis

# 5.连接redis服务端，指定ip地址和端口，以及密码连接redis
-p 指定端口
-h 指定ip地址
auth 指令，用于密码验证

[root@localhost redis]# redis-cli -p 6500 -h 192.168.254.130
192.168.254.130:6500> auth haohaio
OK
192.168.254.130:6500> ping
PONG
192.168.254.130:6500>
```

向redis中写入一些数据，重启进程，查看数据是否会丢失

```shell
# 1.环境准备，准备一个redis.conf配置文件，不开启数据数据持久化,先杀死所有的redis进程，重新写一个配置文件
[root@localhost redis]# cat no_rdb_redis.conf
bind 0.0.0.0
daemonize yes

# 2.指定该文件启动
[root@localhost redis]# redis-server no_rdb_redis.conf
[root@localhost redis]# ps -ef | grep redis

# 3.登录数据库，写入数据，然后杀死redis进程，查看数据是否存在(不存在)
```

以上的操作，都是为了演示，redis如果不配置持久化，数据会丢失，请看如下的持久化机制

```shell
# 1.配置rdb机制的数据持久化，数据文件是一个看不懂的二进制文件，且配置触发的时间机制
[root@localhost redis]# vim yes_rdb_redis.conf
    daemonize yes				  # 后台运行
    port 6379					  # 端口 
    logfile /data/6379/redis.log  # 指定redis的运行日志，存储位置
    dir /data/6379				  # 指定redis的数据文件，存放路径 
    dbfilename  s25_dump.rdb	  # 指定数据持久化的文件名字 
    bind 127.0.0.1				  # 指定redis的运行ip地址
    # redis触发save指令，用于数据持久化的时间机制  
    # 900秒之内有1个修改的命令操作，如set .mset,del
    save 900 1		
    # 在300秒内有10个修改类的操作
    save 300 10
    # 60秒内有10000个修改类的操作
    save 60  10000

# 2.创建redis的数据文件夹
mkdir -p /usr/local/data/6379

# 3.指定配置文件启动redis
[root@localhost redis]# redis-server yes_rdb_redis.conf

# 4.如果没有触发redis的持久化时间机制，数据文件是不会生成的，数据重启进程也会丢
[root@localhost redis]# redis-cli
127.0.0.1:6379> ping
PONG
127.0.0.1:6379> set name joker
OK
127.0.0.1:6379> set age 18
OK
127.0.0.1:6379> set addr china
OK
127.0.0.1:6379> keys *
1) "addr"
2) "age"
3) "name"
[root@localhost redis]# pkill -9 redis
[root@localhost redis]# redis-server  yes_rdb_redis.conf
[root@localhost redis]# redis-cli
127.0.0.1:6379> keys *
(empty list or set)

# 5.可以通过编写脚本，让redis手动执行save命令，触发持久化，在redis命令行中，直接输入save即可触发持久化
[root@localhost redis]# redis-cli
127.0.0.1:6379> set name joker
OK
127.0.0.1:6379> set age 18
OK
127.0.0.1:6379> set addr china
OK
127.0.0.1:6379> save
[root@localhost redis]# pkill -9 redis
[root@localhost redis]# redis-server  yes_rdb_redis.conf
[root@localhost redis]# redis-cli
127.0.0.1:6379> keys *
1) "addr"
2) "name"
3) "age"

# 6.存在了rdb持久化的文件之后，重启redis进程，数据也不会丢了，redis在重启之后，会读取dump.rdb文件中的数据
[root@localhost 6379]# pwd
/usr/local/data/6379
[root@localhost 6379]# ll
total 12
-rw-r--r--. 1 root root 5786 Oct 30 01:15 redis.log
-rw-r--r--. 1 root root  129 Oct 30 01:14 yes_dump.rdb

# 注：rdb的弊端在于什么，如果没有触发持久化机制，就发生了机器宕机，数据就会丢失了，因此redis有一个更好的aof机制
```

### 6.2 AOF持久化

把修改类的redis命令操作，记录下来，追加写入到 **aof** 文件中，且是我们能够看得懂的日志文件

```shell
# 1.准备一个新的配置文件，里面定义了aof的功能性参数即可使用
[root@localhost redis]# vim aof_redis.conf
	# 写入以下内容
	daemonize yes
    port 6379
    logfile /usr/local/data/6379aof/redis.log
	dir /usr/local/data/6379aof/
    appendonly yes	# 开启aof功能
    appendfsync everysec  # 每秒钟持久化一次

# 2.创建aof的数据文件夹
[root@localhost data]# mkdir -p /usr/local/data/6379aof

# 3.启动aof的redis的数据库
[root@localhost redis]# redis-server aof_redis.conf

# 4.aof机制的数据库，在首次启动的时候，就会生成aof数据文件了，如下
[root@localhost redis]# cd /usr/local/data/6379aof/
[root@localhost 6379aof]# ll
total 4
-rw-r--r--. 1 root root    0 Oct 30 01:33 appendonly.aof
-rw-r--r--. 1 root root 1416 Oct 30 01:33 redis.log

# 5.登录redis，写入数据，写入的操作，会被记录到aof文件日志中
[root@localhost redis]# redis-cli
127.0.0.1:6379> set name ike
OK
127.0.0.1:6379> set age 19
OK
127.0.0.1:6379> keys *
1) "name"
2) "age"

# 6.杀死所有的redis进程，且重启
[root@localhost redis]# pkill -9 redis
[root@localhost redis]# ps -ef |grep reids

# 7.redis的aof持久化机制，是在重启的时候，redis重新执行一遍aof文件中的命令，实现数据复现
[root@localhost redis]# redis-server aof_redis.conf
127.0.0.1:6379> keys *
1) "name"
2) "age"

# 注：如果该aof日志文件被删除，数据也就无法恢复了
```

```powershell
由于单纯 RDB 的话，可能存在数据的丢失，而频繁的 AOF 又会影响了性能，在 Redis 4.0 之后，支持了混合持久化，也就是每次启动时候通过 RDB+增量的 AOF 文件来进行回复，由于增量的 AOF 仅记录了开始持久化到持久化结束期间发生的增量，这样日志不会太大，性能相对较高。
```

### 6.3 redis安全相关

由于发现众多同学，在使用云服务器时，安装的redis3.0+版本都关闭了protected-mode，因而都遭遇了挖矿病毒的攻击，使得服务器99%的占用率！！

因此我们在使用redis时候，最好更改默认端口，并且使用redis密码登录。

1. redis没有用户概念，redis只有密码
2. redis默认在工作在保护模式下。不允许远程任何用户登录的（protected-mode）

redis.conf设置

```shell
protected-mode yes   # 打开保护模式
port 6350  			 # 更改默认启动端口
requirepass xxxxxx   # 设置redis启动密码，xxxx是自定义的密码
```

启动redis服务端

```shell
redis-server /opt/redis-4.0.10/redis.conf   # 指定配置文件启动redis，且后台启动
```

使用密码登录redis，使用6350端口

```shell
# 方法一：推荐
redis-cli -p 6350
127.0.0.1:6350> auth haohaio
OK
127.0.0.1:6350>

# 方法二：不推荐，这样会把密码暴漏在命令行，埋下安全隐患
redis-cli -p 6350 -a xxxx
```

检查redis是否设置了密码

```shell
127.0.0.1:6350> config get requirepass
1) "requirepass"
2) "xxxxxx"
```

如果没有，也可以给redis设置密码（命令方式）

```shell
config set "xxxxxx"
```

### 6.4 redis数据同步复制

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180922122856003-755522114.png" alt="img" style="zoom:50%;" />

1. 从服务器向主服务器发送 SYNC 命令。

2. 接到 SYNC 命令的主服务器会调用BGSAVE 命令，创建一个 RDB 文件，并使用缓冲区记录接下来执行的所有写命令。

3. 当主服务器执行完 BGSAVE 命令时，它会向从服务器发送 RDB 文件，而从服务器则会接收并载入这个文件。

4. 主服务器将缓冲区储存的所有写命令发送给从服务器执行。

---

1. 在开启主从复制的时候，使用的是RDB方式，同步主从数据；同步开始之后，通过主库命令传播的方式，主动的复制方式实现；以后实现PSYNC的机制，实现断线重连

在一台机器上运行2个及以上的redis，是redis支持多实例的功能，基于端口号的不同，就能够运行多个相互独立的redis数据库。

```shell
# 1.准备好2个redis的配置文件，分别写入如下内容，第一个配置文件
[root@localhost bin]# pwd
/usr/local/bin
[root@localhost bin]# vim master-redis.conf
	# 写入以下内容：
    port 6379
    daemonize yes
    pidfile /usr/local/data/6379master/redis.pid
    loglevel notice
    logfile "/usr/local/data/6379master/redis.log"
    dbfilename dump.rdb
    dir /usr/local/data/6379master
    protected-mode no
    
# 2.准备第二个配置文件
[root@localhost bin]# vim slave-redis.conf
	# 写入以下内容
	port 6389
    daemonize yes
    pidfile /usr/local/data/6389slave/redis.pid
    loglevel notice
    logfile "/usr/local/data/6389slave/redis.log"
    dbfilename dump.rdb
    dir /usr/local/data/6389slave
    protected-mode no
    
# 3.分别生成2个redis的数据文件夹
[root@localhost bin]# mkdir -p /usr/local/data/{6379master,6389slave}

# 4.分别启动2个redis数据库，查看他们的身份复制关系
[root@localhost bin]# redis-server master-redis.conf
[root@localhost bin]# redis-server slave-redis.conf
[root@localhost bin]# ps -ef | grep redis
root      31261      1  0 05:00 ?        00:00:00 redis-server *:6379
root      31283      1  0 05:00 ?        00:00:00 redis-server *:6389

# 5.分别检查他们的进程
root@localhost bin]# redis-cli -p 6379 info replication
[root@localhost bin]# redis-cli -p 6389 info replication

# 6.通过一条命令，配置他们的复制关系，注意，这个命令只是临时配置redis的复制关系，想要永久修改，还得修改配置文件
[root@localhost bin]# redis-cli -p  6389  slaveof  127.0.0.1 6379
OK

# 注：此时6379已然是主库，6389已然是从库。可以向6379中写入数据，能够同步到6389中，6389是一个只读的数据库，无法写入数据

# 7.想永久配置复制关系，在配置文件中定义好复制关系，启动后，立即就会建立复制
slaveof  127.0.0.1  6379
```

**一主多从的形式，以及主从复制故障切换**

```shell
# 1.再创建一个配置文件，port是6399，且加入到一主一从的复制关系中去
[root@localhost bin]# vim slave2-redis.conf
	# 写入以下内容
    port 6399
    daemonize yes
    pidfile /usr/local/data/6399slave/redis.pid
    loglevel notice
    logfile "/usr/local/data/6399slave/redis.log"
    dbfilename dump.rdb
    dir /usr/local/data/6399slave
    protected-mode no
    slaveof 127.0.0.1 6379

# 2.创建数据文件夹
[root@localhost bin]# mkdir -p /usr/local/data/6399slave

# 3.启动6399的数据库，查看身份复制关系
[root@localhost bin]# redis-cli -p 6379 info replication

# 4.分别查看redis的复制关系
[root@localhost bin]# redis-cli -p 6389 info replication
# Replication
role:slave
master_host:127.0.0.1
master_port:6379
[root@localhost bin]# redis-cli -p 6399 info replication
# Replication
role:slave
master_host:127.0.0.1
master_port:6379

# 5. kill掉6379的进程
[root@localhost bin]# ps -ef | grep redis
kill -9 2260

# 此时留下了两个从库，没有了主库，只能访问数据，无法新增数据
# 6.手动去掉从库的身份，把自己改为主库
[root@localhost bin]# redis-cli -p 6399 slaveof no one
OK

# 7.查看6399的复制关系
[root@localhost bin]# redis-cli -p 6399 info replication
# Replication
role:master
connected_slaves:1

# 8.此时6399已然是主库了，修改6389的复制信息，改为6399即可
[root@localhost bin]# redis-cli -p 6389 slaveof 127.0.0.1 6399
OK Already connected to specified master

# 9.查看他们的复制关系
[root@localhost bin]# redis-cli -p 6389 info replication
# Replication
role:slave
master_host:127.0.0.1
master_port:6399
[root@localhost bin]# redis-cli -p 6399 info replication
# Replication
role:master
connected_slaves:1

# 10.此时可以向主库6399写入数据，6389查看数据即可
[root@localhost bin]# redis-cli -p 6399
127.0.0.1:6399> set name 1
OK
127.0.0.1:6399> get name
"1"
```

你会发现，如此的手动切换复制关系，其实是很难受的。手动操作总有不及时的时候，比如在夜深人静的时候，引入以下自动切换主库。

### 6.5 高可用哨兵

Redis-Sentinel是redis官方推荐的高可用性解决方案，当用redis作master-slave的高可用时，如果master本身宕机，redis本身或者客户端都没有实现主从切换的功能。而redis-sentinel就是一个独立运行的进程，用于监控多个master-slave集群，自动发现master宕机，进行自动切换slave > master。

**sentinel主要功能如下：**

- 不时的监控redis是否良好运行，如果节点不可达就会对节点进行下线标识
- 如果被标识的是主节点，sentinel就会和其他的sentinel节点“协商”，如果其他节点也认为主节点不可达，就会选举一个sentinel节点来完成自动故障转移
- 在master-slave进行切换后，master_redis.conf、slave_redis.conf和sentinel.conf的内容都会发生改变，即master_redis.conf中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换

**Sentinel的工作方式：**

```powershell
1.每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令。
2.如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线。
3.如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态。
4.当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线
5.在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有Master，Slave发送 INFO 命令
6.当Master被 Sentinel 标记为客观下线时，Sentinel 向下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次
7.若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的客观下线状态就会被移除。
8.若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的主观下线状态就会被移除。

主观下线和客观下线:

1.主观下线：Subjectively Down，简称 SDOWN，指的是当前 Sentinel 实例对某个redis服务器做出的下线判断。
2.客观下线：Objectively Down， 简称 ODOWN，指的是多个 Sentinel 实例在对Master Server做出 SDOWN 判断，并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后，得出的Master Server下线判断，然后开启failover.
3.SDOWN适合于Master和Slave，只要一个 Sentinel 发现Master进入了ODOWN， 这个 Sentinel 就可能会被其他 Sentinel 推选出， 并对下线的主服务器执行自动故障迁移操作。
4.ODOWN只适用于Master，对于Slave的 Redis 实例，Sentinel 在将它们判断为下线前不需要进行协商， 所以Slave的 Sentinel 永远不会达到ODOWN。
```

**redis主从复制背景：**

`Redis` 主从复制可将主节点数据同步给从节点，从节点此时有两个作用：

- 一旦主节点宕机，从节点作为主节点的备份可以随时顶上来。
- 扩展主节点的读能力，分担主节点读压力。

但是问题是：

- 一旦主节点宕机，从节点上位，那么需要人为修改所有应用方的主节点地址（改为新的master地址），还需要命令所有从节点复制新的主节点

那么这个问题，redis-sentinel就可以解决了

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180928114913282-867816204-163582532667312.png" alt="img" style="zoom: 33%;" />

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180928115021716-2071889796-163582532212110.png" alt="img" style="zoom:33%;" />

**Redis Sentinel架构：**

redis的一个进程，但是不存储数据，只是监控redis。

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20181124181822218-1125753774-16358253160028.png" alt="img" style="zoom: 50%;" />

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180928115711245-1503702880-16358253106106.png" alt="img" style="zoom: 67%;" />

<img src="https://img2018.cnblogs.com/blog/1132884/201809/1132884-20180928145626051-1360651849.png" alt="img" style="zoom: 67%;" />

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180928145734973-1288883859-163582538011315.png" alt="img" style="zoom:67%;" />

**补充知识：**

```shell
# 官网地址：http://redisdoc.com/
redis-cli info # 查看redis数据库信息
redis-cli info replication # 查看redis的复制授权信息
redis-cli info sentinel   # 查看redis的哨兵信息
```

**redis-sentinel 高可用故障实验：**

大概思路：

- 杀掉主节点的redis进程6379端口，观察从节点是否会进行新的master选举，进行切换
- 重新恢复旧的“master”节点，查看此时的redis身份

redis-sentinel

```shell
# 1.准备3个redis节点，1主2从的redis集群，redis支持多个实例，运行多个相互独立的redis进程
[root@localhost bin]# pwd
/usr/local/bin
[root@localhost bin]# vim master-redis-sentinel.conf
port 6379
daemonize yes
logfile "6379.log"
dbfilename "dump-6379.rdb"
dir "/usr/local/data/6379master-sentinel/"

[root@localhost bin]# vim slave-redis-sentinel.conf
port 6380
daemonize yes
logfile "6380.log"
dbfilename "dump-6380.rdb"
dir "/usr/local/data/6380slave1-sentinel"
replicaof 127.0.0.1 6379

[root@localhost bin]# vim slave2-redis-sentinel.conf
port 6381
daemonize yes
logfile "6381.log"
dbfilename "dump-6381.rdb"
dir "/usr/local/data/6381slave1-sentinel"
replicaof 127.0.0.1 6379


# 2.查看3个配置文件，准备分别启动该进程
[root@localhost bin]# ll *sentinel.conf
-rw-r--r--. 1 root root 113 Nov  2 20:20 master-redis-sentinel.conf
-rw-r--r--. 1 root root 114 Nov  3 05:26 slave2-redis-sentinel.conf
-rw-r--r--. 1 root root 138 Nov  3 05:26 slave-redis-sentinel.conf

# 3.分别启动3个进程后，检查进程情况
[root@localhost bin]# redis-server master-redis-sentinel.conf
[root@localhost bin]# redis-server slave-redis-sentinel.conf
[root@localhost bin]# redis-server slave2-redis-sentinel.conf
[root@localhost bin]# ps -ef | grep redis
root       1746      1  0 00:14 ?        00:00:00 redis-server *:6379
root       1751      1  0 00:14 ?        00:00:00 redis-server *:6380
root       1757      1  0 00:14 ?        00:00:00 redis-server *:6381
root       1767   1638  0 00:15 pts/0    00:00:00 grep --color=auto redis

# 4.确认3个库的主从关系
[root@localhost bin]# redis-cli -p 6379 info replication
# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=182,lag=1
slave1:ip=127.0.0.1,port=6381,state=online,offset=182,lag=1
```

1主2从的环境搭建好了，准备招来3个值班的也就是，redis的哨兵

```shell
# 1.分别准备3个哨兵的配置文件，修改如下，三个哨兵的配置文件，仅仅是端口号的不同
[root@localhost bin]# vim sentinel-26379.conf
port 26379  
dir "/usr/local/data/redis-sentinel"
logfile "26379.log"

# 当前Sentinel节点监控 192.168.119.10:6379 这个主节点
# 2代表判断主节点失败至少需要2个Sentinel节点同意
# mymaster是主节点的别名
sentinel monitor sentinel-redis-26379 127.0.0.1 6379 2

# 每个Sentinel节点都要定期PING命令来判断Redis数据节点和其余Sentinel节点是否可达，如果超过30000毫秒30s且没有回复，则判定不可达
sentinel down-after-milliseconds sentinel-redis-26379 30000

# 当Sentinel节点集合对主节点故障判定达成一致时，Sentinel领导者节点会做故障转移操作，选出新的主节点，原来的从节点会向新的主节点发起复制操作，限制每次向新的主节点发起复制操作的从节点个数为1
sentinel parallel-syncs sentinel-redis-26379 1

# 故障转移超时时间为180000毫秒
sentinel failover-timeout sentinel-redis-26379 180000
daemonize yes

[root@localhost bin]# vim sentinel-26389.conf
port 26389
dir "/usr/local/data/redis-sentinel"
logfile "26389.log"
sentinel monitor sentinel-redis-26379 127.0.0.1 6379 2
sentinel down-after-milliseconds sentinel-redis-26379 30000
sentinel parallel-syncs sentinel-redis-26379 1
sentinel failover-timeout sentinel-redis-26379 180000
daemonize yes

[root@localhost bin]# vim sentinel-26399.conf
port 26399
dir "/usr/local/data/redis-sentinel"
logfile "26399.log"
sentinel monitor sentinel-redis-26379 127.0.0.1 6379 2
sentinel down-after-milliseconds sentinel-redis-26379 30000
sentinel parallel-syncs sentinel-redis-26379 1
sentinel failover-timeout sentinel-redis-26379 180000
daemonize yes

# 2.分别启动3个哨兵进程，以及查看进程信息
[root@localhost bin]# redis-sentinel sentinel-26379.conf
[root@localhost bin]# redis-sentinel sentinel-26389.conf
[root@localhost bin]# redis-sentinel sentinel-26399.conf
[root@localhost bin]# ps -ef | grep redis
root       1746      1  0 00:14 ?        00:00:09 redis-server *:6379
root       1751      1  0 00:14 ?        00:00:08 redis-server *:6380
root       1757      1  0 00:14 ?        00:00:08 redis-server *:6381
root       1946      1  0 01:41 ?        00:00:00 redis-sentinel *:26379 [sentinel]
root       1952      1  0 01:42 ?        00:00:00 redis-sentinel *:26389 [sentinel]
root       1958      1  0 01:42 ?        00:00:00 redis-sentinel *:26399 [sentinel]
root       1963   1638  0 01:42 pts/0    00:00:00 grep --color=auto redis

# 3.检查redis哨兵的配置文件，以及哨兵的状态
[root@localhost bin]# redis-cli -p 26379 info sentinel
# Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=sentinel-redis-26379,status=ok,address=127.0.0.1:6379,slaves=2,sentinels=3
```

在哨兵搭建好了之后，模拟干掉主库，然后等待主从的一个自动化切换

```shell
# 1.检查6379的进程，杀死后，哨兵能够自动的，进行投票选举，剩下来的一个slave为新的master，然后重新分配主从关系
[root@localhost bin]# kill -9 1746
[root@localhost bin]# redis-cli -p 6380 info replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6381,state=online,offset=136117,lag=0

# 2.故障的修复，修复6379这个redis数据库，且检查它的一个复制关系,6379数据库会重新加入到主从复制，且变为一个新的从库

# 3.如果你想恢复他们的主从关系，全部kill掉，重新启动，默认就会以配置文件分配主从关系了
```

### 6.6 redis-cluster

**为什么要使用redis-cluster**

1. 并发问题：redis官方生成可达到每秒执行10万条命令，加入业务需要每秒执行100万条命令呢？
2. 数据量太大：一台服务器内存正常是16~256G，假如你的业务需要500G内存

> 新浪微博作为世界上最大的redis存储，就超过1TB的数据，去哪买这么大的内存条？各大公司有自己的解决方案，推出各自的集群功能，核心思想都是将数据分片（sharding）存储在多个redis实例中，每一片就是一个redis实例。
>
> 各大企业集群方案：
>
> 1. twemproxy由Twitter开源
> 2. Codis由豌豆荚开发，基于GO和C开发
> 3. redis-cluster官方3.0版本后的集群方案

**解决方案如下：**

1. 配置一个超级牛逼的计算机，超大内存，超强cpu，但是问题是再大的内存也扛不住日积月累的堆加

<img src="https://img2018.cnblogs.com/blog/1132884/201810/1132884-20181024160915096-209591231.png" alt="img" style="zoom: 80%;" />

2. 正确的应该是考虑分布式，加机器。把数据分到不同的位置，分摊集中式的压力，**一堆机器做一件事**

![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20181024161047230-851823170.png)

**客户端分片**

> redis3.0集群采用P2P模式，完全去中心化，将redis所有的key分成了16384个槽位，每个redis实例负责一部分slot，集群中的所有信息通过节点数据交换而更新。
>
> redis实例集群主要思想是将redis数据的key进行散列，通过hash函数特定的key会映射到指定的redis节点上。

**数据分布原理图**

 ![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20181125134928388-1528161304.png) 

**数据分布理论**

分布式数据库首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。

常见的分区规则有哈希分区和顺序分区。`Redis Cluster`采用哈希分区规则。

- 节点取余分区
- 一致性哈希分区
- 虚拟槽区分区（**redis-cluster采用的方式**）

**顺序分区原理图**

![1639294725729](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294725729.png)

**哈希分区原理图**

![1639294763567](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294763567.png)

例如按照节点取余的方式，分三个节点

1~100的数据对3取余，可以分为三类

- 余数为1
- 余数为0
- 余数为2

同样的道理，分4个节点，就是hash(key)%4来计算。节点取余的优点是简单，客户端分片直接是哈希+取余

**一致性哈希**

客户端进行分片，哈希+顺时针取余

**虚拟槽分区**

`Redis Cluster` 采用虚拟槽分区

```shell
# 1.虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有的数据映射到一个固定范围内的整数集合，整数定义为槽（slot）。
# 2.Redis Cluster槽的范围是0 ～ 16383。
# 3.槽是集群内数据管理和迁移的基本单位。采用大范围的槽的主要目的是为了方便数据的拆分和集群的扩展，
# 4.每个节点负责一定数量的槽。
```

![1639294819755](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639294819755.png)

#### 6.6.1 搭建redis cluster

搭建集群分为几步骤：

- 准备节点（几匹马）
- 节点通信（几匹马分配主从）
- 分配槽位给节点（每匹马分配slot）

redis-cluster集群架构

```shell
# 1.多个服务端，负责读写，彼此通信，redis指定了16384个槽。
# 2.多匹马儿，负责运输数据，马儿分配16384个槽位，管理数据。
# 3.ruby的脚本自动就把分配槽位这事做了
```

 ![img](J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20181024170727913-1418417703.png) 

**安装方式**

官方提供通过ruby语言的脚本一键安装。

**环境搭建**

```shell
# 1.准备好6匹马儿，也就是6个redis节点，也就是6个配置文件，redis集群节点最少是使用6个，这6个配置文件，仅仅是端口号的不同而已
[root@localhost redis-cluster]# cp redis-cluster-7000.conf redis-cluster-7001.conf
[root@localhost redis-cluster]# cp redis-cluster-7000.conf redis-cluster-7002.conf
[root@localhost redis-cluster]# cp redis-cluster-7000.conf redis-cluster-7004.conf
[root@localhost redis-cluster]# cp redis-cluster-7000.conf redis-cluster-7003.conf
[root@localhost redis-cluster]# cp redis-cluster-7000.conf redis-cluster-7005.conf
[root@localhost redis-cluster]# vim redis-cluster-7001.conf
[root@localhost redis-cluster]# vim redis-cluster-7002.conf
[root@localhost redis-cluster]# vim redis-cluster-7003.conf
[root@localhost redis-cluster]# vim redis-cluster-7004.conf
[root@localhost redis-cluster]# vim redis-cluster-7005.conf
port 7000
daemonize yes
dir "/opt/redis/data"
logfile "7000.log"
dbfilename "dump-7000.rdb"
cluster-enabled yes  # 开启集群模式
cluster-config-file nodes-7000.conf  # 集群内部的配置文件
cluster-require-full-coverage no  # redis cluster需要16384个slot都正常的时候才能对外提供服务，换句话说，只要任何一个slot异常那么整个cluster不对外提供服务。 因此生产环境一般为no

# 2.生成数据文件夹
[root@localhost redis-cluster]# mkdir -p /opt/redis/data

# 3.分别启动6个redis节点，且检查进程
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7000.conf
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7001.conf
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7002.conf
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7003.conf
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7004.conf
[root@localhost bin]# redis-server redis-cluster/redis-cluster-7005.conf

# 4.此时你尝试着写入数据，看一看是否能写进去，不能写入数据
[root@localhost bin]# redis-cli -p 7000
127.0.0.1:7000> set name joker
(error) CLUSTERDOWN Hash slot not served

# 5.查看redis服务器端口、进程
[root@localhost bin]# netstat -tunlp | grep redis
[root@localhost bin]# ps -ef | grep redis

# 注：我们仅仅是启动了6个redis节点，准备好了6匹马儿，马儿身上的框还没分配
[root@localhost bin]# redis-cli -p 7000
127.0.0.1:7000> set name joker
(error) CLUSTERDOWN Hash slot not served
```

此时准备好ruby的环境，用于一键创建redis集群，给马儿分配框，给redis节点分配slot槽位，用于写入数据

```shell
# 1.直接yum安装ruby解释器ruby和python一样是一个解释性编程语言，日本大神开发的
ruby=====python    gem====pip3   gem是ruby的包管理工具
[root@localhost bin]# yum install ruby -y

# 2.检查ruby和gem的环境
[root@localhost bin]# ruby -v
ruby 2.0.0p648 (2015-12-16) [x86_64-linux]

# 3.下载ruby操作redis的模块，用于创建集群
[root@localhost bin]# wget http://rubygems.org/downloads/redis-3.3.0.gem

# 4.用gem安装此模块,ruby就可以操作redis数据库了
[root@localhost bin]# gem install -l redis-3.3.0.gem  # 就如同python的 pip3 install xxxx，不难理解

# 5.搜索ruby创建redis集群的脚本
[root@localhost bin]# find / -name "redis-trib.rb"  # 默认会在redis数据库的编译安装路径下
/opt/redis-5.0.10/src/redis-trib.rb

# 6.查看gem有哪些包
[root@localhost bin]# gem list -- check redis gem

# 7.安装redis-trib.rb命令
[root@localhost bin]# pwd
/usr/local/bin
[root@localhost bin]# cp /opt/redis-5.0.10/src/redis-trib.rb ./

# 8.一键开启redis集群，且自动分配槽位，可以写入数据了
# 每个主节点，有一个从节点，代表--replicas 1
# redis5.0之前使用这个命令
[root@localhost bin]# redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005

# redis5.0之后使用这个命令
[root@localhost bin]# redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1
# 注：这其实也不是错，不要慌，只是因为从redis5.0开始，建议使用redis-cli作为创建集群的命令，不推荐再使用redis-trib.rb来创建集群了，毕竟使用redis-trib.rb还要安装Ruby程序，比redis-cli麻烦的多。

# 9.查看集群状态
redis-cli -p 7000 cluster info  
# 等同于查看nodes-7000.conf文件节点信息
redis-cli -p 7000 cluster nodes  
# 集群主节点状态
redis-cli -p 7000 cluster nodes | grep master
# 集群从节点状态
redis-cli -p 7000 cluster nodes | grep slave

[root@localhost bin]# redis-cli -p 7000 cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:23544
cluster_stats_messages_pong_sent:22375
cluster_stats_messages_sent:45919
cluster_stats_messages_ping_received:22370
cluster_stats_messages_pong_received:23544
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:4591

# 9.进入集群写入数据，查看数据重定向
[root@localhost bin]# redis-cli -c -p 7000
127.0.0.1:7000> set name joker
-> Redirected to slot [5798] located at 127.0.0.1:7001
OK
127.0.0.1:7001> keys *
1) "name"
127.0.0.1:7001> get name
"joker"
127.0.0.1:7001> exit
[root@localhost bin]# redis-cli -c -p 7000
127.0.0.1:7000> keys *
(empty list or set)
127.0.0.1:7000> get name
-> Redirected to slot [5798] located at 127.0.0.1:7001
"joker"
127.0.0.1:7001> keys *
1) "name"
```

**工作原理：redis客户端任意访问一个redis实例，如果数据不在该实例中，通过重定向引导客户端访问所需要的redis实例。**

### 6.7redis面试题

```shell
# 1.redis和memcached比较？

# 2.redis中数据库默认是多少个db 及作用？

# 3.python操作redis的模块？

# 4.如果redis中的某个列表中的数据量非常大，如果实现循环显示每一个值？

# 5.redis如何实现主从复制？以及数据同步机制？

# 6.redis中的sentinel的作用？

# 7.如何实现redis集群？

# 8.redis中默认有多少个哈希槽？

# 9.简述redis的有哪几种持久化策略及比较？

# 10.列举redis支持的过期策略。

# 11.MySQL 里有 2000w 数据，redis 中只存 20w 的数据，如何保证 redis 中都是热点数据？ 

# 12.写代码，基于redis的列表实现 先进先出、后进先出队列、优先级队列。

# 13.如何基于redis实现消息队列？

# 14.如何基于redis实现发布和订阅？以及发布订阅和消息队列的区别？

# 15.什么是codis及作用？

# 16.什么是twemproxy及作用？

# 17.写代码实现redis事务操作。

# 18.redis中的watch的命令的作用？

# 19.基于redis如何实现商城商品数量计数器？

# 20.简述redis分布式锁和redlock的实现机制。

# 21.什么是一致性哈希？Python中是否有相应模块？

# 22.如何高效的找到redis中所有以oldboy开头的key？
```

### 6.8 配置文件详解

```shell
# Redis 配置文件

# 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)
#
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
#
# 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB

# daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yes
daemonize yes

# 当redis在后台运行的时候，Redis默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。
# 当运行多个redis服务时，需要指定不同的pid文件和端口
pidfile /var/run/redis.pid

# 指定redis运行的端口，默认是6379
port 6379

# 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，
# 在生产环境中最好设置该项
# bind 127.0.0.1

# Specify the path for the unix socket that will be used to listen for
# incoming connections. There is no default, so Redis will not listen
# on a unix socket when not specified.
#
# unixsocket /tmp/redis.sock
# unixsocketperm 755

# 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接
# 0是关闭此设置
timeout 0

# 指定日志记录级别
# Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose
# debug    记录很多信息，用于开发和测试
# varbose    有用的信息，不像debug会记录那么多
# notice    普通的verbose，常用于生产环境
# warning    只有非常重要或者严重的信息会记录到日志
loglevel debug

# 配置log文件地址
# 默认值为stdout，标准输出，若后台模式会输出到/dev/null
#logfile stdout
logfile /var/log/redis/redis.log

# To enable logging to the system logger, just set 'syslog-enabled' to yes,
# and optionally update the other syslog parameters to suit your needs.
# syslog-enabled no

# Specify the syslog identity.
# syslog-ident redis

# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.
# syslog-facility local0

# 可用数据库数
# 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间
databases 16

################################ 快照 #################################
#
# 保存数据到磁盘，格式如下:
#
# save <seconds> <changes>
#
# 指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。
# 相当于条件触发抓取快照，这个可以多个条件配合
# 
# 比如默认配置文件中的设置，就设置了三个条件
#
# save 900 1 900秒内至少有1个key被改变
# save 300 10 300秒内至少有300个key被改变
# save 60 10000 60秒内至少有10000个key被改变

save 900 1
save 300 10
save 60 10000

# 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yes
rdbcompression yes

# 本地持久化数据库文件名，默认值为dump.rdb
dbfilename dump.rdb

# 工作目录
#
# 数据库镜像备份的文件放置的路径。
# 这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，
# 再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。
# 
# AOF文件也会存放在这个目录下面
# 
# 注意这里必须制定一个目录而不是文件
dir ./

################################# 复制 #################################

# 主从复制. 设置该数据库为其他数据库的从数据库. 
# 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步
#
# slaveof <masterip> <masterport>

# 当master服务设置了密码保护时(用requirepass制定的密码)
# slav服务连接master的密码
# 
# masterauth <master-password>


# 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：
#
# 1) 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求
# 
# 2) 如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个
# 错误"SYNC with master in progress"
#
slave-serve-stale-data yes

# 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒
#
# repl-ping-slave-period 10

# repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒
# 一定要确保repl-timeout大于repl-ping-slave-period
# repl-timeout 60

################################## 安全 ###################################

# 设置客户端连接后进行任何其他指定前需要使用的密码。
# 警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解
#
# requirepass foobared

# 命令重命名.
#
# 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。
#
# 举例:
#
# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
#
# 如果想删除一个命令，直接把它重命名为一个空字符""即可，如下：
#
# rename-command CONFIG ""

################################### 约束 ####################################

# 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，
# 如果设置 maxclients 0，表示不作限制。
# 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息
#
# maxclients 128

# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key
# Redis同时也会移除空的list对象
#
# 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作
# 
# 注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区
#
# maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。
# 当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销
# maxmemory <bytes>

# 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择
# 
# volatile-lru -> 利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )
# allkeys-lru -> 利用LRU算法移除任何key
# volatile-random -> 移除设置过过期时间的随机key
# allkeys->random -> remove a random key, any key 
# volatile-ttl -> 移除即将过期的key(minor TTL)
# noeviction -> 不移除任何可以，只是返回一个写错误
# 
# 注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误
#
# 写命令包括: set setnx setex append
# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd
# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby
# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby
# getset mset msetnx exec sort
#
# 默认是:
#
# maxmemory-policy volatile-lru

# LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。
# Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置
#
# maxmemory-samples 3

############################## AOF ###############################


# 默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。
# 所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。
# 开启append only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。
# 但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。
# 你可以同时开启asynchronous dumps 和 AOF

appendonly no

# AOF文件名称 (默认: "appendonly.aof")
# appendfilename appendonly.aof

# Redis支持三种同步AOF文件的策略:
#
# no: 不进行同步，系统去操作 . Faster.
# always: always表示每次有写操作都进行同步. Slow, Safest.
# everysec: 表示对写操作进行累积，每秒同步一次. Compromise.
#
# 默认是"everysec"，按照速度和安全折中这是最好的。
# 如果想让Redis能更高效的运行，你也可以设置为"no"，让操作系统决定什么时候去执行
# 或者相反想让数据更安全你也可以设置为"always"
#
# 如果不确定就用 "everysec".

# appendfsync always
appendfsync everysec
# appendfsync no

# AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作
# 在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理
#
# 为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite
#
# This means that while another child is saving the durability of Redis is
# the same as "appendfsync none", that in pratical terms means that it is
# possible to lost up to 30 seconds of log in the worst scenario (with the
# default Linux settings).
# 
# If you have latency problems turn this to "yes". Otherwise leave it as
# "no" that is the safest pick from the point of view of durability.
no-appendfsync-on-rewrite no

# Automatic rewrite of the append only file.
# AOF 自动重写
# 当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写 
# 
# 它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)
#
# 基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动
# 同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况
# 设置 percentage 为0就关闭这个特性

auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

################################## SLOW LOG ###################################

# Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间
# 
# 可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，
# 另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除

# 下面的时间以微妙微单位，因此1000000代表一分钟。
# 注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录
slowlog-log-slower-than 10000

# 对日志长度没有限制，只是要注意它会消耗内存
# 可以通过 SLOWLOG RESET 回收被慢日志消耗的内存
slowlog-max-len 1024

################################ VM ###############################

### WARNING! Virtual Memory is deprecated in Redis 2.4
### The use of Virtual Memory is strongly discouraged.

# Virtual Memory allows Redis to work with datasets bigger than the actual
# amount of RAM needed to hold the whole dataset in memory.
# In order to do so very used keys are taken in memory while the other keys
# are swapped into a swap file, similarly to what operating systems do
# with memory pages.
#
# To enable VM just set 'vm-enabled' to yes, and set the following three
# VM parameters accordingly to your needs.

vm-enabled no
# vm-enabled yes

# This is the path of the Redis swap file. As you can guess, swap files
# can't be shared by different Redis instances, so make sure to use a swap
# file for every redis process you are running. Redis will complain if the
# swap file is already in use.
#
# The best kind of storage for the Redis swap file (that's accessed at random) 
# is a Solid State Disk (SSD).
#
# *** WARNING *** if you are using a shared hosting the default of putting
# the swap file under /tmp is not secure. Create a dir with access granted
# only to Redis user and configure Redis to create the swap file there.
vm-swap-file /tmp/redis.swap

# vm-max-memory configures the VM to use at max the specified amount of
# RAM. Everything that deos not fit will be swapped on disk *if* possible, that
# is, if there is still enough contiguous space in the swap file.
#
# With vm-max-memory 0 the system will swap everything it can. Not a good
# default, just specify the max amount of RAM you can in bytes, but it's
# better to leave some margin. For instance specify an amount of RAM
# that's more or less between 60 and 80% of your free RAM.
vm-max-memory 0

# Redis swap files is split into pages. An object can be saved using multiple
# contiguous pages, but pages can't be shared between different objects.
# So if your page is too big, small objects swapped out on disk will waste
# a lot of space. If you page is too small, there is less space in the swap
# file (assuming you configured the same number of total swap file pages).
#
# If you use a lot of small objects, use a page size of 64 or 32 bytes.
# If you use a lot of big objects, use a bigger page size.
# If unsure, use the default :)
vm-page-size 32

# Number of total memory pages in the swap file.
# Given that the page table (a bitmap of free/used pages) is taken in memory,
# every 8 pages on disk will consume 1 byte of RAM.
#
# The total swap size is vm-page-size * vm-pages
#
# With the default of 32-bytes memory pages and 134217728 pages Redis will
# use a 4 GB swap file, that will use 16 MB of RAM for the page table.
#
# It's better to use the smallest acceptable value for your application,
# but the default is large in order to work in most conditions.
vm-pages 134217728

# Max number of VM I/O threads running at the same time.
# This threads are used to read/write data from/to swap file, since they
# also encode and decode objects from disk to memory or the reverse, a bigger
# number of threads can help with big objects even if they can't help with
# I/O itself as the physical device may not be able to couple with many
# reads/writes operations at the same time.
#
# The special value of 0 turn off threaded I/O and enables the blocking
# Virtual Memory implementation.
vm-max-threads 4

############################### ADVANCED CONFIG ###############################

# 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，
# hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值
# Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，
# 这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,
# 当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。
hash-max-zipmap-entries 512
hash-max-zipmap-value 64

# list数据类型多少节点以下会采用去指针的紧凑存储格式。
# list数据类型节点值大小小于多少字节会采用紧凑存储格式。
list-max-ziplist-entries 512
list-max-ziplist-value 64

# set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。
set-max-intset-entries 512

# zsort数据类型多少节点以下会采用去指针的紧凑存储格式。
# zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用
# 
# 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。
#
# 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存
activerehashing yes

################################## INCLUDES ###################################

# 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件

# include /path/to/local.conf
# include /path/to/other.conf
redis.conf详解--补充--
```

---

## 第七章 Docker

**什么是docker**

>Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。
>Docker 使用 Google 公司推出的 Go 语言 进行开发实现。
>docker是linux容器的一种封装，提供简单易用的容器使用接口。它是最流行的Linux容器解决方案。
>docker的接口相当简单，用户可以方便的创建、销毁容器。
>docker将应用程序与程序的依赖，打包在一个文件里面。运行这个文件就会生成一个虚拟容器。
>程序运行在虚拟容器里，如同在真实物理机上运行一样，有了docker，就不用担心环境问题了。

**应用场景**

> 1.web应用的自动化打包和发布
>
> 2.自动化测试和持续集成、发布
>
> 3.在服务型环境中部署和调整数据库或其他应用

**为什么要使用docker**

我们先看看之前，服务器是怎么部署应用的

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1132884-20180922153951410-114250843.png" alt="img" style="zoom: 25%;" />

由于物理机的诸多问题，后来出现了虚拟机

<img src="https://img2018.cnblogs.com/blog/1132884/201809/1132884-20180922154359164-1082041085.png" alt="img" style="zoom: 25%;" />

```powershell
但是虚拟化也是有局限性的，每一个虚拟机都是一个完整的操作系统，要分配系统资源，虚拟机多道一定程度时，操作系统本身资源也就消耗殆尽，或者说必须扩容
```

**docker与虚拟机的区别**

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211105112934600.png" alt="image-20211105112934600" style="zoom:50%;" />

| 特性       | 容器（docker）     | 虚拟机（VM） |
| ---------- | ------------------ | ------------ |
| 启动       | 秒级               | 分钟级       |
| 硬盘使用   | 一般为 MB          | 一般为 GB    |
| 性能       | 接近原生           | 弱           |
| 系统支持量 | 单机支持上千个容器 | 一般几十个   |

**环境配置的难题**

让开发人员最头疼的麻烦事之一就是环境配置了，每台计算机的环境都不相同，应该如何确保自己的程序换一台机器能运行起来呢？

用户必须确保的是：

- 操作系统相同
- 各种平台库和组件的安装
- 例如python依赖包、环境变量等

如何一些低版本的依赖模块和当前环境不兼容，那就头疼了！

如果环境配置这么痛苦的话，换一台机器，就得重新配置一下，那么在安装软件的时候，带着原始环境一模一样的复制过来。

**解决方案一：虚拟机**

虚拟机也可以制作模板，基于模板创建虚拟机，保证环境问题一致。

虚拟机（virtual machine）就是带环境安装的一种解决方案。它可以在一种操作系统里面运行另一种操作系统，比如在 Windows 系统里面运行 Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。

虽然用户可以通过虚拟机还原软件的原始环境。但是，这个方案有几个缺点。

（1）资源占用多

虚拟机会独占一部分内存和硬盘空间。它运行的时候，其他程序就不能使用这些资源了。哪怕虚拟机里面的应用程序，真正使用的内存只有 1MB，虚拟机依然需要几百 MB 的内存才能运行。

（2）冗余步骤多

虚拟机是完整的操作系统，一些系统级别的操作步骤，往往无法跳过，比如用户登录。

（3）启动慢

启动操作系统需要多久，启动虚拟机就需要多久。可能要等几分钟，应用程序才能真正运行。

<img src="https://img2018.cnblogs.com/blog/1132884/201809/1132884-20180922155740630-1447130345.png" alt="img" style="zoom: 25%;" />

**解决方案二：Linux容器**

> 自从用上docker容器后，可以实现开发、测试和生产环境的统一化和标准化。
>
> 镜像作为标准的交付件，可在开发、测试和生产环境上以容器来运行，最终实现三套环境上的应用以及运行所依赖内容的完全一致。

由于虚拟机的诸多问题，Linux发展出了另一种虚拟化技术：Linux容器（Linux Containers，缩写LXC）

**Linux容器不是模拟一个完整的操作系统，而是对进程进行隔离。在正常进程的外面套了一个保护层，对于容器里面进程来说，它接触的资源都是虚拟的，从而实现和底层系统的隔离。**

（1）启动快

容器里面的应用，直接就是底层系统的一个进程，而不是虚拟机内部的进程。所以，启动容器相当于启动本机的一个进程，而不是启动一个操作系统，速度就快很多。

（2）资源占用少

容器只占用需要的资源，不占用那些没有用到的资源；虚拟机由于是完整的操作系统，不可避免要占用所有资源。另外，多个容器可以共享资源，虚拟机都是独享资源。

（3）体积小

容器只要包含用到的组件即可，而虚拟机是整个操作系统的打包，所以容器文件比虚拟机文件要小很多。

总之，容器有点像轻量级的虚拟机，能够提供虚拟化的环境，但是成本开销小得多。

<img src="https://img2018.cnblogs.com/blog/1132884/201809/1132884-20180922113245731-202924980.png" alt="img" style="zoom: 50%;" />

**docker容器的优势**

```shell
# 1.更高效的利用系统资源
由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker对系统 资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。

# 2.更快速的启动时间
传统的虚拟机技术启动应用服务往往需要数分钟，而Docker容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。

# 3.一致的运行环境
开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些bug并未在开发过程中被发现。而Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题。
```

```shell
# 1.持续交付和部署
对开发和运维(DevOps)人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。
使用Docker可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过Dockerfile来进行镜像构建，并结合持续集成(Continuous Integration)系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合持续部署(Continuous Delivery/Deployment)系统进行自动部署。而且使用Dockerfile使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。
```

```shell
# 1.更轻松的迁移
由于Docker确保了执行环境的一致性，使得应用的迁移更加容易。Docker可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。
```

<img src="https://img2018.cnblogs.com/blog/1132884/201809/1132884-20180922162938061-1646086257.png" alt="img" style="zoom:50%;" />

### 7.1 docker三大概念

容器三大基本概念

- 镜像：image
- 容器：container
- 仓库：repository

docker整个生命周期就是这三个概念。

**docker镜像**

> ```shell
> # Docker镜像就是一个只读的模板。
> 例如：一个镜像可以包含一个完整的CentOS操作系统环境，里面仅安装了Apache或用户需要的其他应用程序。
> 
> # 镜像可以用来创建Docker容器。
> Docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。
> ```

**image的分层存储**

> ```shell
> # 因为镜像包含完整的root文件系统，体积是非常庞大的，因此docker在设计时按照Union FS的技术，将其设计为分层存储的架构。
> 
> # 镜像不是ISO那种完整的打包文件，镜像只是一个虚拟的概念，他不是一个完整的文件，而是由一组文件组成，或者多组文件系统联合组成。
> ```

**docker容器(container)**

> ```shell
> # image和container的关系，就像面向对象程序设计中的类和实例一样，镜像是静态的定义（class），容器是镜像运行时的实体（object）。
> # 容器可以被创建、启动、停止、删除、暂停
> # Docker利用容器来运行应用。
> 
> 容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的，保证安全的平台。
> 
> 可以把容器看做是一个简易版的Linux环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。
> 
> # 注意：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。
> ```

**docker仓库(repository)**

> ```shell
> # 仓库是集中存放镜像文件的场所。有时候把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签(tag)。
> 
> # 仓库分为公开仓库(Public)和私有仓库(Private)两种形式。
> 
> 最大的公开仓库是Docker Hub，存放了数量庞大的镜像供用户下载。国内的公开仓库包括Docker Pool等，可以提供大陆用户更稳定快速的访问。
> 
> 当用户创建了自己的镜像之后就可以使用push命令将它上传到公有或者私有仓库，这样下载在另外一台机器上使用这个镜像时候，只需要从仓库上pull下来就可以了。
> 
> # 注意：Docker仓库的概念跟Git类似，注册服务器可以理解为GitHub这样的托管服务。
> ```

---

**docker Registry**

Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务 供用户管理私有镜像。
最常使用的 Registry 公开服务是官方的 Docker Hub，这也是默认的 Registry，并拥有大量的高质量的官方镜像。除此以外，还有 CoreOS 的 Quay.io，CoreOS 相关的镜像存储在这里；Google 的 Google Container Registry，Kubernetes 的镜像使用的就是这个服务。
由于某些原因，在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对 Docker Hub 的镜像服务(Registry Mirror)，这些镜像服务被称为加速器。常见的有阿里云加速器、DaoCloud 加速器、灵雀云加速器等。使用加速器会直接从国内的地址下载 Docker Hub 的镜像，比直接从官方网站下载速度会提高很多。国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如，时速云镜像仓 库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。

### 7.2 搭建docker

**docker版本**

```powershell
Docker 是一个开源的商业产品，有两个版本：社区版（Community Edition，缩写为 CE）和企业版（Enterprise Edition，缩写为 EE）。
企业版包含了一些收费服务，个人开发者一般用不到。本文的介绍都针对社区版。
```

**使用docker镜像**

- 从仓库获取
- 管理本地主机镜像

```shell
# docker是把应用程序和其依赖打包在image文件里面，只有通过这个镜像文件才能生成docker容器。
# 一个image文件可以生成多个容器实例。image文件是通用，可以共享的。
```

官网安装教程如下链接：

https://docs.docker.com/install/linux/docker-ce/centos/#upgrade-docker-after-using-the-convenience-script

```shell
# 使用阿里云的yum源，可以直接安装docker软件，阿里云的docker软件版本可能较低，如果要下载新的，去docker官网找

# docker最低支持centos7且在64位平台上，内核版本在3.10以上
[root@localhost bin]# uname -r
3.10.0-1160.el7.x86_64

# docker镜像加速器
curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://95822026.m.daocloud.io

# 1.安装docker社区版
[root@localhost bin]# yum install docker -y 

# 2.启动docker
[root@localhost bin]# systemctl start docker

# 3.配置docker的镜像加速器，加速系统镜像的下载，默认是去国外下载，比较慢
	# 能够加速下载你所需要的各种镜像，来自如如下提供的2个镜像站点
	# 比如你想快速的使用tornado模块去开发一些东西
    	# -编译安装python3
		# -安装tornado模块及依赖关系
		# -加上你的代码才能够运行 
[root@localhost bin]# vim /etc/docker/daemon.json
{
  "registry-mirrors": [
    "https://dockerhub.azk8s.cn",
    "https://hub-mirror.c.163.com"
  ]
}

# 4.重启docker，运行docker
[root@localhost bin]# systemctl restart docker

# 5.获取一个ubuntu的基础镜像，docker的系统镜像，非常小， centos只有200M左右
[root@localhost bin]# docker pull ubuntu
```

**镜像的增删改查命令**

增

```shell
# 1.从dockerhub 仓库中获取docker的镜像，从github获取代码一个道理
[root@localhost bin]# docker pull ubuntu

# 2.获取一个hello-world进程
[root@localhost bin]# docker pull hello-world

# 3.搜索相关镜像
[root@localhost bin]# docker search centos

# 示例：比如你想用nginx，又不想修改宿主机的一个软件环境，直接用docker安装
docker search nginx
docker pull nginx
docker run nginx  # nginx服务器就能够运行在容器中，然后和宿主机有一个端口映射，就可以访问了
```

删

```shell
# 1.删除容器记录的命令
docker rm   # 容器id前3位 
[root@localhost bin]# docker rm 570
570

# 2.删除本地镜像文件
[root@localhost bin]# docker rmi feb

# 3.批量清空无用的docker容器记录，容器记录非常容易创建docke run  
# 批量删除挂掉的容器记录
[root@localhost bin]# docker rm `docker ps -aq`  # 把docker容器记录的id号，保存在反引号中，丢给docker rm实现批量删除

# 4.批量删除镜像
[root@localhost bin]# docker rmi `docker images -aq`

# 5.批量停止容器
docker stop `docker ps -aq`
docker start 容器id  	 # 启动暂停的容器
docker stop 容器id	 # 暂停一个容器
docker restart 容器id  # 重启容器
```

改

```shell
# 1.运行第一个docker的容器实例，运行镜像文件，产生容器进程
[root@localhost bin]# docker run hello-world
[root@localhost bin]# docker run  centos  # 运行centos基础镜像，如果docker容器中没有在后台运行的进程，容器会直接挂掉
# 如果你发现你的容器没有启动成功，说明容器内部出错了，程序没有运行     docker ps -a

# 2.运行一个hello-world容器进程
[root@localhost bin]# docker run hello-world

# 3.docker run指令还有一个功能是，当镜像不存在的时候，会自动去下载该进程
[root@localhost bin]# docker run centos  # 有2个功能，下载镜像，执行镜像
Unable to find image 'centos:latest' locally
Trying to pull repository docker.io/library/centos ...

# 4.交互式的运行一个存活的docker容器，centos
# -it参数： -i 是交互式的命令操作   -t开启一个终端   /bin/bash 指定shell解释器
# 容器空间内，有自己的文件系统
[root@localhost bin]# docker run -it centos /bin/bash  # 进入容器后，容器空间内是以容器id命名的
[root@6d9f3d17dcfb /]#
[root@localhost bin]# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                      PORTS               NAMES
6d9f3d17dcfb        centos              "/bin/bash"         About a minute ago   Exited (0) 19 seconds ago                       jolly_franklin

# 5.运行出一个活着的容器，在后台不断执行程序的容器
# docker run  运行镜像文件 -d 是让容器后台运行 -c 指定一段shell代码
# 运行centos镜像，生成容器实例，且有一段shell代码，在后台不断运行，死循环打印一句话，每秒钟打印一次
[root@localhost bin]# docker run -d centos /bin/sh -c "while true;do echo 我在学习docker;sleep 1;done"
2380d2c942c628362ce5ea627a31a828e0bacacbb4626e1c9def53933010a20b

# 6.运行docker容器，且指定名字，便于管理
[root@localhost bin]# docker run --name "python-docker" -d centos /bin/sh -c "while true;do echo 我在学习docker11;sleep 2;done"
b09f5586c8c1e93ba9e52903ca28385476564f98e820d569cc0217393bbc1c0e
[root@localhost bin]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
b09f5586c8c1        centos              "/bin/sh -c 'while..."   6 seconds ago       Up 5 seconds                            python-docker

# 7.进入一个正在运行的容器空间，进入一个线上正在运行的容器程序，修改其内部的资料
docker exec -it  容器id   /bin/bash  
[root@localhost bin]# docker exec -it b09 /bin/bash

# 8.如何进入容器空间内，修改容器内的环境，以及代码等内容，修改软件等操作，且提交镜像，发送给其他人
	# 8.1 进入容器内，安装一个vim
	[root@localhost bin]# docker run -it centos /bin/bash
	[root@af8bb7f2be55 /]# yum install vim -y
	
	# 8.2 提交该容器，生成新的镜像文件
	[root@localhost bin]# docker commit af8bb7f2be55 joker/joker-vim
	sha256:0bed4bbabbb4d380be3e612e9246989d9e4d3bce1de563e51de33c2fc344110c
	
	# 8.3 查看镜像
	[root@localhost bin]# docker images
	
# 9.导出你的docker镜像，可以发送给同事，或是其他人使用
docker save  镜像id   >   镜像的压缩文件
# 官方文档解释的是，docker save用的是tar命令压缩，应该是没有其他压缩格式的
[root@localhost lib]# docker save 6aa24867e7d7 > /opt/docker-centos-vim.tar.gz
# 你可以删掉本地的镜像，然后重新导入该压缩文件，模拟发送给同事的操作

# 10.如何进行docker镜像导入
docker  load   <   /opt/s25-centos-vim.tar.gz
首次导入该进项的时候，发现丢失了镜像tag标签，重新赋予一个即可
[root@localhost lib]# docker save 6aa24867e7d7 > /opt/docker-centos-vim.tar.gz  # 导出镜像
[root@localhost lib]# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
joker/joker-vim     latest              6aa24867e7d7        3 minutes ago       306 MB
docker.io/centos    latest              5d0da3dc9764        7 weeks ago         231 MB
[root@localhost lib]# docker rmi 6aa24867e7d7									# 删除镜像
Untagged: joker/joker-vim:latest
Deleted: sha256:6aa24867e7d73b7cf30380c908846a0e7aff9a5bdec299e29336a59d41034750
Deleted: sha256:fca69518b158efc68efefbeebc2ba7864c13692819fcc54e643c703c0c7b5334
[root@localhost lib]# docker images												# 查看是否删除成功
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
docker.io/centos    latest              5d0da3dc9764        7 weeks ago         231 MB
[root@localhost lib]# docker load < /opt/docker-centos-vim.tar.gz				# 导入镜像
98ce3121c9ab: Loading layer [==================================================>] 76.35 MB/76.35 MB
Loaded image ID: sha256:6aa24867e7d73b7cf30380c908846a0e7aff9a5bdec299e29336a59d41034750
[root@localhost lib]# docker images												# 查看是否导入成功
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
<none>              <none>              6aa24867e7d7        10 minutes ago      306 MB
docker.io/centos    latest              5d0da3dc9764        7 weeks ago         231 MB
[root@localhost lib]# docker tag 6aa24867e7d7 centos-vim						# 重新定义tag名
[root@localhost lib]# docker images												# tag名定义成功
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
centos-vim          latest              6aa24867e7d7        11 minutes ago      306 MB
docker.io/centos    latest              5d0da3dc9764        7 weeks ago         231 MB

# 11.如何在docker内，运行一个python web的程序，需要用到端口映射知识
# -d 后台运行
# -P 大写的P参数，作用是随机的端口映射
# -p 小写的p，指定端口，不随机生成
# training/webapp 是镜像的名字，默认没有会去在线下载
# python app.py   代表启动容器后，让容器执行的命令是它
# 因此这个命令作用是，启动一个webapp镜像，且在容器中执行 python app.py
# -p 6000:5000  访问宿主机的6000，就是访问容器的5000了
[root@localhost lib]# docker run --name "docker-first-webapp" -d -p 6000:5000 training/webapp python app.py

	# 11.1查看宿主机与容器的映射端口
	[root@localhost lib]# docker ps
    CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS                    NAMES
    7cb6a45dce18        training/webapp     "python app.py"     About a minute ago   Up 40 seconds       0.0.0.0:6000->5000/tcp   s25webdocker

	# 11.2查看指定容器的映射端口
	[root@localhost lib]# docker port 7cb
	5000/tcp -> 0.0.0.0:6000
	
	# 11.3使用curl命令访问路由
	[root@localhost lib]# curl 127.0.0.1:6000
	Hello world![root@localhost lib]#
	
	# 11.4查看容器内的进程
	[root@localhost lib]# docker top 7cb
    UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
    root                3792                3776                0                   19:05               ?                   00:00:00            python app.py


# 12.进入该webapp的容器，查看里面的内容
docker exec -it  容器id   /bin/bash  # 进入容器内，可以进行相应的修改操作
	# 12.修改内容后，重启容器，方可生效
	docker restart  容器id  # 重启该容器，重新读取代码，即可生效
```

查

```shell
# 1.查看本地机器，所有的镜像文件内容
[root@localhost bin]# docker images

# 2.查看docker正在运行的容器进程
[root@localhost bin]# docker ps

# 3.查看所有运行，以及挂掉的容器进程
[root@localhost bin]# docker ps -a

# 4.查看容器内的运行日志
docker logs  容器id
docker logs -f  容器id   # 实时刷新容器内的日志，例如检测nginx等日志信息

# 5.查看容器内的端口转发情况
docker port  容器id  # 查看容器的端口转发
```

<img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211106105949792.png" alt="image-20211106105949792" style="zoom:50%;" />

### 7.3 Dockerfile

> 镜像是容器的基础，每次执行docker run的时候都会指定哪个镜像作为容器运行的基础。我们之前的例子都是使用来自docker hub的镜像，直接使用这些镜像只能满足一定的需求，当镜像无法满足我们的需求时，就得自定制这些镜像。

```shell
# 镜像的定制就是定制每一层所添加的配置、文件。
# 如果可以把每一层修改、安装、构建、操作的命令都写入到一个脚本，用脚本来构建、定制镜像，这个脚本就是dockerfile。
# Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令 构建一层，因此每一条指令的内容，就是描述该层应当如何构建。
```

```shell
# FROM 指令表示，告诉该dockerfile以哪个镜像为基础
FROM scratch # 制作base image 基础镜像，尽量使用官方的image作为base image
FROM centos # 使用base image 为基础镜像
FROM ubuntu:14.04 # 带有tag的base image


# LABEL标签，定义变量，定义注释信息等
LABEL version=“1.0” # 容器元信息，帮助信息，Metadata，类似于代码注释
LABEL maintainer=“yc_uuu@163.com"


# RUN是一个完成指令，你可以用它在docker中执行任意的命令，告诉容器要做哪些配置
示例：RUN命令
RUN mdkir /docker-python
RUN cd /docker-python
RUN cd
RUN pwd  # 输出家目录
# 对于复杂的RUN命令，避免无用的分层，多条命令用反斜线换行，合成一条命令！
RUN yum update && yum install -y vim \
    Python-dev # 反斜线换行
RUN /bin/bash -c "source $HOME/.bashrc;echo $HOME”


# 切换目录使用WORKDIR，一定要用大写！！
WORKDIR /root # 相当于linux的cd命令，改变目录，尽量使用绝对路径！！！不要用RUN cd
WORKDIR /test # 如果没有就自动创建
WORKDIR demo  # 再进入demo文件夹
RUN pwd       # 打印结果应该是/test/demo


# ADD指令用于添加宿主机的文件，放入到容器空间内
# 宿主机有自己的文件系统，文件夹，文件，目录等
# 容器内也有一套自己的文件系统，独立的文件信息
# 把宿主机的代码，拷贝到容器内
# ADD还有解压缩的功能，这是一个坑，需要注意！
ADD and COPY 
ADD hello.txt /opt 			# 把本地文件添加到镜像中：把本地的hello可执行文件拷贝到镜像的 /opt 目录下
ADD test.tar.gz /opt    	# 添加到根目录并解压
RUN tar -zxvf test.tar.gz   # 直接报错，文件不存在 ，因为上一步，ADD指令已经对tar.gz压缩包解压缩了

WORKDIR /root
ADD hello.txt test/  		# 进入/root/ 添加hello.txt可执行命令到test目录下，也就是/root/test/hello 一个绝对路径
COPY hello test/  			# 等同于上述ADD效果

# 注：ADD与COPY
   #  -优先使用COPY命令
   #  -ADD除了COPY功能还有解压功能
# 添加远程文件/目录使用curl或 wget



ENV # 环境变量，尽可能使用ENV增加可维护性
ENV MYSQL_VERSION 5.6 # 设置一个mysql常量
RUN yum install -y mysql-server=“${MYSQL_VERSION}” 
RUN yum install -y mysql-server=“${MYSQL_VERSION}” 
RUN yum install -y mysql-server=“${MYSQL_VERSION}” 
RUN yum install -y mysql-server=“${MYSQL_VERSION}”

------这里需要稍微理解一下了-------中级知识---先不讲
VOLUME and EXPOSE 
存储和网络

RUN and CMD and ENTRYPOINT
RUN：执行命令并创建新的Image Layer
CMD：设置容器启动后默认执行的命令和参数
ENTRYPOINT：设置容器启动时运行的命令

Shell格式和Exec格式
RUN yum install -y vim
CMD echo ”hello docker”
ENTRYPOINT echo “hello docker”

Exec格式
RUN [“apt-get”,”install”,”-y”,”vim”]
CMD [“/bin/echo”,”hello docker”]
ENTRYPOINT [“/bin/echo”,”hello docker”]


通过shell格式去运行命令，会读取$name指令，而exec格式是仅仅的执行一个命令，而不是shell指令
cat Dockerfile
    FROM centos
    ENV name Docker
    ENTRYPOINT [“/bin/echo”,”hello $name”]#这个仅仅是执行echo命令，读取不了shell变量
    ENTRYPOINT  [“/bin/bash”,”-c”,”echo hello $name"]

CMD
容器启动时默认执行的命令
如果docker run指定了其他命令(docker run -it [image] /bin/bash )，CMD命令被忽略
如果定义多个CMD，只有最后一个执行

ENTRYPOINT
让容器以应用程序或服务形式运行
不会被忽略，一定会执行
最佳实践：写一个shell脚本作为entrypoint
COPY docker-entrypoint.sh /usr/local/bin
ENTRYPOINT [“docker-entrypoint.sh]
EXPOSE 27017
CMD [“mongod”]

[root@master home]# more Dockerfile
FROm centos
ENV name Docker
#CMD ["/bin/bash","-c","echo hello $name"]
ENTRYPOINT ["/bin/bash","-c","echo hello $name”]
```

**dockfile实战，写一个flask容器脚本**

构建镜像的步骤

```shell
# 1.准备好一个flask代码，检查需要哪些依赖步骤
[root@localhost opt]# cat docker-flask.py
from flask import Flask
app=Flask(__name__)
@app.route('/')
def hello():
    return "linux就即将结束了，祝大家，找到好工作，有linux问题呢，尽量和我来沟通，互相学习"
if __name__=="__main__":
    app.run(host='0.0.0.0',port=8080)
    
# 2.在宿主机环境检查如何能够运行该脚本，发现需要安装flask模块
[root@localhost opt]# pip3 install -i https://pypi.douban.com/simple flask

# 3.编写dockerfile脚本，注意名字必须是 大写Dockerfile，写入如下的内容
[root@localhost opt]# vim Dockerfile
FROM python
RUN pip3 install -i https://pypi.douban.com/simple flask
ADD docker-flask.py /opt
WORKDIR /opt
EXPOSE 8080
CMD ["python3", "docker-flask.py"]

# 4.检查准备的脚本代码，以及Dockerfile文件
[root@localhost docker-build]# ll
total 8
-rw-r--r--. 1 root root 155 Nov  6 19:34 Dockerfile
-rw-r--r--. 1 root root 264 Nov  6 19:28 docker-flask.py

# 5.构建该dockerfile，生成镜像
[root@localhost opt]# docker build .

# 6.检查docker的镜像，是否生成
[root@localhost docker-build]# docker images
REPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE
<none>                      <none>              e3ec536d05af        17 seconds ago      928 MB
centos-vim                  latest              6aa24867e7d7        About an hour ago   306 MB
# 6.1修改一下镜像的标签名
[root@localhost docker-build]# docker tag e3e python-flask
[root@localhost docker-build]# docker images
REPOSITORY                  TAG                 IMAGE ID            CREATED              SIZE
python-flask                latest              e3ec536d05af        About a minute ago   928 MB

# 7.运行该镜像文件，查看是否能够运行容器内的flask
[root@localhost docker-build]# docker run  -d  -p  8000:8080  e3e
043fef3c03f7d83914735e83ba781f78f950412ddea5ee66f8ff7b13b615cd7b

# 8.访问宿主机端口，查看容器内的应用
[root@localhost docker-build]# curl 127.0.0.1:8000
linux就即将结束了，祝大家，找到好工作，有linux问题呢，尽量和我来沟通，互相学习[root@localhost docker-build]#

# 9.可以修改容器内的代码，重启容器
[root@localhost docker-build]# docker exec -it 043 /bin/bash
	# 9.1修改容器内的代码
	root@043fef3c03f7:/opt# sed -i "s/linux就即将结束了，祝大家，找到好工作，有linux问题呢，尽量和我来沟通，互相学习/修改成功了/" docker-flask.py


# 10.重启容器
[root@localhost docker-build]# docker restart 043

# 11.再次访问容器内应用，查看更新的代码内容
[root@localhost docker-build]# curl 127.0.0.1:8000
修改成功了[root@localhost docker-build]#
```

**docker基础命令注释**

```shell
[root@docker ~]# docker --help

Usage:
docker [OPTIONS] COMMAND [arg...]

       docker daemon [ --help | ... ]

       docker [ --help | -v | --version ]

 

A
self-sufficient runtime for containers.

 

Options:

 

  --config=~/.docker              Location of client config files  #客户端配置文件的位置

  -D, --debug=false               Enable debug mode  #启用Debug调试模式

  -H, --host=[]                   Daemon socket(s) to connect to  #守护进程的套接字（Socket）连接

  -h, --help=false                Print usage  #打印使用

  -l, --log-level=info            Set the logging level  #设置日志级别

  --tls=false                     Use TLS; implied by--tlsverify  #

  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA  #信任证书签名CA

  --tlscert=~/.docker/cert.pem    Path to TLS certificate file  #TLS证书文件路径

  --tlskey=~/.docker/key.pem      Path to TLS key file  #TLS密钥文件路径

  --tlsverify=false               Use TLS and verify the remote  #使用TLS验证远程

  -v, --version=false             Print version information and quit  #打印版本信息并退出

 

Commands:

    attach    Attach to a running container  #当前shell下attach连接指定运行镜像

    build     Build an image from a Dockerfile  #通过Dockerfile定制镜像

    commit    Create a new image from a container's changes  #提交当前容器为新的镜像

    cp    Copy files/folders from a container to a HOSTDIR or to STDOUT  #从容器中拷贝指定文件或者目录到宿主机中

    create    Create a new container  #创建一个新的容器，同run 但不启动容器

    diff    Inspect changes on a container's filesystem  #查看docker容器变化

    events    Get real time events from the server#从docker服务获取容器实时事件

    exec    Run a command in a running container#在已存在的容器上运行命令

    export    Export a container's filesystem as a tar archive  #导出容器的内容流作为一个tar归档文件(对应import)

    history    Show the history of an image  #展示一个镜像形成历史

    images    List images  #列出系统当前镜像

    import    Import the contents from a tarball to create a filesystem image  #从tar包中的内容创建一个新的文件系统映像(对应export)

    info    Display system-wide information  #显示系统相关信息

    inspect    Return low-level information on a container or image  #查看容器详细信息

    kill    Kill a running container  #kill指定docker容器

    load    Load an image from a tar archive or STDIN  #从一个tar包中加载一个镜像(对应save)

    login    Register or log in to a Docker registry#注册或者登陆一个docker源服务器

    logout    Log out from a Docker registry  #从当前Docker registry退出

    logs    Fetch the logs of a container  #输出当前容器日志信息

    pause    Pause all processes within a container#暂停容器

    port    List port mappings or a specific mapping for the CONTAINER  #查看映射端口对应的容器内部源端口

    ps    List containers  #列出容器列表

    pull    Pull an image or a repository from a registry  #从docker镜像源服务器拉取指定镜像或者库镜像

    push    Push an image or a repository to a registry  #推送指定镜像或者库镜像至docker源服务器

    rename    Rename a container  #重命名容器

    restart    Restart a running container  #重启运行的容器

    rm    Remove one or more containers  #移除一个或者多个容器

    rmi    Remove one or more images  #移除一个或多个镜像(无容器使用该镜像才可以删除，否则需要删除相关容器才可以继续或者-f强制删除)

    run    Run a command in a new container  #创建一个新的容器并运行一个命令

    save    Save an image(s) to a tar archive#保存一个镜像为一个tar包(对应load)

    search    Search the Docker Hub for images  #在docker
hub中搜索镜像

    start    Start one or more stopped containers#启动容器

    stats    Display a live stream of container(s) resource usage statistics  #统计容器使用资源

    stop    Stop a running container  #停止容器

    tag         Tag an image into a repository  #给源中镜像打标签

    top       Display the running processes of a container #查看容器中运行的进程信息

    unpause    Unpause all processes within a container  #取消暂停容器

    version    Show the Docker version information#查看容器版本号

    wait         Block until a container stops, then print its exit code  #截取容器停止时的退出状态值

 

Run 'docker COMMAND --help' for more information on a command.  #运行docker命令在帮助可以获取更多信息
```

---

## 第八章 自动化运维

### 8.1 自动化运维介绍

传统的IT运维是将数据中⼼中的⽹络设备、服务器、数据 库、中间件、存储、虚拟化、硬件等资源进⾏统⼀监 控，当资源出现告警时，运维⼈员通过⼯具或者基于经验进⾏排查，找出问题并加以解决。但是，随着互联⽹+时代的到来，移动互联⽹、云计算和⼤数据技术得到了 ⼴泛应⽤，从⽽导致企业所管理的IT架构不断扩⼤，服务 器、虚拟化、存储设备的数量越来越多，⽹络也变得更加复杂，业务流程越来越繁琐，传统的运维管理也越来越⼒不从⼼。

![1639295365737](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639295365737.png)

**⾃动化运维**

⾃动化运维，可实现⽇常设备监控、主动发现问题、⾃动分析定位、基于标准化流程⼯具规范化处理、通过⾃动化运维操作⼯具处理修复等功能，最终实现监管治⾃动化运维。

**⾃动化**

- 监控⾃动化 

- 数据采集⾃动化 

- 数据分析⾃动化 

- ⽇常巡检⾃动化 

- 设备配置⽐对⾃动化 

- 故障定位⾃动化  

- 故障处理⾃动化 

- 流程处理⾃动化 

- ⽇常备份⾃动化 

- 系统优化⾃动化  

- ⼤批量配置⾃动化 

![1639295446358](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639295446358.png)

> SALTSTACK: https://docs.saltstack.com/en/latest/
>
> PUPPET: https://puppet.com/?_ga=2.223205342.4314246.1540783578-2053614486.1540783578
>
> ANSIBIE: https://www.ansible.com/

![1639295392252](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639295392252.png)

### 8.2 Saltstack部署

- SaltStack管理⼯具允许管理员对多个操作系统创建⼀个⼀致的管理系统
- Saltstack最主要的两个功能是：配置管理与远程执⾏
- Saltstack不只是⼀个配置管理⼯具，还是⼀个云计算与数据中⼼架构编排的利器
- 采⽤订阅者模式的分布式管理⼯具 管理节点【发命令】——>被管理节点 【去执⾏ 返回结果】
- 采⽤⾃定义协议zeromq ⽐SSH（⼀对⼀）速度更快

![1639295473334](J:\homework\Linux学习笔记\Linux学习笔记.assets\1639295473334.png)

**salt ⼯作原理**

SaltStack 采⽤ C/S模式，server端就是salt的master，client端就是minion， minion与master之间通过ZeroMQ消息队列通信。

minion上线后先与master端联系，把⾃⼰的pub key发过去，这时master端通过salt-key -L命令就会看到minion的key，接受该minion-key后，也就是master与minion已经互信。

master可以发送任何指令让minion执⾏了，salt有很多可执⾏模块，⽐如说cmd模块，在安装minion的时候已经⾃带了，它们通常位于你的python库中，locate salt |  grep /usr/ 可以看到salt⾃带的所有东⻄。

这些模块是python写成的⽂件，⾥⾯会有好多函数，如cmd.run，当我们执⾏salt '*' cmd.run 'uptime'的时候，master下发任务匹配到的minion上去，minion执⾏模块函数，并返回结果。master监听4505和4506端⼝，4505对应的是ZMQ的PUB system，⽤来发送消息，4506对应的是REP system是来接受消息的。

⾃⼰也可以管理⾃⼰  

1) 安装包 

2）配置⽂件  

3）调整平台 

4）应⽤ 

5）部署⼀个环境使⽤salt

**salt-master安装**

- yum install salt-syndic 分布式代理
- yum install salt-cloud 也是基于openstack来做的，它可以⽀持多种云的使⽤

如：Aliyun、Azure、DigitalOcean、EC2、Google Compute Engine、HP Cloud、OpenStack

```shell
# 1.运行以下命令来安装 SaltStack 存储库和密钥
sudo rpm --import https://repo.saltproject.io/py3/redhat/7/x86_64/3004/SALTSTACK-GPG-KEY.pub
curl -fsSL https://repo.saltproject.io/py3/redhat/7/x86_64/3004.repo | sudo tee /etc/yum.repos.d/salt.repo
cd /etc/yum.repos.d

# 2.清理缓存
[root@localhost yum.repos.d]# sudo yum clean expire-cache

# 3.安装
[root@localhost yum.repos.d]# yum install salt-master  # salt-master
[root@localhost yum.repos.d]# yum install salt-minion  # salt-minion

# 4.把下载的包放到/usr/src目录下，执行以下命令
[root@localhost master]# cd /usr/src/master 配置minion  cd /usr/src/minion
[root@localhost master]# yum -y localinstall -y ./*

# 5.修改配置文件
[root@localhost salt]# cd /etc/salt/
[root@localhost salt]# cp master master.bak 配置minion cp minion minion.bak
[root@localhost salt]# cat /etc/salt/master | head -2
master: 192.168.88.128
user: root

[root@localhost salt]# cat /etc/salt/minion | head -3
master: 192.168.88.128
user: root
id: minion-01

# 6.启动，查看主/minion机端口
[root@localhost salt]# service salt-master start
[root@localhost salt]# systemctl start salt-minion
[root@localhost salt]# netstat -ntlp
tcp        0      0 0.0.0.0:4505            0.0.0.0:*               LISTEN      6091/python3
tcp        0      0 0.0.0.0:4506            0.0.0.0:*               LISTEN      6097/python3

# 7.列出待加入的机器
[root@localhost salt]# salt-key -L
Accepted Keys:  # 同意加⼊的
Denied Keys:	# 未被加入的
Unaccepted Keys: # 检测到未处理的
minion-01
Rejected Keys:	# 备拒绝的
```

salt-key相关命令：

- salt-key -L 列出来认⼤哥的
- salt-key -A 所有⼩弟一起认
- salt-key -a 单个小弟认
- salt-key -D 剔除所有⼩弟
- salt-key -d 删除某个

```shell
# 1.把minion-01加入
[root@localhost salt]# salt-key -a minion-01
The following keys are going to be accepted:
Unaccepted Keys:
minion-01
Proceed? [n/Y] y
Key for minion minion-01 accepted.
```

**使用salt**

```shell
# 1.查看模块列表module
[root@localhost salt]# salt 'minion-01' sys.list_modules

# 2.查看指定module的function⽤法
[root@localhost salt]# salt 'minion-01' sys.list_functions uptime

# 3.查看指定模块的详细⽤法
[root@localhost salt]# salt 'minion-01' sys.doc uptime.create
```

**查看minion**

```shell
# 1.查看minion的状态
[root@localhost salt]# salt-run manage.status
down:
up:
    - minion-01
    
# 2.查看minion在线状态
[root@localhost salt]# salt-run manage.up
- minion-01

# 3.查看minion不在线状态
[root@localhost salt]# salt-run manage.down
```

**salt-cp** 拷贝单个文件到minion上，不支持目录分发，通常在master上执行

```shell
# 1.拷贝单个文件
[root@localhost salt]# salt-cp minion-01 roster /tmp
minion-01:
    ----------
    /tmp/roster:
        True

# 2.拷贝单文个文件到minion的指定目录并改名
[root@localhost salt]# salt-cp minion-01 proxy /tmp/test.py
minion-01:
    ----------
    /tmp/test.py:
        True

```

**salt-call** minion执⾏本地命令，自己执行可执行模块，不是通过master下发job

```shell
# 1.minion本地执行命令
[root@localhost tmp]# salt-call test.ping
local:
    True

# 2.master执行i检测minion机器状态
[root@localhost salt]# salt "*" test.ping
minion-01:
    True

```

模块命令

```shell
# 1.minion查看用户
[root@localhost salt]# vipw

# 2.给minion插入一个用户
[root@localhost salt]# salt 'minion-01' user.add xxx
minion-01:
    True
```

系统命令

```shell
# 通过master向minion下发系统命令
[root@localhost srv]# salt "minion-01" cmd.run 'hostname'
minion-01:
    relase1
```

执行脚本

```shell
# 1.在/srv/salt新建一个测试脚本
[root@localhost srv]# cat test.sh
#!/bin/bash

echo "$HOSTNAME"
echo
echo
echo "`date +%F`"
echo "this is a test for salt"

# 2.通过master向minion下发脚本
[root@localhost salt]# salt "*" cmd.script "salt://test.sh"
minion-01:
    ----------
    pid:
        3152
    retcode:
        0
    stderr:
    stdout:
        relase1


        2021-12-03
        this is a test for salt
        
# 注意：/srv/salt 脚本路径 默认，所以要先创建好路径
```

查看salt minion在执⾏什么操作

```shell
# 1.开启两个master，一个监控，一个执行
[root@localhost ~]# salt-run jobs.active
20211203105133957541:
    ----------
    Arguments:
        - yum update -y
    Function:
        cmd.run
    Returned:
    Running:
        |_
          ----------
          minion-01:
              3168
    StartTime:
        2021, Dec 03 10:51:33.957541
    Target:
        *
    Target-type:
        glob
    User:
        root

[root@localhost salt]# salt "*" cmd.run "yum update -y"

```

查看正在运⾏的任务,找到jid

```shell
# 1.查看正在执行任务的jid
[root@localhost ~]# salt 'minion-01' saltutil.running

# 2.根据jid杀掉任务
[root@localhost ~]# salt 'minion-01' saltutil.kill_job 20181029231137078329

# 3.清除minion缓存
[root@localhost ~]# salt '*' saltutil.clear_cache
```

## 第九章 shell脚本

 通过shell语言将完成一个任务的所有代码写入一个文件，并给执行权限。 

**shell优势**

- 解放运维人员：7X24小时监控，监控为例，监控帮你干活，你只需要处理问题就好。
- 提升业务能力：业务初始化，自动备份，日志分析，你的工作脚本来做，效率更高。
- 提升管理能力：从系统安装到业务部署，再到服务器管理维护，实现自动化运维，批量管理机器与业务。
- 提升运维薪资：技术能力和工资成正比。

- 上手快
- 入门简单
- 学习周期短

**shell能干什么**

**重点：重复性的工作，全部通过脚本来完成。高效的同时还不出错。**

- 根据企业架构自定义监控系统，量身打造企业级监控系统
- 业务初始化部署系统，业务初始化全部一键搞定，省去繁琐的安装与排错
- 一键备份，分分钟搞定备份问题
- 日志分析，繁琐又复杂的日志分析让机器取做吧。
- 三方软件模块插件的编写：根据业务定制三方软件的功能，更贴合自己的业务。

 ![shell学习路线图.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600908281266.png) 

**成长路径**

  ![shell编程掌握等级图.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600908316400.png) 

```shell
# 1.能看到代码实现的算法、原理
# 2.能根据自己的脚本应用要求修改脚本
# 3.能根据业务需求写脚本
# 4.能根据脚本执行问题优化脚本代码
```

### 9.1 常用命令复习

 shell脚本可以让降低大家的工作强度，提升大家的管理能力和薪资报酬，还可以让大家有时间学习提升自己。所以，学好shell那就是非常必要的啦，那么在学习shell之前我们得热热身，做一下准备工作了。 

> ```
> shell脚本本质上就是要将完成一件事情的所有命令按照执行的先后顺序写入一个文件，并给予文件执行权限
> ```

 想学好shell脚本首先要考核一下自己的linux命令学的是否扎实，接下来为了能让大家学习shell更加顺畅，我把常用的一些数据处理的命令给大家回顾一下。 

```shell
# 我们重点回顾以下几类命令

# 1.数据检索命令
	# 1.1 行检索：grep  egrep
    # 1.2 字符串检索:cut  tr

# 2.数据处理命令       
	# 2.1 数据排序：sort
  	# 2.2 数据去重: uniq
  	# 2.3 文本数据合并: paste
  	# 2.4 数据输出: tee
  	# 2.5 数据处理: xargs    
```

#### 9.1.1 数据处理命令

**grep: 负责从数据源中检索对应的字符串，行过滤。**

```shell
# grep用于根据关键字进行行过滤
grep options 'keys' filename
OPTIONS:
    -i: # 不区分大小写
    -v: # 查找不包含指定内容的行,反向选择
    -w: # 按单词搜索
    -n: # 显示行号
    -A: # 显示匹配行及后面多少行 -A 5
    -B: # 显示匹配行及前面多少行
    
    -o: # 打印匹配关键字
    -c: # 统计匹配到的次数
    -r: # 逐层遍历目录查找
    -C: # 显示匹配行前后多少行
    -l:	# 只列出匹配的文件名
    -L:	# 列出不匹配的文件名
    -e:	# 使用正则匹配
    -E:	# 使用扩展正则匹配
    ^key:	# 以关键字开头
    key$:	# 以关键字结尾
    ^$:		# 匹配空行
    --color=auto ： # 可以将找到的关键词部分加上颜色的显示
```

常用命令必会，示例：

```shell
grep -i root passwd		# 忽略大小写匹配包含root的行
grep -w ftp passwd 		# 精确匹配ftp单词
grep -wo ftp passwd 	# 打印匹配到的关键字ftp
grep -n root passwd 	# 打印匹配到root关键字的行号
grep -ni root passwd 	# 忽略大小写匹配统计包含关键字root的行
grep -nic root passwd 	# 忽略大小写匹配统计包含关键字root的行数
grep -i ^root passwd 	# 忽略大小写匹配以root开头的行
grep bash$ passwd 		# 匹配以bash结尾的行
grep -n ^$ passwd 		# 匹配空行并打印行号
grep ^# /etc/vsftpd/vsftpd.conf 	# 匹配以#号开头的行
grep -v ^# /etc/vsftpd/vsftpd.conf 	# 匹配不以#号开头的行
grep -A 5 mail passwd   # 匹配包含mail关键字及其后5行
grep -B 5 mail passwd   # 匹配包含mail关键字及其前5行
grep -C 5 mail passwd 	# 匹配包含mail关键字及其前后5行
```

centos8中已经为大家设置了，存放在/etc/profile.d/colorgrep.sh文件中，如若大家使用的系统中没有设置颜色输出，可以使用以下方法来自行设置。

```shell
# 临时设置：
alias grep='grep --color=auto'  # 只针对当前终端和当前用户生效

# 永久设置：全局
vim /etc/bashrc
alias grep='grep --color=auto'
source /etc/bashrc

# 局部：针对某个用户
vim ~/.bashrc
alias grep='grep --color=auto'

# 注意：如果希望你对环境变量的设置立刻生效，可以使用以下命令而不需要重启计算机
source ~/.bashrc
```

#### 9.1.2 cut数据截取

```shell
# cut用于列截取
-c:  # 以字符为单位进行分割。
-d:  # 自定义分隔符，默认为制表符。\t
-f:  # 与-d一起使用，指定显示哪个区域。

# 示例：
cut -d ":" -f3 1.txt 		# 以:冒号分割，截取第3列内容
cut -d ":" -f 1,3,5 1.txt	# 以:冒号分割，截取第1,3,5列内容
cut -c 4 1.txt				# 截取文件中每行第4个字符
cut -c1-4 1.txt				# 截取文件中每行的1-4个字符
cut -c4-10 1.txt			# 截取文件中每行第4-10个字符
cut -c5- 1.txt 				# 从第5个字符开始截取后面所有字符
```

#### 9.1.3 tr字符转换

```shell
# tr用来从标准输入中通过替换或删除操作进行字符转换；主要用于删除文件中控制字符或进行字符转换。
# 使用tr时要转换两个字符串：字符串1用于查询，字符串2用于处理各种转换。

语法：
commands|tr  'string1'  'string2'
tr  'string1'  'string2' < filename

tr options 'string1' < filename

-d : # 删除字符串1中所有输入字符。
-s : # 删除所有重复出现字符序列，只保留第一个；即将重复出现字符串压缩为一个字符串。

a-z 任意小写
A-Z 任意大写
0-9 任意数字

# 示例：
tr -d '[:/]' < 3.txt 		# 删除文件中的:和/
cat 3.txt |tr -d '[:/]' 	# 删除文件中的:和/
tr '[0-9]' '@' < 3.txt 		# 将文件中的数字替换为@符号
tr '[a-z]' '[A-Z]' < 3.txt 	# 将文件中的小写字母替换成大写字母
tr -s '[a-z]' < 3.txt		# 匹配小写字母并将重复的压缩为一个
tr -s '[a-z0-9]' < 3.txt	# 匹配小写字母和数字并将重复的压缩为一个
tr -d '[0-9]' < 1.txt       # 删除文件中0-9字符
```

#### 9.1.4 sort排序

```shell
# 将文件的每一行作为一个单位;从首字符向后;依次按ASCII码值进行比较，最后将他们按升序输出。

语法：
sort [options] [filename]

-u ：# 去除重复行
-r ：# 降序排列，默认是升序
-o : # 将排序结果输出到文件中  类似 重定向符号 >
-n ：# 以数字排序，默认是按字符排序
-t ：# 分隔符
-k ：# 第N列

-b ： # 忽略前导空格。
-R ： # 随机排序，每次运行的结果均不同。
 
# 示例：
sort -n -t: -k3 1.txt 	# 按照用户的uid进行升序排列
sort -nr -t: -k3 1.txt 	# 按照用户的uid进行降序排列
sort -n 2.txt 			# 按照数字排序
sort -nu 2.txt 			# 按照数字排序并且去重
sort -nr 2.txt 			# 按数字降序排序
sort -nru 2.txt 		# 按数字降序排序去重
sort -n 2.txt -o 3.txt 	# 按照数字排序并将结果重定向到文件
sort -R 2.txt 			# 随机排序
sort -u 2.txt 			# 去重
```

#### 9.1.5 uniq 连续去重

 **应用技巧：先排序后去重** 

```shell
# 去除连续重复行、

语法：
uniq [options] [filename]

-i: 	# 忽略大小写
-c: 	# 统计重复行次数
-d: 	# 只显示重复行

sort -n test.txt | uniq		# 按数字排序后去重
sort -n test.txt | uniq -c	# 按数字排序去重，统计去重个数
sort -n test.txt | uniq -dc	# 按数字排序去重，显示重复行，统计重复个数
```

#### 9.1.6 tee双向输出

```shell
# tee工具从标准输入读取并写入标准输出和文件，即：双向覆盖重定向<屏幕输出|文本输入>
somecommand |tee filename

-a 	# 双向追加重定向

echo hello world			# 打印hello world
echo hello world|tee file1	# 打印hello world 输入到文件
cat file1 					# 查看文件
echo 999|tee -a file1		# 打印 999 追加到file1
cat file1 					# 查看文件
```

#### 9.1.7 paste合同文件

```shell
# paste工具用于合并文件行输出到屏幕，不会改动源文件
-d：	# 自定义间隔符，默认是tab,只接受一个字符
-s：	# 将每个文件中的所有内容按照一行输出，文件中的行与行以TAB间隔。

[root@localhost opt]# paste b.txt a.txt
world   hello
888
999
[root@localhost opt]# paste -d "#" b.txt a.txt
world#hello
888#
999#
[root@localhost opt]# paste -s b.txt a.txt
world   888     999
hello

```

#### 9.1.8 xargs命令

```shell
# 上一个命令的输出作为下一个命令的输入，做的是数据源。

命令格式：
''[somecommand]|[filename]'' |xargs -item  command

OPTIONS:
-a 	# file 从文件中读入作为sdtin
-E 	# flag flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。
-p 	# 当每次执行一个argument的时候询问一次用户。
-n 	# num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。
-t 	# 表示先打印命令，然后再执行。
-i 	# 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给 {}，可以用 {} 代替。
-r 	# no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。
-d 	# delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。

[root@localhost opt]# touch test.txt
[root@localhost opt]# find ./ -name test.txt | xargs gzip
[root@localhost opt]# ll
total 8
-rw-r--r-- 1 root root 25 Dec  4 16:18 1.txt
-rw-r--r-- 1 root root 29 Dec  4 16:22 test.txt.gz
[root@localhost opt]# xargs -a 1.txt
1 2 3 4 5 6 7 8 9 10 11
[root@localhost opt]# xargs -a 1.txt -E 6
1 2 3 4 5
[root@localhost opt]# xargs -a 1.txt -p
echo 1 2 3 4 5 6 7 8 9 10 11 ?...y
1 2 3 4 5 6 7 8 9 10 11
[root@localhost opt]# xargs -a 1.txt -t
echo 1 2 3 4 5 6 7 8 9 10 11
1 2 3 4 5 6 7 8 9 10 11
[root@localhost opt]# xargs -a 1.txt -d " "
1
2
3
4
5
6
7
8
9
10
11
```

#### 9.1.9 shell字符

```shell
# 1.有基础的同学不要和正则表达式中的符号含义搞混淆了。
    !:               #  执行历史命令   !! 执行上一条命令
    $:               #  变量中取内容符
    + - * / %:       #  对应数学运算  加 减 乘 除 取余数  
    &:               #  后台执行
    ;：				# 分号可以在shell中一行执行多个命令，命令之间用分号分割    
    \:               #  转义字符
    ``:              #  反引号 命令中执行命令    echo "today is `date +%F`"
    ' ':             #  单引号，脚本中字符串要用单引号引起来，但是不同于双引号的是，单引号不解释变量
    " ":             #  双引号，脚本中出现的字符串可以用双引号引起来
    
# 2.通配符 
    ~:                # 家目录    # cd ~ 代表进入用户家目录
    *:                # 星号是shell中的通配符  匹配所有
    ?:                # 问号是shell中的通配符  匹配除回车以外的一个字符
    [list]: 		  # 匹配[list]中的任意单个字符
	[!list]: 		  # 匹配除list中的任意单个字符
{string1,string2,...}：# 匹配string1,string2或更多字符串

# 3.重定向
	>       		  # 覆盖输入 
	>> 				  # 追加输入
	< 				  # 输出
	<< 				  # 追加输出

# 4.管道命令
    |：               # 管道符 上一个命令的输出作为下一个命令的输入   cat filename | grep "abc"
```

#### 9.2.0 组合命令实战

```shell
# 1.job1: 检索本机的IP、NETMASK、MAC地址、广播地址
    IP:  172.20.10.3
    NetMask:  255.255.255.240
    Broadcast:  172.20.10.15
    MAC Address:  00:0c:29:8d:49:ea
[root@localhost opt]# ifconfig ens33|grep -w "inet"|tr -s " "|cut -d " " -f3|xargs echo "ip: "
ip:  192.168.88.128
[root@localhost opt]# ifconfig ens33|grep -w "inet"|tr -s " "|cut -d " " -f5|xargs echo "netmask: "
netmask:  255.255.255.0
[root@localhost opt]# ifconfig ens33|grep -w "inet"|tr -s " "|cut -d " " -f7|xargs echo "broadcast: "
broadcast:  192.168.88.255
[root@localhost opt]# ifconfig ens33|grep -w "ether"|tr -s " "|cut -d " " -f3|xargs echo "MAC: "
MAC:  00:0c:29:32:06:9d

# 2.job2: 将系统中root的用户名、密码和默认shell保存到一个文件中，要求用户名密码和默认shell之间用tab键分割
ayitula x /bin/bash
[root@localhost opt]# grep -i "bash" /etc/passwd|cut -d ":" -f1,2,7|tr ":" "\t" > root.txt
root    x       /bin/bash

# 组合命令应用的精髓：先处理成统一格式，然后再截取
	1.先截取行
	2.统一数据格式
	3.截取字符串
	4.处理字符串
```

### 9.2 shell脚本介绍

**编程语言介绍：**

编程语言是指计算机能理解的语言，人类通过使用计算机语言可以给计算机批量下达任务，让其按照人类的思想去完成工作。最常见的语言有：汇编语言、C语言、java语言、php语言、Python语言、golang语言等等。

编程语言分类：

- 编译型语言
  程序在执行之前需要一个专门的编译过程，把程序编译成为机器语言文件，运行时不需要重新翻译，直接使用编译的结果就行了。程序执行效率高，依赖编译器，跨平台性差些。如C、C++、java
- 解释型语言
  程序不需要编译，程序在运行时由解释器翻译成机器语言，每执行一次都要翻译一次。因此效率比较低。比如Python/JavaScript/ Perl /ruby/Shell等都是解释型语言。

> 计算机的本质：输入、运算、输出

- 总结

1. 编译型语言比解释型语言快，但是没有解释性语言跨平台性好。
2. 如果做底层开发或者大型应用程序或者操作系统，一般都用编译型语言。
3. 如果是一些服务器脚本，或一些辅助接口，对速度要求不高，对各个平台有要求的话，选择解释性语言。

**shell语言介绍：**

 shell在计算机中起到什么作用呢？为什么要求shell呢，我们可以看看计算机操作系统的组成： 

  <img src="J:\homework\Linux学习笔记\Linux学习笔记.assets\1600050867343.png" alt="OS分层.png" style="zoom:50%;" /> 

看图之前问大家个问题，两个人在电话聊天：只会说法语的法国人，只会说汉语的你。如何沟通呢？

请个翻译在你两中间

同理，系统内核只知道二进制

如果你想给计算机内核下任务，让其驱动硬件干活，那么有两种选择

1、你学会二进制

2、找个翻译

**shell介绍**

shell就是我们找来的翻译。

shell是一个程序，采用C语言编写，是用户和linux内核沟通的桥梁。它既是一种命令语言，又是一种解释性的编程语言。通过一个图表来查看一下shell的作用。

  ![00_shell.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600050927043.png) 

**bash shell基本特性**

知己知彼方可百战百胜，如何应用Bash shell，我们先看看他的特性，有助于我们快速应用。

- 命令和文件自动补全

  Tab只能补全命令和文件 （RHEL6/Centos6）

- 常见的快捷键—提升操作熟练度

```shell
^c    		# 终止前台运行的程序
^z    		# 将前台运行的程序挂起到后台
^d    		# 退出 等价exit
^l    		# 清屏 
^a |home    # 光标移到命令行的最前端
^e |end   	# 光标移到命令行的后端
^u    		# 删除光标前所有字符
^k    		# 删除光标后所有字符
^r   		# 搜索历史命令
```

**shell脚本介绍**

 简单来说就是将需要完成某个任务所执行的命令按照执行顺序保存到文本中，并给予执行权限。 

- 按照顺序执行。

- 它是解释型的，意味着不需要编译。

- 准确来说：若干命令 + 脚本的基本格式 + 脚本特定语法 + 思想= shell脚本

```shell
# 脚本命令演示
创建一个用户：harry     useradd harry
密码设置为:yunwei.98989 echo "yunwei.98989"|passwd --stdin harry
该用户创建文件夹/tmp/zutuanxue   mkdir /tmp/zutuanxue
该用户创建文件/tmp/zutuanxue/README  touch /tmp/zutuanxue/README
将“hello world“输入到/tmp/zutuanxue/README  echo 'hello world' > /tmp/zutuanxue/README

# 实现代码 01_task.sh
#!/bin/bash

#DESC: this is a test script 
#AUTHOR: Bai Shuming
#RELEASE: 1.0

#main 

#创建用户harry
useradd harry

#设置用户密码 yunwei.98989
echo "yunwei.98989"|passwd --stdin harry

#使用harry创建文件夹，文件，输入文件中内容
su - harry -c "mkdir /tmp/zutuanxue"
su - harry -c "touch /tmp/zutuanxue/README"
su - harry -c "echo 'hello world' > /tmp/zutuanxue/README"
```

什么时候用到脚本？

重复化、复杂化的工作，通过把工作的命令写成脚本，以后仅仅需要执行脚本就能完成这些工作。

1. 自动化分析处理

2. 自动化备份

3. 自动化批量部署安装

4. 等等…

如何学习shell脚本？

尽可能记忆更多的命令

掌握脚本的标准的格式（指定魔法字节、使用标准的执行方式运行脚本）

必须熟悉掌握脚本的基本语法（重点)

学习脚本的秘诀：多看（看懂）——>多模仿（多练）——>多思考

### 9.3 shell脚本语法

shell脚本组成

  ![shell程序组成.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600051087772.png) 

- 脚本命名
  nginx_install.sh 脚本名称 脚本扩展名 .sh
  名字不要太长 26个字节内
- 代码规范：

```shell
1、#!/bin/bash
脚本第一行， #！魔法字符，指定脚本代码执行的程序。即它告诉系统这个脚本需要什么解释器来执行，也就是使用
哪一种Shell

2、# 代表注释，#！特例 

3、以下内容是对脚本的基本信息的描述,大家可以根据实际情况尽可能的写详细一些，方便后续使用者
# Name: 脚本名字
# Desc:描述describe
# Path:存放路径
# Usage:用法
# Update:更新时间
# Author:作者
# Release: 分发版本

下面就是脚本的具体内容
commands
...
```

- 脚本执行方法：

1. 标准脚本执行方法（建议）：

```shell
[root@zutuanxue shell01]# cat 1.sh 
#!/bin/bash
#xxxx
#xxx
#xxx
hostname
date
[root@zutuanxue shell01]# chmod +x 1.sh 
[root@zutuanxue shell01]# ll
total 4
-rwxr-xr-x 1 root root 42 Jul 22 14:40 1.sh
[root@zutuanxueshell01]# /shell/shell01/1.sh 
zutuanxue
Sun Jul 22 14:41:00 CST 2018
[root@zutuanxue shell01]# ./1.sh 
zutuanxue
Sun Jul 22 14:41:30 CST 2018
```

2. 非标准的执行方法（不建议）：

```shell
[root@zutuanxue shell01]# bash 1.sh 
zutuanxue
Sun Jul 22 14:42:51 CST 2018
[root@zutuanxue shell01]# sh 1.sh
zutuanxue
Sun Jul 22 14:43:01 CST 2018
[root@zutuanxue shell01]# 
[root@zutuanxue shell01]# bash -x 1.sh
+ hostname
zutuanxue
+ date
Sun Jul 22 14:43:20 CST 2018

-x:		# 一般用于排错，查看脚本的执行过程
-n:		# 用来查看脚本的语法是否有问题

# 注意：如果脚本没有加可执行权限，不能使用标准的执行方法执行，bash 1.sh

其他：
[root@zutuanxueshell01]# source 2.sh
server
Thu Nov 22 15:45:50 CST 2018
[root@zutuanxue shell01]# . 2.sh
server
Thu Nov 22 15:46:07 CST 2018

source 和 . 表示读取文件，执行文件里的命令
```

3. 命令式脚本执行方法： 

```shell
# 定义命令路径变量  PATH
PATH=$PATH:脚本路径

# 备注：脚本必须给执行权限
```

### 9.4 shell变量

在编程中，我们总有一些数据需要临时存放在内存，以待后续使用时快速读出。先了解一下计算机的存储单位吧。 

```shell
# 计算机的单位:
1B=8bit

1KB=1024B
1MB=1024KB
1GB=1024MB
1TB=1024GB
1PB=1024TB
1EB=1024PB
1ZB=1024EB
...
好了，已经够大了！当然还有YB、BB更大的单位，同样进制也是1024.

1G=1024*1024*1024=1073741824B
```

假如你将一个1B的字符存入内存，如何读出呢？有没有一种大海捞针的感觉啊！我们讨论一下计算机是如何通过让我们人类快速将数据存在内存，如何从内存中读出数据的。我们研究过变量后就明白了。

**变量：变量是编程中最常用的一种临时在内存中存取数据的一种方式。**

 变量存取原理 

```shell
# 关于内存的说明
a、系统启动    内存被按照1B一个单位划分成N块     并且以十六进制为每一个空间编号

b、内存跟踪表记录  使用和未使用的内存的地址编号

c、内存申请    系统从未使用的内存中拿出一个或者一段连续空间  给你使用   同时在内存跟踪表中记录
该地址被占用不在分给别人，同时在系统中建立映射机制   
比如:变量名 STRING1=‘ABC’

name<==>0x5

d、释放内存
从内存跟踪表中将记录删除，下次存数据直接覆盖
```

  ![变量存储.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600685218168.png) 

```
CHAR1(0x3)=A
从图片可以看出，当我们在脚本中定义变量存值的时候，可以从以下方面看到变化：
a、内存占用：如果存的是一个字符则占用1个字节，如果存的是字符串则是字符串的长度加1个字节长度(\0是一个
特殊字符，代表字符串结束)。

b、变量名与内存空间关系：计算机中会将对应的内存空间地址和变量名称绑定在一起，此时代表这段内存空间已经被
程序占用，其他程序不可复用；然后将变量名对应的值存在对应内存地址的空间里。
```

**1. 变量定义**

- 什么时候需要定义变量？

   如果某个内容需要多次使用，并且在代码中重复出现，那么可以用变量代表该内容。这样在修改内容的时候，仅仅需要修改变量的值。
  在代码运作的过程中，可能会把某些命令的执行结果保存起来，后续代码需要使用这些结果，就可以直接使用这个变量。 

- ##### 定义一个变量

  变量格式： 变量名=值

  在shell编程中的变量名和等号之间不能有空格。

```shell
# 变量名命名规则：
    1.命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。
    2.中间不能有空格，可以使用下划线（_）。
    3.不能使用标点符号。
    4.不能使用bash里的关键字（可用help命令查看保留关键字）。

# 定义变量举例：
VAR1=1
age=18 	# 整形
name=‘baism’ # 字符串
score=88.8 # 浮点 
```

 定义变量演示：

```shell
[root@localhost ~]# name="joker"
[root@localhost ~]# school="xiamen"
[root@localhost ~]# age=30
[root@localhost ~]# score=36.8
```

取消变量：

```shell
# 取消当前环境中的变量，如果是变量设置是保存在文件中，下次重启又会恢复
[root@localhost ~]# echo $name
joker
[root@localhost ~]# unset name
```

有类型变量：

```shell
-i 	# 将变量看成整数
-r 	# 使变量只读 readonly,该变量的值无法改变，并且不能为unset
-x 	# 标记变量通过环境导出 export
-a 	# 指定为索引数组（普通数组）；查看普通数组
-A 	# 指定为关联数组；查看关联数组

[root@localhost ~]# declare -i num='asa'
[root@localhost ~]# echo $num
0
[root@localhost ~]# num=100
[root@localhost ~]# echo $num
100
[root@localhost ~]# declare -r num
[root@localhost ~]# unset num
-bash: unset: num: cannot unset: readonly variable

[root@localhost ~]# declare -x
declare -x DISPLAY="localhost:10.0"
declare -x HISTCONTROL="ignoredups"
declare -x HISTSIZE="1000"
declare -x HOME="/root"
declare -x HOSTNAME="localhost.localdomain"
declare -x LANG="en_US.UTF-8"
declare -x LESSOPEN="||/usr/bin/lesspipe.sh %s"
declare -x LOGNAME="root"
```

**2. 变量分类**

系统中的变量根据作用域及生命周期可以分为四类：本地变量、环境变量、全局变量、内置变量。

- 本地变量

 用户自定义的变量，定义在<font color="red">脚本或者当前终端中</font>，脚本执行完毕或终端结束变量失效。 

- 环境变量

   定义在用户<font color="red">家目录下的.bashrc或.bash_profile</font>文件中，用户私有变量，只能本用户使用。 

   查看当前用户的环境变量 **env** 

   查询当前用户的所有变量(临时变量与环境变量) **set** 

示例：将当前便令变成环境变量  <font color="red">export</font>

```shell
# 1.定义一个临时变量
[root@localhost ~]# name=joker
[root@localhost ~]# export name
[root@localhost ~]# env | grep name
name=joker

# 2.定义一个永久生效的变量
vim .bash_profile 或者 ~/.bashrc
name=joker
```

> 关于export说明
> **用户登录时:**
> 1) 用户登录到Linux系统后，系统将启动一个用户shell。在这个shell中，可以使用shell命令或声明变量，也可以创建并运行 shell脚本程序。
>
> **运行脚本时:**
> 2) 运行shell脚本程序时，系统将创建一个子shell。此时，系统中将有两个shell，一个是登录时系统启动的shell，另一个是系统为运行脚本程序创建的shell。当一个脚本程序运行完毕，它的脚本shell将终止，可以返回到执行该脚本之前的shell。
>
> 从这种意义上来说，用户可以有许多 shell，每个shell都是由某个shell（称为父shell）派生的。
> 在子shell中定义的变量只在该子shell内有效。如果在一个shell脚本程序中定义了一个变量，当该脚本程序运行时，这个定义的变量只是该脚本程序内的一个局部变量，其他的shell不能引用它，要使某个变量的值可以在其他shell中被改变，可以使用export命令对已定义的变量进行输出。 
>
> export命令将使系统在创建每一个新的shell时定义这个变量的一个拷贝。这个过程称之为变量输出。

  ![shell父子关系.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1600685550727.png) 

- 全局变量

**使用export**命令将本地变量输出为当前shell中的环境变量；
所有用户及shell都可以使用，可以在/etc/profile /etc/bashrc下永久定义 。

```shell
# 打印全局变量
[root@localhost ~]# export name=joker
[root@localhost ~]# printenv | grep name
name=joker
[root@localhost ~]# su - joker
Last login: Sun Dec  5 11:36:55 CST 2021 on pts/0
[joker@localhost ~]$ printenv | grep name
```

- 内置变量

 系统变量(内置bash中变量) ： shell本身已经固定好了它的名字和作用。

```shell
$?：上一条命令执行后返回的状态，当返回状态值为0时表示执行正常，非0值表示执行异常或出错
 若退出状态值为0，表示命令运行成功
 若退出状态值为127,表示command not found
 若退出状态值为126,表示找到了该命令但无法执行（权限不够）
 若退出状态值为1&2,表示没有那个文件或目录
 
$$：当前所在进程的进程号     echo $$   eg：kill -9 `echo $$`  = exit   退出当前会话

$!：后台运行的最后一个进程号  （当前终端）  # gedit &
!$ 调用最后一条命令历史中的参数
!! 调用最后一条命令历史


$#：脚本后面接的参数的个数
$*：脚本后面所有参数，参数当成一个整体输出，每一个变量参数之间以空格隔开
$@: 脚本后面所有参数，参数是独立的，也是全部输出

$0：当前执行的进程/程序名  echo $0     
$1~$9 位置参数变量
${10}~${n} 扩展位置参数变量  第10个位置变量必须用{}大括号括起来
./1.sh a b c
```

示例：

```shell
[root@localhost opt]# cat 2.sh
#!/bin/bash
#xxxxxxxx
echo "\$0 = $0"
echo "\$# = $#"
echo "\$* = $*"
echo "\$@ = $@"
echo "\$1 = $1"
echo "\$2 = $2"
echo "\$3 = $3"
echo "\$11 = ${11}"
echo "\$12 = ${12}"
[root@localhost opt]# sh 2.sh a b c d e f g h i j k l m n
$0 = 2.sh
$# = 14
$* = a b c d e f g h i j k l m n
$@ = a b c d e f g h i j k l m n
$1 = a
$2 = b
$3 = c
$11 = k
$12 = l
```

$*和$@的区别：
$* ：表示将变量看成一个整体
$@ ：表示变量是独立的

```shell
[root@localhost opt]# sh 3.sh a b c
a
b
c
======我是分割线=======
a b c
```

**变量总结说明：**

本地变量：当前用户自定义的变量。当前进程中有效，其他进程及当前进程的子进程无效。

环境变量：当前进程有效，并且能够被子进程调用。

全局变量：全局所有的用户和程序都能调用，且继承，新建的用户也默认能调用.

内置变量：shell本身已经固定好了它的名字和作用.

| 变量类型 | 作用域                                                    | 生命周期           |
| -------- | --------------------------------------------------------- | ------------------ |
| 本地变量 | 当前shell环境(子shell不能用)                              | 脚本结束或终端结束 |
| 环境变量 | 当前shell或者子shell,文件中，用户私有变量，只能本用户使用 | 当前进程结束       |
| 全局变量 | 所有用户及shell环境                                       | 关机               |
| 内置变量 | 所有用户及shell环境                                       | 关机               |

**3. 变量取值**

```shell
[root@localhost opt]# name=joker
[root@localhost opt]# echo '$name'
$name
[root@localhost opt]# echo "$name"
joker

# 注意
# 变量读取过程中，默认单引号是不解释变量的.比如
[root@localhost opt]# echo '$name'
$name

# 如果必须使用单引号还要读取变量的值可以使用eval命令[重新运算求出参数的内容] 
[root@localhost opt]# eval  echo '$name'
joker
```

**4. 其他变量**

```shell
1）取出一个目录下的目录和文件：dirname和 basename 
2）变量"内容"的删除和替换
一个“%”代表从右往左去掉一个/key/
两个“%%”代表从右往左最大去掉/key/
一个“#”代表从左往右去掉一个/key/
两个“##”代表从左往右最大去掉/key/

[root@localhost opt]# A=/root/Desktop/shell/mem.txt 
[root@localhost opt]# echo $A
/root/Desktop/shell/mem.txt
[root@localhost opt]# dirname $A   # 取出目录
/root/Desktop/shell
[root@localhost opt]# basename $A  # 取出文件
mem.txt

url=www.taobao.com
echo ${#url}      	 # 获取变量的长度
echo ${url#*.}       # 以分隔符.界限  *匹配所有
echo ${url##*.}
echo ${url%.*}
echo ${url%%.*}
```

### 9.5 数据磁盘初始化

**1. 应用场景**

生产环境中的服务器一般会分为系统盘和数据盘两种磁盘，以dell R730举例，该服务器是一个2U的机架式服务器，满载可以挂载14块磁盘[2块在机箱内做系统盘，12块在面板做数据盘]，我们一般的策略是系统盘做raid1，保障系统稳定性12块数据磁盘我们做raid10 或者 raid50，保障数据盘容错的同时还能做到优化IO的效果。

raid磁盘的容量是一定的，线上的数据又是不断增长的，也就是说总有一天会把你的数据磁盘填满，那怎么办？为了解决这个问题，人们想到了LVM[逻辑卷管理系统]，当前数据盘容量不够用的时候，我们可以通过san存储获得网络磁盘，然后将该网络存储动态加入LVM中的卷组后就可以扩大LV了。整个过程采用在线扩容的方式，不会影响线上业务正是基于这个原因，我们又在系统中把raid数据盘在存数据之前做成了LVM磁盘，方便后续的扩容。

<font color="red"> 注意：有数据的磁盘不能再做LVM，因为需要格式化，数据会全部丢失。必须提前布局，否则就得提前准备跑路资金了。 </font>

**2. 案例需求**

```shell
# 给虚拟机添加一块磁盘(以sdb为例)，要求使用脚本对该磁盘分三个区：

 1）主分区 /dev/sdb3 543M 文件系统 ext4 要求开机自动挂载到/data/data1目录

 2) 逻辑分区 /dev/sdb5 2G

 3) 逻辑分区 /dev/sdb6 3G

使用/dev/sdb5 /dev/sdb6 新建卷组vg100，并创建一个PE为16M,容量为2.5G的逻辑卷lv100，
格式化为xfs,默认开机自动挂载到/data/data2目录
```

**3. 案例算法**

 算法：完成一个任务的代码思路。 

```shell
# 脚本思路---算法
1、分区
2、创建逻辑卷
    2.1  创建物理卷
    2.2  创建卷组
    2.3  创建逻辑卷
3、格式化 /dev/sdb3   /dev/vg100/lv100
4、修改/etc/fstab文件
5、挂载分区
6、验证并输出挂载结果
```

**4. 代码实现**

 代码实现的要点：要清楚每一步的步骤，不同的系统可能有细微的差别，一味的复制可不行的，需要提前手动做一下，把步骤捋清楚。 

```shell
# disk_partition.sh

#!/bin/bash
# 
#Author: Bai Shuming
#Created Time: 2019/11/1 21:05
#Release: 
#Description:
#
#给虚拟机添加一块磁盘(以sdb为例)，要求使用脚本对该磁盘分三个区：
#  1）主分区 /dev/sdb3   543M   文件系统 ext4  要求开机自动挂载到/data/data1目录
#  2)   逻辑分区  /dev/sdb5   2G
#  3)   逻辑分区  /dev/sdb6   3G
#使用/dev/sdb5   /dev/sdb6   新建卷组vg100，并创建一个PE为16M,容量为2.5G的逻辑卷lv100，
#格式化为xfs,默认开机自动挂载到/data/data2目录

# 1、分区
fdisk /dev/sdb <<EOF
n
p
3

+543M
n
e
4


n

+2G
n

+3G
w
EOF

# 2、创建逻辑卷
   #2.1 创建物理卷
    pvcreate /dev/sdb5 /dev/sdb6
   #2.2 创建卷组
    vgcreate -s 16M vg100 /dev/sdb{5..6}
   #2.3 创建逻辑卷
    lvcreate -L 2.5G -n lv100 vg100
# 3、格式化
mkfs.ext4 /dev/sdb3
mkfs.xfs /dev/vg100/lv100

# 4、修改/etc/fstab,实现自动挂载
echo  "/dev/sdb3   /data/data1 ext4  defaults   0 0" >> /etc/fstab
echo "/dev/vg100/lv100 /data/data2  xfs   defaults 0 0" >> /etc/fstab

# 5、挂载分区
mkdir -p /data/data{1..2}
mount -a

# 6、验证并输出挂载结果
mount |grep "/dev/sdb3"
test $? -eq 0&&echo "/dev/sdb3 挂载成功" || echo "/dev/sdb3挂载失败"

##注意检索的时候，mount输出中LV的表示方式，或者直接检索挂载点/data/data2也可以。
mount |grep "vg100-lv100"
test $? -eq 0&&echo "/dev/vg100/lv100 挂载成功" || echo "/dev/vg100/lv100挂载失败"
```

### 9.6 shell脚本格式化输出

计算机程序其实就是三步:输入、运算、输出，这个理论也适应于shell编程。

那么计算机是如何将信息按照比较舒服的格式输出到屏幕或者KFC的打印纸上的呢！如果让计算机能够输出一种格式，让人看起来很舒服，那么我们就要学习一下计算机的格式化输出，让计算机程序将信息输出的时候美美哒！让人一目了然看到需要的信息。

**1. shell格式化输出**

 一个赏心悦目的界面是一个程序给用户的第一个映像，好的界面可以让用户更加容易上手使用。windows之所以能被个人用户喜欢就是因为它的界面更加容易和用户交互，只要用户能识别文字，懂得点击鼠标就能操作电脑；而linux之所以无法被广大个人用户使用的瓶颈就是图形界面无法完成所有工作，需要命令配合才可以，这就把非专业用户拒之门外了，想用就必须去学习命令。
我们在使用shell写一个程序的时候，如果想让广大的用户都能使用，都能快速上手，那么好的交互界面就太重要了。我们可以使用多种方法开发好的、易交互的界面，常用的工具有：dialog、echo、printf等命令。
本节课主要给大家介绍一个最简单易用的命令：echo 

**echo命令**

**功能：将内容输出到默认显示设备**

**应用场景：需要计算机程序输出的地方**

echo命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。 功能说明:显示文字。 

```shell
# 语法:echo [-ne][字符串]

# 补充说明:
1、echo会将输入的字符串送往标准输出。
2、输出的字符串间以空白字符隔开,并在最后加上换行号。

# OPTIONS：
-n	不要在最后自动换行
-e	若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出:

转义字符
\a	# 发出警告声;
\b	# 删除前一个字符;
\t	# 插入tab;
\n	# 换行且光标移至行首;

\c	# 最后不加上换行符号;
\f	# 换行但光标仍旧停留在原来的位置;
\r	# 光标移至行首，但不换行;
\v	# 与\f相同;
\		# 插入\字符;
\0nnn	# 打印nnn(八进制)所代表的ASCII字符;  备注：数字0  不要理解成字母o
\xNN  	# 打印NN(十六进制)所代表的ASCII字符;

-–help		# 显示帮助
-–version	# 显示版本信息
```

**输出颜色字体**

脚本中echo显示内容带颜色显示，echo显示带颜色，**需要使用参数-e** 

```shell
# 格式如下：
echo -e "\033[字背景颜色；文字颜色m字符串\033[0m"

示例：
echo -e "\033[41;36m something here \033[0m"
```

其中41的位置代表底色， 36m的位置是代表字的颜色

1、字背景颜色和文字颜色之间是英文的

2、文字颜色后面有个m

3、字符串前后可以没有空格，如果有的话，输出也是同样有空格

```shell
# 下面是相应的字和背景颜色，可以自己来尝试找出不同颜色搭配
　　echo -e "\033[31m 红色字 \033[0m
　　echo -e "\033[34m 黄色字 \033[0m
　　echo -e "\033[41;33m 红底黄字 \033[0m
　　echo -e "\033[41;37m 红底白字 \033[0m
　　
# 字颜色：30—–37
　　echo -e "\033[30m 黑色字 \033[0m"
　　echo -e "\033[31m 红色字 \033[0m"
　　echo -e "\033[32m 绿色字 \033[0m"
　　echo -e "\033[33m 黄色字 \033[0m"
　　echo -e "\033[34m 蓝色字 \033[0m"
　　echo -e "\033[35m 紫色字 \033[0m"
　　echo -e "\033[36m 天蓝字 \033[0m"
　　echo -e "\033[37m 白色字 \033[0m"

　　
# 字背景颜色范围：40—–47
　　echo -e "\033[40;37m 黑底白字 \033[0m"
　　echo -e "\033[41;37m 红底白字 \033[0m"
　　echo -e "\033[42;37m 绿底白字 \033[0m"
　　echo -e "\033[43;37m 黄底白字 \033[0m"
　　echo -e "\033[44;37m 蓝底白字 \033[0m"
　　echo -e "\033[45;37m 紫底白字 \033[0m"
　　echo -e "\033[46;37m 天蓝底白字 \033[0m"
　　echo -e "\033[47;30m 白底黑字 \033[0m"
　　
# 最后面控制选项说明
　　\033[0m  # 关闭所有属性
　　\033[1m  # 设置高亮度
　　\033[4m  # 下划线
　　\033[5m  # 闪烁
　　\033[7m  # 反显
　　\033[8m  # 消隐

　　\033[30m — \33[37m 

# 设置前景色
　　\033[40m — \33[47m  # 设置背景色
　　
　　
　　\033[nA 		# 光标上移n行
　　\033[nB 		# 光标下移n行
　　\033[nC 		# 光标右移n行
　　\033[nD 		# 光标左移n行
　　\033[y;xH		# 设置光标位置
　　\033[2J 		# 清屏
　　\033[K 		# 清除从光标到行尾的内容
　　\33[s 		# 保存光标位置
　　\033[u 		# 恢复光标位置
　　\033[?25l 	# 隐藏光标
　　\033[?25h 	# 显示光标
　　
# 用法例子  光标下移三行　　
[root@localhost opt]# echo -e "\033[0m today is fine \033[3B"
 today is fine



```

示例：

- echo输出缩进问题
- 字体颜色输出

```shell
# [root@localhost opt]# cat fruite.sh
#!/bin/bash
echo -e "\t\t\033[32m Fruits List \033[0m\n"
echo -e "\033[31mFruit\033[0m\t\t\033[31m Price\033[0m\t\t\033[31m Weight\033[0m"
echo -e "\033[34m1)Apple \t￥10.00\t\t1KG\033[0m"
echo -e "\033[34m1)Banana\t￥9.00\t\t1KG\033[0m"
echo -e "\033[34m1)Orange\t￥15.20\t\t1KG\033[0m"
```

### 9.7 脚本用户交互

**read命令**

功能：默认接受键盘的输入，回车符代表输入结束
应用场景：人机交互
命令选项 ：

> -p打印信息
> -t限定时间
> -s不回显
> -n输入字符个数

示例：

**案例需求：**
写一个系统用户交互登录界面脚本，仿linux文本界面登录
**案例要点：**
了解linux文本界面登陆所需要的输出信息及界面布局 

```shell
[root@localhost opt]# cat login.sh
#!/bin/sash

IP=`ifconfig ens33|egrep -w "inet"|awk '{print $2}'`
clear
echo "CentOS Linux 8 (Core)"
echo -e "Kernel `uname -r` on an `uname -m`\n"
echo -e "Web console: https://localhost:9090/ or https://$IP:9090/ \n"

# 输入用户名
echo -n "$HOSTNAME login: "
read account
# 输入密码
read -p "password: " -s -t 30 pwd

```

### 9.8 shell运算详解

**计算机编程就是三大步：输入、运算、输出**

那么计算机运算有哪些呢，计算机能做哪些运算呢？

**我们来看看常见的计算机运算**

#### 9.8.1 赋值运算

赋值运算 **=**

```shell
a=10
name="joker"
# 注意：字符串必须用引号引起来
```

#### 9.8.2 四则运算

> 运算符于命令
>
> 四则运算： + - * /
>
> 扩展：% **
>
> 运算命令：
>
> - 整形运算
>   - expr
>   - let
>   - (())
>   - bc
>
> - 浮点运算
>   - bc

**1. 整形运算**

 expr 命令：只能做整数运算，格式比较古板，注意空格 

```shell
[root@localhost opt]# expr 1 + 2
3
[root@localhost opt]# expr 5 - 2
3
[root@localhost opt]# expr 5 * 2
expr: syntax error
[root@localhost opt]# expr 5 \* 2
10
[root@localhost opt]# expr 5 / 2
2
```

 let命令：只能做整数运算，且运算元素必须是变量，无法直接对整数做运算 

```shell
[root@localhost opt]# let a=100+3;echo $a
103
[root@localhost opt]# let a=100-3;echo $a
97
[root@localhost opt]# let a=100*3;echo $a
300
[root@localhost opt]# let a=100/3;echo $a
33
[root@localhost opt]# let a=100%3;echo $a
1
[root@localhost opt]# let a=100**3;echo $a
1000000
```

 双小圆括号运算，在shell中(( ))也可以用来做数学运算 

```shell
[root@localhost opt]# echo $((100+3))
103
[root@localhost opt]# echo $((100-3))
97
[root@localhost opt]# echo $((100/3))
33
[root@localhost opt]# echo $((100*3))
300
```

**2. 浮点运算**

 浮点运算是采用的命令组合的方式来实现的 echo “scale=N;表达式”|bc 

```shell
[root@localhost opt]# echo "scale=2;100+3"|bc
103
[root@localhost opt]# echo "scale=2;100-3"|bc
97
[root@localhost opt]# echo "scale=2;100/3"|bc
33.33
[root@localhost opt]# echo "scale=2;100*3"|bc
300
```

示例一：实现一个四则运算的计算器

```shell
[root@localhost opt]# cat calculator.sh
#!/bin/bash
echo "$1 $2 $3"|bc

[root@localhost opt]# sh calculator.sh 1+2+3
6
```

示例二：内存使用率统计，要求打印内存使用率

案例思考：

- 内存总量 获得方式是什么 free top /proc/meminfo
- 内存使用量
- 内存使用率公式 使用量/总量*100%

```shell
[root@localhost opt]# cat memory_use.sh
#!/bin/bash

# 获得内存总量
memory_total=`free -m|grep -i "mem"|tr -s " "|cut -d " " -f2`
# 获得内存适用量
memory_use=`free -m|grep -i "mem"|tr -s " "|cut -d " " -f3`

echo "内存使用率：`echo "scale=2;$memory_use*100/$memory_total"|bc`%"

[root@localhost opt]# sh memory_use.sh
内存使用率：10.55%
```

#### 9.8.3 比较运算

 计算机除了算术和赋值运算外，还有比较运算，比如说比较两个数的关系，比较两个字符串的关系【用户登录系统】等。

**整形比比较运算**

```shell
# 运算符解释：

# 精确比较
	-eq   # 等于 equal

	-gt   # 大于

	-lt   # 小于

# 模糊比较
	-ge   # 大于或等于

	-le   # 小于或等于

	-ne   # 不等于
```

 通过test命令比较两个整数关系

```shell
[root@localhost opt]# test 100 -gt 300;echo $?
1
[root@localhost opt]# test 100 -lt 300;echo $?
0
[root@localhost opt]# test 100 -eq 300;echo $?
1
[root@localhost opt]# test 100 -ge 300;echo $?
1
[root@localhost opt]# test 100 -le 300;echo $?
0
[root@localhost opt]# test 100 -ne 300;echo $?
0
# 备注：linux命令test只能比较两个整数的关系，不会返回结果，需要通过$?才能看到结果
```

示例一：写一个脚本实现对两个整数关系的判断

```shell
[root@localhost opt]# cat yunsuan.sh
#!/bin/bash

if [ $1 -gt $2 ];then
  echo "$1 > $2 "
elif [ $1 -eq $2 ];then
  echo "$1 = $2"
else
  echo "$1 < $2"
fi
[root@localhost opt]# sh yunsuan.sh 3 6
3 < 6
[root@localhost opt]# sh yunsuan.sh 7 6
7 > 6
```

示例二：判断两个浮点数的关系

> 默认情况下shell是不能判断浮点的，那么在linux中又避免不了需要进行浮点运算，那怎么解决
> 解决思路如下：
> 1）两个数据同时放大到整数倍
> 2）处理掉小数点位，保留整数位
> 3）进行整形判断

```shell
#!/bin/bash
# 
#Author: www.zutuanxue.com
#
#Release: 
#Description:判断两位小数点的关系

#1、交互或者外传参的方式获得两个整数
#$1 $2
[ $# -lt 2 ]&&echo "need two args"&&exit 1

#采用外传参的方式接收数据并放大100倍,并处理为整数
num1=`echo "scale=2;$1*100"|bc|cut -d "." -f1`
num2=`echo "scale=2;$2*100"|bc|cut -d "." -f1`

#2、比较运算
if [ $num1 -gt $num2 ];then
   #3、输出结果
   echo "$1 > $2"
elif [ $num1 -lt $num2 ];then
   echo "$1 < $2"
else
   echo "$1 = $2"
fi
[root@localhost opt]# sh yunsuan2.sh 2.5 6.2
2.5 < 6.2
[root@localhost opt]# sh yunsuan2.sh 2.5 0
2.5 > 0
[root@localhost opt]# sh yunsuan2.sh 3
need two args
```

**字符串比较运算**

```shell
# 运算符解释，注意字符串一定别忘了使用引号引起来
  ==   # 等于   
  !=   # 不等于
  -n   # 检查字符串的长度是否大于0  
  -z   # 检查字符串的长度是否为0
```

```shell
[root@localhost opt]# test 'root' == 'root';echo $?
0
[root@localhost opt]# test 'root' == 'roo1t';echo $?
1
[root@localhost opt]# test 'root' != 'roo1t';echo $?
0
[root@localhost opt]# test -n "$a";echo $?
0
[root@localhost opt]# test -z "$a";echo $?
1
```

示例： 模拟一个linux文本界面登陆程序，要求账号密码验证成功进入系统，账号密码验证失败退回登陆界面

```shell
[root@localhost opt]# cat login1.sh
#!/bin/bash
default_account='root'
default_pw='123456'

#1、清屏
clear
#2、输出提示信息
echo "CentOS Linux 7 (Core)"
echo -e "Kernel `uname -r` on an `uname -m`\n"

#3、交互输入登陆名
echo -n "$HOSTNAME login: "
read account

#4、交互输入密码
read -s -t30 -p "Password: " pw

#5、判断用户输入是否正确
if [ "$default_account" == "$account" ] && [ "$default_pw" == "$pw" ];then
   clear
   echo -e "\nwelcome to root"
else
   echo  "用户名或密码错误..."
   #输入错误，再次调用本脚本
   sh $0
fi
```

#### 9.8.4 逻辑运算

完成一个任务中需要多个条件都满足或者多个条件中只要满足一个即可，那么这就是我们的逻辑运算。
通过多个条件判断结果，才能得出结论。

**逻辑运算符**

-  逻辑与运算 &&
-  逻辑或运算 ||
-  逻辑非运算 !

> ```shell
> # 逻辑运算注意事项：
>     逻辑与 或 运算都需要两个或以上条件
>     逻辑非运算只能一个条件。
>     口诀:     逻辑与运算               真真为真 真假为假   假假为假
>              逻辑或运算               真真为真 真假为真   假假为假
>              逻辑非运算               非假为真   非真为假
>              
>              
> # 逻辑与或的短路运算
> # 逻辑与中靠前的条件中出现了假，后面的就不在判断了，因为已经是假的了
> # 逻辑或中靠前的条件中出现了真，就不在往后判断了，结果已经为真了
> ```

示例：

```shell
[root@localhost opt]# cat login2.sh
#!/bin/bash
default_account='root'
default_pw='123456'


#1、清屏
clear
#2、输出提示信息
echo "CentOS Linux 7 (Core)"
echo -e "Kernel `uname -r` on an `uname -m`\n"

#3、交互输入登陆名
echo -n "$HOSTNAME login: "
read account

#4、交互输入密码
read -s -t30 -n6 -p "Password: " pw

#5、判断用户输入是否正确
[ "$default_account" == "$account" ] && [ "$default_pw" == "$pw" ] && echo yew || echo no
```

**文件判断**

linux的设计思路：一切皆文件，对文件系统的操作其实可以狭隘的理解为对文件的操作。如果希望对文件类型和权限或者两个文件做新旧或者是否同一个文件进行判断。 

命令功能：检测文件类型和比较运算

命令用法： test [命令选项] 表达式

命令选项：

```shell
-d   # 检查文件是否存在且为目录
-e   # 检查文件是否存在
-f   # 检查文件是否存在且为文件
-r   # 检查文件是否存在且可读
-s   # 检查文件是否存在且不为空
-w   # 检查文件是否存在且可写
-x   # 检查文件是否存在且可执行
-O   # 检查文件是否存在并且被当前用户拥有
-G   # 检查文件是否存在并且默认组为当前用户组
-nt  # file1 -nt file2  检查file1是否比file2新
-ot  # file1 -ot file2  检查file1是否比file2旧     
-ef  # file1 -ef file2  检查file1是否与file2是同一个文件，判定依据的是i节点

# 以上只列出部分命令选项，详细的可以通过:man test获得。
```

示例：

```shell
[root@localhost opt]# test -f /etc/;echo $?
1
[root@localhost opt]# test -f /etc/passwd;echo $?
0
[root@localhost opt]# test -d /etc/;echo $?
0
[joker@localhost ~]$ test -w /etc/passwd;echo $?
1
[root@localhost opt]# test -w /etc/passwd; echo $?
0
```

### 9.9 shell数组详解

数组可以让用户一次赋予多个值，需要读取数据时只需通过索引调用就可以方便读出了。

普通数组：只能使用整数作为数组索引(元素的索引)
关联数组：可以使用字符串作为数组索引(元素的索引)

#### 9.9.1 普通数组

**数组定义**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

> 数组名称=(元素1 元素2 元素3 ...)

**数组赋值方式**

- 一次附一个值

```powershell
变量名=变量值
array[0]=v1
array[1]=v2
array[3]=v3
```

- 一次赋多个值

```powershell
array=(var1 var2 var3 var4)
array1=(`cat /etc/passwd`)			# 将文件中每一行赋值给array1数组
array2=(`ls /root`)
array3=(harry amy jack "Miss zhang")
array4=(1 2 3 4 "hello world" [10]=linux
```

**数组取值**

取值方式: 

> ${数组名称[索引]} 

示例：

```powershell
${array[i]}  # i表示元素的索引
使用@ 或 * 可以获取数组中的所有元素：
echo ${array[0]}		# 获取第一个元素
echo ${array[*]}		# 获取数组里的所有元素
echo ${#array[*]}		# 获取数组里所有元素个数
echo ${!array[@]}    	# 获取数组元素的索引索引
echo ${array[@]:1:2}    # 访问指定的元素；1代表从索引为1的元素开始获取；2代表获取后面几个元素
```

```powershell
[root@localhost ~]# sh array.sh
var4
root:x:0:0:root:/root:/bin/bash
anaconda-ks.cfg
Miss zhang
linux
[root@localhost ~]# cat array.sh
#!/bin/bash

array=(var1 var2 var3 var4)
array1=(`cat /etc/passwd`)
array2=(`ls /root`)
array3=(harry amy jack "Miss zhang")
array4=(1 2 3 4 "hello world" [10]=linux)

echo ${array[3]}
echo ${array1[0]}
echo ${array2[0]}
echo ${array3[3]}
echo ${array4[10]}

```

#### 9.9.2 关联数组

**1. 定义管理数组**

 关联数组使用首先需要申明该数组为关联数组，申明方式： <font color="red">declare -A 数组名称 </font>

```powershell
# 首先声明关联数组
declare -A asso_array1
declare -A asso_array2
declare -A asso_array3
```

**2. 关联数组赋值**

- 一次赋一个值

```powershell
# 数组名[索引]=变量值
[root@localhost opt]# asso_array1[linux]=one
[root@localhost opt]# asso_array1[java]=two
[root@localhost opt]# asso_array1[php]=three
```

- 一次附多个值

```powershell
[root@localhost opt]# asso_array2=([name1]=harry [name2]=jack [name3]=amy [name4]="Miss zhang")
```

- 查看关联数组

```powershell
[root@localhost opt]# declare -A
declare -A asso_array1='([php]="three" [java]="two" [linux]="one" )'
declare -A asso_array2='([name3]="amy" [name2]="jack" [name1]="harry" [name4]="Miss zhang" )'
```

示例：

```shell
[root@localhost ~]# cat array2.sh
#!/bin/bash

declare -A asso_array1
declare -A asso_array2

asso_array1[linux]=one
asso_array1[java]=two
asso_array1[php]=three

asso_array2=([name1]=harry [name2]=jack [name3]=amy [name4]="Miss zhang")

echo ${asso_array1[linux]}	# 取索引linux的值
echo ${asso_array1[php]}	# 取索引php的值
echo ${asso_array1[*]}		# 取array1所有的值
echo ${!asso_array1[*]}		# 取array1所有的索引
echo ${#asso_array1[*]}		# 取array1的数量
echo ${#asso_array2[*]}		# 取array2的数量
echo ${!asso_array2[*]}		# 取array2所有的索引

[root@localhost ~]# sh array2.sh
one
three
three two one
php java linux
3
4

```

### 9.10 shell流程判断

#### 9.10.1 if介绍

如何写一个高可用性的脚本，赋予脚本智能化，赋予脚本执行逻辑。

比如：nginx安装脚本中

- configure执行成功在执行make,
- make执行成功在执行make install
- 上一步错误就不需要执行后面的代码了。

answer: 加入判断

只要你想在代码中判断一下的时候就第一时间想到if就行了，适用于99%的语言。

当我们在写程序的时候，时常对上一步执行是否成功如何判断苦恼，当我们今天学习了if就可以解决你的苦恼。if语句在我们程序中就是用来做判断的，以后大家不管学习什么语言，以后只要涉及到判断的部分，大家就可以直接拿if来使用，不同的语言之间的if只是语法不同，原理是相同的。

**1. 单if语法**

 适用范围：只需要一步判断，条件返回真干什么。 

> ```powershell
> # 格式
> if [ condition ]           condition 值为 true or false
>    then                    条件为真的时候执行
>       commands             代码块 一行或者多行代码
> fi			   			   语句的结束
> 
> 注：格式非常重要，条件判断一定要注意前后空格
> ```

  ![流程判断11.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1601016825298.png) 

示例：

```powershell
[joker@localhost opt]$ sh if-1.sh
ERROR: need to be root so that!
[joker@localhost opt]$ cat if-1.sh
#!/bin/bash

if [ $USER != "root" ]
    then
        echo "ERROR: need to be root so that!"
   
```

**2. if...else语句**

 适用范围：两步判断，条件为真干什么，条件为假干什么。 

> ```powershell
> if [ condition ]     
>      then           		条件为真
>           commands1     	真  要执行代码块
> else                		条件为假
>           commands2     	假   要执行的代码块
> fi  
> ```

  ![流程判断2.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1601016994805.png) 

示例：

```powershell
[root@localhost opt]# sh if-2.sh
hey admin
[root@localhost opt]# cat if-2.sh
#!/bin/bash

if [ $USER == "root" ]
    then
        echo "hey admin"
else
        echo "hey guest"
fi

```

**3. if...elif...else语句**

 适用范围：多于两个以上的判断结果，也就是多于一个以上的判断条件。 

> ```powershell
> if [ condition 1]     满足第一个条件
>      then          	  真
>             command1  执行command1代码块
> elif [ condition 2]   满足第二个条件
>      then             真
>             commands2 执行command2代码块
>   .......
> else      			  如果条件都不满足
>             commandsX 执行commandX代码块
> fi 					   结束判断
> ```

  ![流程判断3.png](J:\homework\Linux学习笔记\Linux学习笔记.assets\1601017300715.png) 

案例需求：
判断内存的使用率
60以下 ok
60以上 黄色警告
70以上 橙色严重警告
80以上 红色警告 

```powershell
[root@localhost opt]# cat if-3.sh
#!bin/bash
if [ $1 -gt 80 ]
then
    echo -e "\033[31m 警告 \033[0m"
elif [ $1 -gt 70 ]
then
    echo -e "\033[35m 警告 \033[0m"
elif [ $1 -gt 60  ]
then
    echo -e "\033[33m 警告 \033[0m"
else
    echo -e "\033[32m 警告 \033[0m"
fi
```

#### 9.10.2 if语句与shell运算

**1. 与文件是否存在的运算**

```powershell
-e	 # 是否存在，不管是文件还是目录，只要存在，条件就成立
-f	 # 是否为普通文件
-d	 # 是否为目录
-S	 # socket
-p	 # pipe
-c	 # character
-b	 # block
-L	 # 软link
```

- 文件权限相关的判断

```powershell
-r	 # 当前用户对其是否可读
-w	 # 当前用户对其是否可写
-x	 # 当前用户对其是否可执行
-u	 # 是否有suid
-g	 # 是否sgid
-k	 # 是否有t位
```

- 两个文件的比较判断

```shell
file1 -nt  file2	# 比较file1是否比file2新	
file1 -ot  file2 	# 比较file1是否比file2旧
file1 -ef  file2	# 比较是否为同一个文件，或者用于判断硬连接，是否指向同一个inode
```

- 整数之间的判断

```powershell
-eq	 # 相等
-ne	 # 不等
-gt	 # 大于
-lt	 # 小于
-ge  # 大于等于
-le	 # 小于等于

# 浮点比较运算
比较两个浮点数字的大小
给脚本两个浮点数字，返回他俩的关系

# 浮点比较思路
1、获得两个浮点数字
2、处理为整形
3、比较
4、输出
```

- 字符之间的判断

```powershell
-z  					# 是否为空字符串   	 字符串长度为0，就成立
-n  					# 是否为非空字符串    	只要字符串非空，就是成立
string1 == string2 		# 是否相等
string1 != string2 		# 不等
```

示例：字符之间的判断

```powershell
[root@localhost opt]# cat if-5.sh
#!/bin/bash

read -p "username: " myuser
read -p "password: " mypass

if [ -z "$myuser" ];then
    echo "用户名不能为空"
    exit 1

elif [ -n "$mypass" ];then
    if [ $myuser == "root" ] && [ $mypass == "123456" ];then
        echo "welcome $root"
    else
        echo "用户名或密码错误"
        exit 1
    fi
else
    echo "密码不能为空"
    exit 1
fi
```

#### 9.10.3 if高级用法

**1. 条件符号使用双圆括号，可以在条件中植入数学表达式 if (())**

```shell
[root@localhost opt]# cat if-6.sh
#!/bin/bash

if (( (5+5)*5/5+10 > 10 ));then
    echo "yes"
else
    echo "no"
fi
```

**2. 使用双方括号,可以在条件中使用通配符**

 通过代码看下 ，为字符串提供高级功能，模式匹配 r* 匹配r开头的字符串 

```powershell
[root@localhost opt]# cat if-7.sh
#!/bin/bash

for var in ab ac bc aa dd ee
    do
        if [[ $var == a* ]];then
            echo "$var"
        fi
done
```

#### 9.10.4 if简写

省去了关键字，条件为真采用<font color="red">&& </font>符号链接命令块，条件为假采用<font color="red">||</font>链接命令块

简写if一般用在简单的判断中

```powershell
[root@localhost opt]# cat if-8.sh
#!/bin/bash

#if [ ! -d /tmp/joker ];then
#    mkdir /tmp/joker
#fi

简写
[ ! -d /tmp/joker ] && mkdir /tmp/joker

#if [ $USER == "root" ];then
#    echo "hello $USER"
#else
#    echo "hello guest"
#fi

简写
[ $USER == "root" ] && echo "hello $USER" || echo "hello guest"
```

### 9.11 shell流程控制-for

#### 9.11.1 for介绍

脚本在执行任务的时候，总会遇到需要循环执行的时候，比如说我们需要脚本每隔五分钟执行一次ping的操作，除了计划任务，我们还可以使用脚本来完成，那么我们就用到了循环语句。 

**1. for基本语法**

列表for循环：用于将一组命令执行已知的次数，下面给出了for循环语句的基本格式：

```powershell
for variable_name in {list}
     do
          command 
          command
          …
     done
或者
for variable in a b c
     do
         command
         command
         -
     done
```

**2. for条件应用**

 for条件不同的赋值方式 

```powershell
# 1.赋值来自一个范围
for var in {1..10}
do 
	echo $var
done

# 2.直接赋值
for var in 1 2 3 4 5
do 
	echo $var
done

# 3.赋值来自命令
for var in `seq  10`
do 
	echo $var
done
```

示例：

```powershell
[root@localhost opt]# cat for-1.sh
#!/bin/bash

# 1.从1-10正序循环
for var in $(seq 10)
do
    echo $var
done

# 2.从1-10，步长为2循环
for var in {1..10..2}
do
    echo $var
done

# 3.从10-1倒叙循环
for var in {10..1}
do
    echo $var
done

# 4..从10-1倒叙，步长为2循环
for var in {10..1..2}
do
    echo $var
done

# 5.从10-1倒叙，步长为3循环
for var in `seq 10 -3 1`
    do
            echo $var
done
```

**3. 不带列表循环**

 不带列表的for循环执行时由<font color="red">用户指定参数和参数的个数</font>，下面给出了不带列表的for循环的基本格式： 

```powershell
[root@localhost opt]# cat for-2.sh
#!/bin/bash

for var in $inter
do
    echo $var
done

echo "脚本后面有$#个参数"
```

**4. for C语言格式**

**1. 类C风格的for循环**

expr1：定义变量并赋初值   变量初始值 

expr2：决定是否进行循环（条件）  变量的条件 

expr3：决定循环变量如何改变,决定循环什么时候退出 自增或自减运算 

```powershell
[root@localhost opt]# cat for-3.sh
#!/bin/bash
for ((i=1;i<=10;i++));do
    echo $i
done

```

多变量用法

```powershell
[root@localhost opt]# cat for-3.sh
#!/bin/bash
for ((A=1,B=10;A<B,B>A;A++,B--));do
    echo $A
    echo $B
done

```

#### 9.11.2 循环控制

**1. bread语句**

作用：跳出当前循环，执行后续的代码

示例： 循环打印输出数字1到9，当执行输出到5时终止循环。 

```powershell
[root@localhost opt]# cat for-4.sh
#!/bin/bash

for var in $(seq 10);do
    echo $var
    if [ $var -eq 5 ];then
        break
    fi
done
echo "执行完毕"
```

**2. continue语句**

作用： 结束本次循环，继续执行下一次循环

示例： 循环打印输出数字1到9，当执行输出到5时跳过本次循环。 

```powershell
[root@localhost opt]# cat for-5.sh
#!/bin/bash

for var in `seq 10`;do
    if [ $var -eq 5 ];then
        continue
    fi
        echo $var
done
echo "执行完成"
```

**3. sleep语句**

 作用： 外部传参到循环时，参数管理命令 ，使位置参数向左移动，默认移动1位，可以使用shift 2 传参要是N的整数倍。

示例：通过外部传参的方式向脚本内的循环传递参数，要求打印每次循环使用的参数。

```powershell
[root@localhost opt]# cat for-6.sh
#!/bin/bash

# 1.判断外传参的数量
[ $# -lt 3 ] && echo "最少输入三个参数" $0 "$1 $2 $3.." && exit 1
count=$#

# 2.通过shift左移参数输出
# 3.使位置参数向左移动，默认移动1位，可以使用shift 2 传参要是N的整数倍
for (( i=1;i<=$count;i++ ));do
    echo "参数数量$#"
    echo "当前$1的数值是: $1"
    shift 2
    sleep 1
done
echo "执行完毕"
```

**4. 脚本退出命令**

 作用：退出程序并释放占用的系统资源 

示例： 循环输出数字1-9，当循环到5时退出脚本。 

```powershell
[root@localhost opt]# cat for-7.sh
#!/bin/bash
for i in `seq 1 9`;do
    echo $i
    if [ $i -eq 5 ];then
        exit 1
    fi
done
```

#### 9.11.3 for嵌套

**1. for嵌套if **

示例： 输出1-9，当输出5时停止输出 

```pow
[root@localhost opt]# cat for-8.sh
#!/bin/bash

for ((i=1;i<=10;i++));do
    echo $i
    [ $i -eq 5 ] && break
done

```

**2.  for嵌套for** 

示例： 打印99乘法表 

```powershell
[root@localhost opt]# cat for-9.sh
#!/bin/bash
for ((a=1;a<=9;a++));do
    for ((b=1;b<=$a;b++));do
        echo -e -n "$b*$a=$(($a*$b))\t"
    done
    echo
done

```

#### 9.11.4 for与数组

示例： 使用for循环遍历读出数组 

```powershell
[root@localhost opt]# cat for-10.sh
#!/bin/bash
name=('tom' 'jarry' 'harry' 'barry')
for i in 0 1 2 3
  do
      echo ${name[$i]}
 done

```

示例：使用for循环进行数组存值 

```shell
[root@localhost opt]# cat for-11.sh
#!/bin/bash

for i in `seq 1 10`;do
    read -p "name: " name[$i]
done

```

### 9.12 shell流程控制-while

#### 9.12.1 while介绍

**特点：**条件为真就进入循环；条件为假就退出循环，一般应用在未知循环次数的环境。 

**1. 基本语法**

```powershell
while [ 表达式 ]
	do
		command...
	done
	
while  [ 1 -eq 1 ] 或者 (( 1 > 2 ))
  do
     command
     command
     ...
 done
```

示例： 使用for循环和while循环分别循环打印数组1-5 

```powershell
#!/bin/bash

# 1.for循环
for i in $(seq 1 5);do
    echo $i
done

# 2.while循环
num=1
while [ $num -le 5 ];do
    echo $num
    let num++
done

```

**备注：** 知道循环次数就可以用for，比如说一天需要循环24次；如果不知道代码要循环多少次，那就用while，比如我们作业中要求写的猜数字，每个人猜对一个数字的次数都是不能固定的，也是未知的。 

#### 9.12.2 while与shell运算

**1. 比较运算**

示例：循环交互输入一个小写字母，按Q退出循环

```powershell
[root@localhost opt]# cat for-13.sh
#!/bin/bash

read -p "输入一个小写字母，按Q退出：" chose
while [ $chose !=  "Q" ];do
    echo "你输入的是：$chose"
    read -p "输入一个小写字母，按Q退出：" chose
done

```

**2. 逻辑运算**

示例： 使用循环语句帮助丈母娘批量选择女婿

```powershell
[root@localhost opt]# cat for-14.sh
#!/bin/bash
read -p "多少钱：" money
read -p "几辆车：" car
read -p "几套房：" house

while [[ $money -lt 10000 || $car -lt 2 || $house -lt 2 ]];do
    echo "下一个"
    read -p "多少钱：" money
    read -p "几辆车：" car
    read -p "几套房：" house
done
echo "搞定！
```

**3. 文件类型判断**

示例： 使用循环判断/tmp/xxx目录下的文件，如果不是文件类型的打印字符串"目录" 

```powershell
[root@localhost opt]# cat for-15.sh
#!/bin/bash
while [ ! -f /tmp/abc ];do
    echo "目录不存在"
    sleep 1
done

```

**4. 特殊条件**

 while语句中可以使用特殊条件来进行循环 ：

- 符号":" 条件代表真，适用与无限循环
- 字符串 “true” 条件代表真，适用与无限循环
- 字符串 "false"条件代表假

```powershell
[root@localhost opt]# cat while-4.sh
#!/bin/bash

# 1. 符号":" 条件代表真，适用与无限循环
while :;do
    echo haha
    sleep 1
done

# 2.字符串 “true” 条件代表真，适用与无限循环
while true;do
    echo haha
    sleep 1
done

# 3.字符串 "false"条件代表假

while 为 false 字符串代表假，在while中不会开始循环
```

#### 9.12.3 while与循环控制语句

**1. sleep语句**

```powershell
[root@localhost opt]# cat while-5.sh
#!/bin/bash
time=9

while [ $time -ge 0 ];do
     echo -n -e  "\b$time"
     let time--
     sleep 1
done

```

**2. break语句**

```powershell
[root@localhost opt]# cat while-6.sh
#!/bin/bash

num=1

while [ $num -lt 10 ];do
    echo $num
    if [ $num -eq 5 ];then
        break
    fi
    let num++
done

```

**3. continue语句**

```powershell
[root@localhost opt]# cat while-7.sh
#!/bin/bash

num=0
while [ $num -lt 9 ];do
    let num++
    if [ $num -eq 5 ];then
        continue
    fi
    echo $num
done

```

#### 9.12.4 while嵌套其他语句

**1. while嵌套if**

```powershell
[root@localhost opt]# cat while-8.sh
#!/bin/bash

num=1

while [ $num -lt 10 ];do
    echo $num
    if [ $num -eq 5 ];then
        break
    fi
    let num++
done

```

**2. while嵌套for**

```powershell
[root@localhost opt]# cat while-9.sh
#!/bin/bash

A=1

while [ $A -lt 10 ];do
    for ((b=1;b<=$A;b++));do
        echo -n -e "$b*$A=$((A*b)) \t"
    done
    echo
    let A++
done

```

**3.while嵌套while**

```powershell
[root@localhost opt]# cat while-10.sh
#!/bin/bash
a=1
while [ $a -lt 10 ];do
    b=1
    while [ $b -le $a ];do
        echo -n -e "$a*$b=$((a*b)) \t"
        let b++
    done
    echo
    let a++
done

```

### 9.13 shell流程控制-untile

系统中还有一个类似while的循环语句，不同于while的是，当条件为假时开始until循环。 

特点：条件为假就进入循环，条件为真就退出循环 。

```powershell
until expression   [ 1 -eq 1 ]  (( 1 >= 1 ))
	do
		command
		command
		...
	done
```

示例：使用while循环和until循环打印数字接龙，要求while循环输出1-5，until循环输出6-9。

```powershell
[root@localhost opt]# cat while-11.sh
#!/bin/bash
a=1
while [ $a -le 5 ];do
    echo $a
    let a++
    until [ $a -le 5 ];do
        echo $a
    let a++
    [ $a -eq 10 ] && break
    done
done

```

### 9.14 shell流程控制-case

#### 9.14.1 case介绍

在生产环境中，我们总会遇到一个问题需要根据不同的状况来执行不同的预案，那么我们要处理这样的问题就要首先根据可能出现的情况写出对应预案，根据出现的情况来加载不同的预案。 

特点：根据给予的不同条件执行不同的代码块 

```powershell
比如你去相亲：你会在脑子里出现以下的预案：
第一眼看到对方父亲，你应该说：伯父好
第一眼看到对方母亲，你应该说：伯母好
第一眼看到对方奶奶，你应该说：奶奶好
。。。。


而这个例子中触发就是你第一眼看到了对方的谁，预案则是叫什么称呼。

再来说一个计算机的相关例子---监控内存使用率
内存使用率低于80%，脚本输出： 绿色字体的 Memory use xx%
内存使用率大于80%小于90%，脚本输出： 黄色字体的 Memory use xx%
内存使用大于90%，脚本输出： 红色字体的 Memory use xx%
```

```powershell
# 语法
case $var in             定义变量;var代表是变量名
pattern 1)               模式1;用 | 分割多个模式，相当于or
    command1             需要执行的语句
    ;;                   两个分号代表命令结束
pattern 2)
    command2
    ;;
pattern 3)
    command3
    ;;
		  *)              default，不满足以上模式，默认执行*)下面的语句
    command4
    ;;
esac							esac表示case语句结束
```

示例：nginx启动脚本

```powershell
#!/bin/bash

# 1.variables
nginx_install_doc=/usr/local/nginx
proc=nginx
nginxd=$nginx_install_doc/sbin/nginx
pid_file=$nginx_install_doc/logs/nginx.pid

# 2.source function library
if [ -f /etc/init.d/functions ];then
    . /etc/init.d/functions
else
    echo "not found file /etc/init.d/functions"
    exit
fi

# 3.determine whether pid exists
if [ -f $pid_file ];then
    nginx_process_id=`cat $pid_file`
    nginx_process_num=`ps aux |grep $nginx_process_id |grep -v "grep" |wc -l`
fi

# 4.function_start
function start {
    # 如果nginx没有启动，直接移动，否则报已启动
    if [ -f $pid_file ] && [ $nginx_process_num -ge 1 ];then
        echo "nginx running......"
    # 如果pid文件存在，但是没有进程，说明上一次非法关闭了nginx,造成pid文件没有自动删除,所以启动nginx之前先删除旧的pid文件
    else
        if [[ -f $pid_file && $nginx_process_num -lt 1 ]];then
            rm -rf $pid_file
            # 两种方法执行命令，返回执行结果：
            # 1.daemon
            # 2.action

            # echo "nginx start `deamon $nginxd`"
            action "nginx start" $nginxd
        fi
        # echo "nginx start `deamon $nginxd`"
         action "nginx start" $nginxd
    fi
}

# 5.function_stop
function stop {
    # 判断nginx启动的情况下才会执行关闭，如果没有启动直接报错，或者提示用户服务没启动
    if [ -f $pid_file ] && [ $nginx_process_num -ge 1 ];then
        action "nginx stop" killall -s QUIT $proc
        rm -rf $pid_file
    else
        action "nginx stop" killall -a QUIT $proc 2>/dev/null
    fi
}

# 6.function_restart
function restart {
    stop
    sleep 1
    start
}

# 7.function_reload
function reload {
    # 重载的目的是让主进程重新加载配置文件,但是前提是服务必须开启
    # 这里先判断服务是否开启，开启就执行加载，没有开启直接报加载错误
    if [[ -f $pid_file && $nginx_process_num -ge 1 ]];then
        action "nginx reload" killall -s HUP $proc
    else
        action "nginx reload" killall -s HUP $proc 2>/dev/null
    fi
}

# 8.status
function status {
    if [[ -f $pid_file && $nginx_process_num -ge 1 ]];then
        echo "nginx running..."
    else
        echo "ningx stop"
    fi
}

# 9.callable
case $1 in
start) start;;
stop) stop;;
restart) restart;;
reload) reload;;
status) status;;
*) echo "USAGE: $0 start|stop|restart|reload|status";;
esac

```

### 9.15 shell函数

shell脚本中的代码是按照执行的优先级的顺序从上往下抒写的，代码量越大，在脚本调试的时候就越难排错，当因执行需要调整代码执行顺序的时候就需要不断的复制粘贴，或者删除部分代码来完成，这和从写一个脚本花费的时候相比甚至需要更长的时间。 

代码量大后遇到的问题：

- 单个脚本代码量大 （300-500行）
- 阅读修改耗时费力
- 排错困难
- 改变执行顺序困难

为了解决这些问题，我们可以把代码模块化，按需调用。

#### 9.15.1 函数定义

shell中允许将 **一组命令集合** 或 **语句** 形成一段 **可用代码** ，这些代码块称为shell函数。给这段代码起个名字称为函数名，后续可以直接调用该段代码的功能。

将完成一个功能的一段代码进行命名、封装

函数的优点：

1. 代码模块化，调用方便，节省内存
2. 代码模块化，代码量少，排错简单
3. 代码模块化，可以改变代码的执行顺序

**1. 函数的定义**

```powershell
语法一:

函数名 () {
    代码块
    return N
    }


语法二：
function 函数名 {
      代码块
      return N
      }
      
      
函数中return说明：
1.return可以结束一个函数，类似于前面讲的循环控制语句break(结束当前循环，执行循环体后面的代码)
2.return默认返回函数中最后一个命令的退出状态，也可以给定参数值，该参数值的范围是0-256之间。
3.如果没有return命令，函数将返回最后一个Shell的退出值。
```

**2. 函数调用**

- 当前命令行调用

```powershell
[root@localhost opt]# cat fun1.sh
#!/bin/bash
function hello {
    echo "hello world $1"
    hostname
}

menu(){
cat <<-EOF
1.mysql
2.web
3.app
4.exit
EOF
}

[root@localhost opt]# source fun1.sh
[root@localhost opt]# . fun1.sh
[root@localhost opt]# hello 6666
hello world 6666
localhost.localdomain
[root@localhost opt]# menu
1.mysql
2.web
3.app
4.exit
```

- 定义到用户变量中

```powershell
/etc/profile	/etc/bashrc		~/.bash_profile 	~/.bashrc
[root@localhost opt]# cat ~/.bashrc
# .bashrc

# User specific aliases and functions

alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3
export WORKON_HOME=$HOME/.virtualenvs
source /usr/local/bin/virtualenvwrapper.sh

注意：
当用户打开bash的时候会读取该文件
```

- 在脚本中调用

```powershell
[root@localhost opt]# cat while-12.sh
#!/bin/bash

source ./fun1.sh
menu(){
cat <<-END
    h   显示命令帮助
    f   显示磁盘分区
    d   显示磁盘挂载
    m   查看内存使用
    u   查看系统负载
    q   退出程序
END
}
menu

```

---

## 第十章 持续化集成

### 10.1 持续集成基础概念

**1. 持续集成**

持续集成（Continuous integration）是一种软件开发实践，即团队开发成员经常集成他们的工作，通常每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试)来验证，从而尽快地发现集成错误。许多团队发现这个过程可以大大减少集成的问题，让团队能够更快的开发内聚的软件。 

持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。 

**持续集成的好处主要有两个**: 

1. 快速发现错误，每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 

2. 防止分支大幅偏离主干，如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 

持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。 持续集成并不能消除 Bug，而是让它们非常容易的发现和改正。 

![1642994536826](J:\homework\Linux学习笔记\Linux学习笔记.assets\1642994536826.png)

**2. 持续交付**

持续交付（Continuous delivery）指的是，频繁地将软件的新版本，交付给质量团队或者用户，以供评审。如果评审通过，代码就进入生产阶段。 持续交付可以看作持续集成的下一步，它强调的是，不管怎么更新，软件是随时随地可以交付的。

![1642994556738](J:\homework\Linux学习笔记\Linux学习笔记.assets\1642994556738.png)

**3. 持续部署**

持续部署（continuous deployment）是持续交付的下一步，指的是代码通过评审以后， 自动部署到生产环境。 持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。持续部署的前提是能自动化完成测试、构建、部署等步骤。

**4. 持续集成的一般流程**

根据持续集成的设计，代码从提交到生产，整个过程有以下几步： 

**1. 提交** 

流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。 

**2. 测试（第一轮）** 

代码仓库对 commit 操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。 

**3. 构建** 

通过第一轮测试，代码就可以合并进主干，就算可以交付了。交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、JS 脚本、图片）等等。 常用的构建工具如下。jeknins、Travis、codeship 等 

**4. 测试（第二轮）** 

构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。第二轮是全面测试，单元测试和集成测试都会跑，有条件的话，也要做端对端测试。所有测试以自动化为主，少数无法自动化的测试用例，就要人工跑。 

**5. 部署** 

通过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本 的所有文件打包（ tar filename.tar * ）存档，发到生产服务器。生产服务器将打包文件，解包成本地的一个目录，再将运行路径的符号链接（symlink）指向这个目录，然后重新启动应用。这方面的部署工具有 Ansible，Chef，Puppet 等。 

**6. 回滚** 

一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。

### 10.2 认识 DevOps

#### 10.2.1 什么是DevOps

**1. 什么是DevOps**

DevOps 一词的来自于 Development 和 Operations 的组合，突出重视软件开发人员和运维人员的沟通合作，通过自动化流程来使得软件构建、测试、发布更加快捷、频繁和可靠。

目前对 DevOps 有太多的说法和定义，不过它们都有一个共同的思想：“解决开发者与运维者之间曾经不可逾越的鸿沟，增强开发者与运维者之间的沟通和交流”。而我个人认为，DevOps可以用一个公式表达：

​	文化观念的改变 + 自动化工具 = 不断适应快速变化的市场

​	强调：DevOps 是一个框架，是一种方法论，并不是一套工具，他包括一系列的基本原则和实践。

其核心价值在于以下两点：

​	更快速地交付，响应市场的变化，更多地关注业务的改进与提升。

**2. 为什么需要DevOps**

**2.1 产品迭代**

在现实工作中，往往都是用户不知道自己想要什么，但是当我们设计完一个产品后，他告诉我们他们不需要什么，这样我们的产品需要反复的迭代，而且过程可能是曲折的，那我们有什么好的办法快速的交付价值，灵活的响应变化呢？答案就是 DevOps。因为 DevOps是面向业务目标，助力业务成功的最佳实践。

**2.2 技术革新**

现在的 IT 技术架构随着系统的复杂化不断的革新，从最期的所有服务在一个系统中，发展到现在的微服务架构、从纯手动操作到全自动流程、从单台物理机到云平台。

![1643008013285](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643008013285.png)

**3. DevOps如何落地**

落实DevOps的指导思想：

- 高效的协作和沟通
- 自动化流程和工具
- 迅速敏捷的开发
- 持续交付和部署
- 不断学习和创新

![1643008234612](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643008234612.png)

敏捷管理：一支训练有素的敏捷开发团队是成功实施 DevOps 的关键。

持续交付部署：实现应用程序的自动化构建、部署、测试和发布。

通过技术工具，把传统的手工操作转变为自动化流程，这不仅有利于提高产品开发、运维部署的效率，还将减少人为因素引起的失误和事故，提早发现问题并及时地解决问题，这样也保证了产品的质量。下图展示了 DevOps 自动化的流程：

![1643008343092](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643008343092.png)

IT 服务管理：可持续的、高可用的 IT 服务是保障业务正常的关键要素，它与业务是一个整体。

IT 服务管理（ITSM），它是传统的“IT 管理”转向为“IT 服务”为主的一种模式，前者可能更关注具体服务器管理、网络管理和系统软件安装部署等工作；而后者更关注流程的规范化、标准化，明确定义各个流程的目标和范围、成本和效益、运营步骤、关键成功因素和绩效指标、有关人员的责权利，以及各个流程之间的关系等，比如建立线上事故解决流程、服务配置管理流程等；

​	精益管理：建立一个流水线式的 IT 服务链，打通开发与运维的鸿沟，实现开发运维一体化的敏捷模式。

​	精益生产主要来源于丰田生产方式 (TPS)的生产哲学，它以降低浪费、提升整体客户价值而闻名，它主要利用优化自动化流程来提高生产率、降低浪费。所以精益生产的精髓是即时制（JIT）和自动化（Jidoka）。

​	精益管理贯穿于整个 DevOps 阶段，它鼓励主动发现问题，不断的优化流程，从而达到持续交付、快速反馈、降低风险和保障质量的目的。

#### 10.2.2 实施DevOps

**建立快速敏捷的团队**

按照业务功能划分团队，建立沟通群组，设置产品负责人（一个策划人员）、Scrum Master（我们一般选择测试人员担任，测试驱动开发模式）和开发者团队（前端工程师、后端工程师、测试各一名），形成如下的组织结构和系统架构：

**提交：**工程师将代码在本地测试后，提交到版本控制系统，如 Git 代码仓库中。 

构建：持续整合系统（如 Jenkins CI），在检测到版本控制系统更新时，便自动从 Git 代码仓库里拉取最新的代码，进行编译、构建。 

**单元测试：**Jenkins 完成编译构建后，会自动执行指定的单元测试代码。 

**部署到测试环境：**在完成单元测试后，Jenkins 可以将应用程序部署到与生产环境相近 的测试环境中进行测试。 

**预生产环境测试：**在预生产测试环境里，可以进行一些最后的自动化测试，例如使用 Appium自动化测试工具进行测试，以及与实际情况类似的一些测试可由开发人员或客户人 员手动进行测试。 

**部署到生产环境：**通过所有测试后，便可以使用灰度更新将最新的版本部署到实际生产环境里。

**DepOps 在落地实施过程中经常会遇到的问题**

- 人手紧缺
- 跨部门协作，前期沟通培训成本高
- 前期投入工作量大见效少

#### 10.2.3 DevOps技术栈

**敏捷管理工具**

- Trello
- Teambition
- Worktile
- Tower

**产品&质量管理**

- confluence
- 禅道
- Jira
- Bugzila

其中 confluence 和禅道主要是产品的需求、定义、依赖和推广等的全面管理工具；而Jira 和 Bugzilla 是产品的质量管理和监控能力，包括测试用例、缺陷跟踪和质量监控等。目前我们使用 Jira 和禅道较多。

**代码仓库管理**

- Git
- Gitlab
- Github

Git 是一个开源的分布式版本控制系统；Gitlab 和 Github 是用于仓库管理系统的开源项目，它们使用 Git 作为代码管理工具，并在此基础上搭建起来的 web 服务。我们主要使用的是 Git 和 Gitlab。

**自动化构建脚本**

- Gradle
- Maven
- SBT
- ANT

**虚拟机与容器化**

- VMware
- VirtualBox
- Vagrant

- Docker

**持续集成（CI）&持续部署（CD）**

- Jenkins
- Hudson
- Travis CI
- CircleCI

Jenkins 是一个开源软件项目，是基于 Java 开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能，它的前身为Hudson。 

Travis CI 是目前新兴的开源持续集成构建项目，它与 jenkins 很明显的区别在于采用yaml 格式，简洁清新独树一帜。 

CircleCI 是一个为 web 应用开发者提供服务的持续集成平台，主要为开发团队提供测试，持续集成，以及代码部署等服务。

**自动化测试**

- Appium

Appium 是一个移动端的自动化框架，可用于测试原生应用，移动网页应用和混合型应用，且是跨平台的。可用于 IOS 和 Android 以及 firefox 的操作系统。

- Selenium

Selenium 测试直接在浏览器中运行，就像真实用户所做的一样。Selenium 测试可以在Windows、Linux 和 Macintosh 上的 Internet Explorer、Mozilla 和 Firefox 中运行。

- Mock 测试

Mock 测试就是在测试过程中，对于某些不容易构造或者不容易获取的对象，用一个虚拟的对象来创建以便测试的测试方法。这个虚拟的对象就是 Mock 对象，Mock 对象就是真实对象在调试期间的代替品。Java 中的 Mock 框架常用的有 EasyMock 和 Mockito 等。

- 消费者驱动契约测试

契约测试是一种针对外部服务的接口进行的测试，它能够验证服务是否满足消费方期待的契约。当一些消费方通过接口使用某个组件的提供的行为时，它们之间就产生了契约。这个契约包含了对输入和输出的数据结构的期望，性能以及并发性。而 PACT 是目前比较流的消费者驱动契约测试框架。

**自动化运维工具**

- Ansible
- Puppet
- Chef
- SaltStack

**监控管理工具**

- Zabbix

Zabbix 是一个基于 WEB 界面的提供分布式系统监视以及网络监视功能的企业级开源解决方案。

- ELK Stack 日志分析系统

ELK Stack 是开源日志处理平台解决方案，背后的商业公司是 Elastic。它由日志采集解析工具 Logstash、基于 Lucene 的全文搜索引擎 Elasticsearch、分析可视化平台Kibana 三部分组成。

- 云监控（如 Amazon CloudWatch）

Amazon CloudWatch 是一项针对 AWS 云资源和在 AWS 上运行的应用程序进行监控的服务。您可以使用 Amazon CloudWatch 收集和跟踪各项指标、收集和监控日志文件、设置警报以及自动应对 AWS 资源的更改。

### 10.3 Git基本使用

- 初始化

```shell
git init
```

- 查看当前状态

```powershell
git status
```

- 提交到暂存区

```powershell
git add .
```

- 从暂存区回滚到本地

```python
git rm --cached a
```

- 删除暂存区的文件同时删除本地文件

```powershell
git rm -f b
```

- 同时修改暂存区和本地的文件名

```python
git mv a  a.txt
```

- 比对本地与暂存区的区别

```python
git diff a
```

- 将暂存区的文件提交到版本库

```powershell
git commit -m "提交备注"
```

- 比对暂存区和版本库的区别

```powershell
git diff --cached a
```

- 查看提交记录

```powershell
git log
```

- 查看详细提交记录

```powershell
git log --online			# 查看提交记录，简写形式
git log --online --decorate	# 查看提交记录，简写形式，包括分支
git log -p					# 查看提交记录，详细信息
```

- 查看历史操作

```powershell
git reflog
```

- 回滚到指定的版本，**这个操作等于 --soft + head + checkout  以下**

```powershell
git reset --hard 版本号
```

- 1.指定某个版本从版本库回滚到缓存区

```powershell
git reset --soft eedd50d77f1ad
```

- 2.从暂存区回滚到本地，重新提交add

```powershell
git reset head a
```

- 3.暂存区的的文件覆盖本地文件内容--危险！

```powershell
git checkout -- a
```

- 指定某个版本从版本库回滚到本地，重新提交，**这个操作等于 --soft + head** 以上

```powershell
git reset --mix eedd50d77f1a
```

- 创建分支

```powershell
git branch dev
```

- 查看分支

```powershell
git branch
```

- 切换分支

```powershell
git checkout 
```

- 合并分支

```powershell
git merge dev
```

- 删除分支

```powershell
git branch -d dev
```

-  创建标签

```powershell
git tag -a v1.0
```

- 删除标签

```powershell
git tag -d v1.0
```

- 查看标签

```powershell
git show v1.0
git tag
```

### 10.4 Gitlab

GitLab 是一个用于仓库管理系统的开源项目。使用Git作为代码管理工具，并在此基础上搭建起来的web服务。可通过Web界面进行访问公开的或者私人项目。它拥有与Github类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序(Wall)进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用。

> 国内镜像下载地址：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/

**1. 安装依赖**

```powershell
yum install curl policycoreutils openssh-server openssh-clients policycoreutils-python
systemctl enable sshd
systemctl start sshd
yum install postfix
systemctl enable postfix
systemctl start postfix
```

**2. 获取rpm安装包**

```powershell
cd /usr/local/src/
wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-10.0.6-ce.0.el7.x86_64.rpm
rpm -ivh gitlab-ce-10.0.6-ce.0.el7.x86_64.rpm
```

**3. 修改配置**

```powershell
vim /etc/gitlab/gitlab.rb
# 修改如下信息
external_url 'http://192.168.88.128'
gitlab-ctl reconfigure	# 重新配置gitlab
```

**4. 启动**

重新配置gitlba后，在浏览地址栏中输入 http://192.168.88.128

#### 10.4.1 常用命令

```powershell
gitlab-ctl start	# 启动全部服务
gitlab-ctl restart	# 重启全部服务
gitlab-ctl stop		# 停止全部服务
gitlab-ctl reconfigure	# 使配置文件生效（一般修改完主配置文件/etc/gitlab/gitlab.rb，需要执行此命令）
gitlab-ctl show-config	# 验证配置文件
gitlab-ctl uninstall	# 删除gitlab（保留数据）
gitlab-ctl cleanse	# 删除所有数据，从新开始
gitlab-ctl tail <service name>	# 查看服务的日志
```

#### 10.4.2 服务与目录

> nginx：静态Web服务器
>
> gitlab-shell：用于处理Git命令和修改authorized keys列表，我们的gitlab是以Git做为最层的，你操作实际上最后就是调用gitlab-shell命令进行处理。
>
> gitlab-workhorse:轻量级的反向代理服务器
>
> logrotate：日志文件管理工具
>
> postgresql：数据库
>
> redis：缓存数据库
>
> sidekiq：用于在后台执行队列任务（异步执行）
>
> unicorn：GitLab Rails应用是托管在这个服务器上面的

```powershell
/var/opt/gitlab/git-data/repositories	# 库默认存储目录
/opt/gitlab	# 应用代码和相应的依赖程序
/var/opt/gitlab	# gitlab-ctl reconfigure命令编译后的应用数据和配置文件，不需要人为修改配置
/etc/gitlab	# 配置文件目录
/var/log/gitlab	# 此目录下存放了gitlab各个组件产生的日志
/var/opt/gitlab/backups	# 备份文件生成的目录
```

#### 10.4.3 Gitlab基本配置

**1.关闭注册**

由于我们Gitlab系统是私有仓库，一般用户都是由管理员创建和分派的，所以我们需要关闭注册。

![1643026716504](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643026716504.png)

![1643026767340](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643026767340.png)



**2. 创建组**

group（项目）下面可以创建subgroup，创建project（项目下的具体工程），添加user。group就是把相关的project或者user放在一起，进行统一的权限管理。

![1643026806614](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643026806614.png)

![1643026915121](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643026915121.png)

visibility Level：选择谁可以访问该组：我们默认选择private,因为我建设的是私有仓库

Private:只有授权的用户才可以看到

Internal：只要是登录gitlab的用户就可以看到

Public：只要可以访问gitlab web页面的人就可以看到

**3. 创建项目**

![1643027943935](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643027943935.png)

选择对应的组

![1643027917707](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643027917707.png)

**4. 创建用户**

![1643027614970](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643027614970.png)

添加用户到组

![1643027656004](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643027656004.png)

选择角色

![1643027687961](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643027687961.png)

**5. 添加SSH Key**

生成公钥

```powershell
[root@localhost ~]# ssh-keygen -t rsa
[root@localhost ~]# cat .ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCuDQc9AnjAuRbJ891lJlzDKmdojxhG7rB9ZmhY2triib2z/gsIV+ejBgxv8bYVk9YZ8q5N+TIg3UG4sB44zMbDUprdDajSuRGUebUMYWuHaYAGvWloMpjEcBTnX874VgAUZmHF3wyKuCPKkTOqcIfMxA63WjpMb6Zm62LxmNv1x50omteBFcliepFgAHVXIfFRYQ62SHK4ylPhowRIL94WDTqxlEljrL34Xt7VCCzgoWN3ieIB3QWMrMaRagvmEoQ3bui4XW6O4C0aixhD2VQN9UWVaRjucix3k17XhyoARfW4vunXztDwcRDB8rE/0nAfPZ/TjeY49JYrmZtHPiF1 root@localhost.localdomain
```

绑定到root用户

![1643028551476](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643028551476.png)

![1643028572522](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643028572522.png)

把代码推到gitlab

```powershell
[root@localhost git_test]# git remote add test_gitlab git@192.168.88.128:test_Gitlab/git_test.git
[root@localhost git_test]# git push test_gitlab master
```

**6. clone到dev用户的机器**

```powershell
[root@localhost ~]# yum install -y git
# 1.生成公钥
[root@localhost ~]# ssh-keygen -t rsa
# 2.查看公钥，配置到dev用户
[root@localhost ~]# cat .ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDLaRtrkYgDWNvWcD8O0Rcn9uhTvGckIzLCs8XHuopdVRYYazsKPEyXs1dlxlOXPIGsjcv/a1ik4APtPfvCXVOzlpTjNDnyB/SEnvRu2mUUPiVlMbF3LjGH7qiKNMiv3szgWrJeJaZfcZUa+QRWLVbOUOnHbH1rlZUWrkorY+LQw9thjgyl+OQXmI5G6ReY1S1NLL/wZo3dmzY7TE74oCMNYavKGKYPyxGZ/MHnyK/QDXVvPNRoIypjuqmtbwOLZ3T7S44LinJyR/2WlkliDQHSI0LJQatPtZXNIi54A7grd/T2YjqXu+NbmDcaGPFXDz2xJ0H3XHXDioqKcmrBLycf root@localhost.localdomain
# 3.clone到本地
[root@localhost ~]# git clone git@192.168.88.128:test_Gitlab/git_test.git
# 4.创建一个文件
[root@localhost ~]# touch dev
# 5.提交到远程仓库
[root@localhost git_test]# git add .
[root@localhost git_test]# git commit -m "commit dev to dev branch"
[root@localhost git_test]# git push origin dev
```

**7. root用户设置master分支保护**

![1643099304281](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643099304281.png)

![1643099392703](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643099392703.png)

```powershell
# dev用户不允许推master分支，dev用户没有master和owner权限
[root@localhost git_test]# git push origin -u master
Total 0 (delta 0), reused 0 (delta 0)
remote: GitLab: You are not allowed to push code to protected branches on this project.
To git@192.168.88.128:test_Gitlab/git_test.git
 ! [remote rejected] master -> master (pre-receive hook declined)
error: failed to push some refs to 'git@192.168.88.128:test_Gitlab/git_test.git'
```

**8. 用户提交dev分支代码，提合并代码申请**

![1643100181876](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643100181876.png)

![1643100255343](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643100255343.png)

root用户进行合并

![1643100912718](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643100912718.png)

#### 10.4.4 Gitlab备份管理

默认的备份文件目录为：/var/opt/gitlab/backups，如果自定义备份目录需要赋予目录 git 权限，具体操作如下：

```powershell
# 修改配置文件
[root@localhost git_test]# vim /etc/gitlab/gitlab.rb
gitlab_rails['backup_path'] = '/data/backup/gitlab'
gitlab_rails['backup_keep_time'] = 604800

# 使用配置生效
[root@localhost git_test]# gitlab-ctl reconfigure

# 自动创建了配置的目录，如果没有创建，则需要手动创建并修改权限
[root@localhost git_test]# mkdir /data/backup/gitlab -p
[root@localhost git_test]# chown -R git.git /data/backup/gitlab

# 执行备份命令
[root@localhost backup]# gitlab-rake gitlab:backup:create
[root@localhost gitlab]# ll /data/backup/gitlab
total 2812
-rw------- 1 git git 2877440 Jan 26 15:53 1643183615_2022_01_26_10.2.2_gitlab_backup.tar

```

测试：删库再恢复

![1643102320577](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643102320577.png)

GitLab 的恢复只能还原到与备份文件相同的 gitlab 版本的系统中，恢复时，停止连接到数据库的进程（也就是停止数据写入服务），但是保持 GitLab 是运行的。

```powershell
[root@localhost gitlab]# gitlab-ctl stop unicorn
ok: down: unicorn: 0s, normally up
[root@localhost gitlab]# gitlab-ctl stop sidekiq
ok: down: sidekiq: 1s, normally up
[root@localhost gitlab]# gitlab-ctl status
run: gitaly: (pid 902) 18982s; run: log: (pid 900) 18982s
run: gitlab-monitor: (pid 904) 18982s; run: log: (pid 903) 18982s
run: gitlab-workhorse: (pid 914) 18982s; run: log: (pid 905) 18982s
run: logrotate: (pid 36036) 981s; run: log: (pid 906) 18982s
run: nginx: (pid 915) 18982s; run: log: (pid 912) 18982s
run: node-exporter: (pid 893) 18982s; run: log: (pid 888) 18982s
run: postgres-exporter: (pid 899) 18982s; run: log: (pid 898) 18982s
run: postgresql: (pid 894) 18982s; run: log: (pid 882) 18982s
run: prometheus: (pid 892) 18982s; run: log: (pid 886) 18982s
run: redis: (pid 895) 18982s; run: log: (pid 887) 18982s
run: redis-exporter: (pid 890) 18982s; run: log: (pid 889) 18982s
down: sidekiq: 13s, normally up; run: log: (pid 896) 18982s
down: unicorn: 60s, normally up; run: log: (pid 885) 18982s

# 执行数据恢复命令
[root@localhost gitlab]# gitlab-rake gitlab:backup:restore BACKUP=1643183615_2022_01_26_10.2.2

# 重启服务即可
[root@localhost gitlab]# gitlab-ctl restart
```

#### 10.4.5 Gitlab升级

```powershell
# 关闭部分服务
[root@localhost gitlab]# gitlab-ctl stop unicorn
[root@localhost gitlab]# gitlab-ctl stop nginx
[root@localhost gitlab]# gitlab-ctl stop sidekiq

# 执行升级命令
[root@localhost gitlab]# rpm -Uvh gitlab-ce-10.0.4-ce.0.el7.x86_64.rpm

# 重启服务即可
[root@localhost gitlab]# gitlab-ctl restart
```

<font color="red">注：升级操作不建议进行。如果确实需要，也可以采取在一台新的服务器上安装新版本的 Gitlab，然后采用导入库的方式将旧系统的代码仓库导入到新 Gitlab 上。</font>

### 10.5 Jenkins

#### 10.5.1 Jenkins安装配置

Jenkins 官方网站及清华镜像站下载 jenkins 安装包，可以使用 YUM 方式安装 JDK1.8 版本

```powershell
# 1.安装jdk
[root@localhost src]# rpm -ivh jdk-8u121-linux-x64.rpm

# 2.安装Jenkins
[root@localhost src]# rpm -ivh jenkins-2.99-1.1.noarch.rpm

# 3.启动Jenkins
[root@localhost src]# systemctl start jenkins
[root@localhost src]# systemctl status jenkins
● jenkins.service - LSB: Jenkins Automation Server
   Loaded: loaded (/etc/rc.d/init.d/jenkins; bad; vendor preset: disabled)
   Active: active (exited) since Wed 2022-01-26 19:30:34 CST; 14s ago
     Docs: man:systemd-sysv-generator(8)
  Process: 57691 ExecStart=/etc/rc.d/init.d/jenkins start (code=exited, status=0/SUCCESS)

Jan 26 19:30:33 localhost.localdomain systemd[1]: Starting LSB: Jenkins Automation Server...
Jan 26 19:30:33 localhost.localdomain runuser[57696]: pam_unix(runuser:session): session opened for user jenkins by (uid=0)
Jan 26 19:30:34 localhost.localdomain runuser[57696]: pam_unix(runuser:session): session closed for user jenkins
Jan 26 19:30:34 localhost.localdomain jenkins[57691]: Starting Jenkins [  OK  ]
Jan 26 19:30:34 localhost.localdomain systemd[1]: Started LSB: Jenkins Automation Server.

# 4.查看端口
[root@localhost src]# netstat -nuplt | grep 8080
tcp6       0      0 :::8080                 :::*                    LISTEN      2219/java
```

浏览器输入 http://您服务器的ip地址:8080，访问 jenkins 服务

![1643116431683](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643116431683.png)

查看默认密码

```powershell
[root@localhost src]# cat /var/lib/jenkins/secrets/initialAdminPassword
268183ea81ea4352a35a470ba039b78c
```

此页面要用户选择初始化安装的插件，我们选择跳过此步，后面我们采用其他方式安装插件。

![1643117194496](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643117194496.png)

点击Jenkins安装配置

![1643117593143](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643117593143.png)

进入Jenkins页面

![1643117502811](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643117502811.png)

#### 10.5.2 Jenkins插件管理

Jenkins 本身是一个引擎、一个框架，只是提供了很简单功能，其强大的功能都是通过插件来实现的，jenkins 有一个庞大的插件生态系统，为 Jenkins 提供丰富的功能扩展。

**1. 自动安装**

![1643120871088](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643120871088.png)

进入插件管理页面，点击可选插件，选择你需要安装的插件

![1643120922700](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643120922700.png)

**2. 手动安装插件**

除了上面的插件安装方法，Jenkins 还为我们提供了手工安装插件的方式，特别是在国内，由于网络的原因，有时候我们使用上述方法安装插件会经常不成功，所以我们可以采用下载插件，然后再上传的方式来安装插件。

> 官方的插件下载地址：http://updates.jenkins-ci.org/
>
> 国内的源：https://mirrors.tuna.tsinghua.edu.cn/jenkins/plugins/

![1643125495364](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643125495364.png)

![1643125621374](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643125621374.png)

下载完成后，手动上传

![1643125698207](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643125698207.png)

#### 10.5.3 覆盖插件安装

我们可以备份已经安装好插件的 Jenkins 服务器上的/var/lib/jenkins/plugins 目录，然后把备份文件上传到我们需要安装插件的新 Jenkins 服务器的对应目录上，然后重启Jenkins。

这种方法其实给我们提供了一种更加快速的安装 Jenkins 插件的方法。建议在初始安装jenkins 时，可以使用此方法，其他时候尽量使用前两种方式。我们本教程使用此方式安装插件。前面我们在初始化 jenkins 的时候，跳过了插件的安装，现在我们的 Jenkins 插件目录为空，因为我们没有安装任何插件：

```powershell
[root@localhost src]# cd /var/lib/jenkins/plugins/
[root@localhost plugins]# ll
total 0
[root@localhost plugins]# ll
-rw-r--r--.   1 root root 232436856 Jan 26 23:55 plugins.tar.gz
[root@localhost plugins]# tar -zxvf plugins.tar.gz
[root@localhost plugins]# cd plugins
[root@localhost plugins]# mv * ../
[root@localhost plugins]# systemctl restart jenkins
```

重启到我们在插件管理页面可以看到我们已经安装的插件

![1643126492015](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643126492015.png)

#### 10.5.4 Jenkins 常用目录及文件

学习 Jenkins，首先要明白一点，那就是 jenkins 下一切兼文件，也就是说 jenkins 没有数据库，所有的数据都是以文件的形式存在，所以我要了解 Jenkins 的主要目录及文件，通过命令我们可以查看到所有的 jenkins 目录及文件的位置。

```powershell
[root@localhost plugins]# rpm -ql jenkins
/etc/init.d/jenkins	# Jenkins启动文件
/etc/logrotate.d/jenkins
/etc/sysconfig/jenkins	# 配置文件目录
/usr/lib/jenkins
/usr/lib/jenkins/jenkins.war
/usr/sbin/rcjenkins
/var/cache/jenkins	# 程序文件目录
/var/lib/jenkins
/var/log/jenkins	# 日志文件目录
```

/etc/sysconfig/jenkins是Jenkins的主配置文件：我们在这里主要配置Jenkins的工作目录、启动用户、启动端口。

![1643127237682](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643127237682.png)

Jenkins默认的用户为jenkins,强烈建议生产环境使用jenkins用户，然后使用sudo进行授权。

```powershell
[root@localhost jenkins]# ll /var/lib/jenkins
total 68
-rw-------.   1 jenkins jenkins   788 Jan 26 23:59 com.dabsquared.gitlabjenkins.connection.GitLabConnectionConfig.xml
-rw-------.   1 jenkins jenkins   365 Jan 26 23:59 com.dabsquared.gitlabjenkins.GitLabPushTrigger.xml
-rw-------.   1 jenkins jenkins  1822 Jan 26 23:59 config.xml
-rw-------.   1 jenkins jenkins   156 Jan 26 23:59 hudson.model.UpdateCenter.xml
-rw-------.   1 jenkins jenkins   370 Jan 26 23:59 hudson.plugins.git.GitTool.xml
-rw-------.   1 jenkins jenkins  1712 Jan 26 21:08 identity.key.enc
-rw-------.   1 jenkins jenkins    94 Jan 26 21:08 jenkins.CLI.xml
-rw-r--r--.   1 jenkins jenkins     4 Jan 26 21:27 jenkins.install.InstallUtil.lastExecVersion
-rw-r--r--.   1 jenkins jenkins     4 Jan 26 21:27 jenkins.install.UpgradeWizard.state
drwxr-xr-x.   2 jenkins jenkins     6 Jan 26 21:08 jobs
drwxr-xr-x.   4 jenkins jenkins    37 Jan 26 23:59 logs
-rw-------.   1 jenkins jenkins   907 Jan 26 23:59 nodeMonitors.xml
drwxr-xr-x.   2 jenkins jenkins     6 Jan 26 21:08 nodes
drwxr-xr-x. 116 jenkins jenkins 12288 Jan 26 23:57 plugins
-rw-------.   1 jenkins jenkins   129 Jan 26 23:59 queue.xml.bak
-rw-------.   1 jenkins jenkins    64 Jan 26 21:08 secret.key
-rw-r--r--.   1 jenkins jenkins     0 Jan 26 21:08 secret.key.not-so-secret
drwx------.   4 jenkins jenkins  4096 Jan 26 21:08 secrets
-rw-r--r--.   1 jenkins jenkins     0 Jan 27 00:18 ThinBackup Worker Thread.log
drwxr-xr-x.   2 jenkins jenkins    24 Jan 26 21:08 userContent
drwxr-xr-x.   3 jenkins jenkins    19 Jan 26 21:08 users
drwxr-xr-x.   2 jenkins jenkins     6 Jan 26 23:59 workflow-libs
```

其中主要的目录为jobs目录：存放jobs的配置及每次构建的结果；plugins目录：Jenkins插件目录，存放我们已经安装的插件；worksspace：工作区目录。

#### 10.5.5 创建一个freestyle

![1643130887221](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643130887221.png)

勾选丢弃旧的构建：

![1643131044560](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643131044560.png)

由于每次构建都会生成很多构建产物，如果频繁构建会占用很多的空间，我们可以通过这几个选项控制构建产物的保留。一般建议选择保留最近5-10的构建为适宜，他会保留最后一次成功构建的job。

![1643131222322](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643131222322.png)

保存后构建

![1643131419806](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643131419806.png)

![1643131748202](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643131748202.png)

构建成功

![1643131855836](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643131855836.png)

在 /var/lib/jenkins/jobs/my-freestyle-job 目录可以看到具体的配置及构建的目录文件。

#### 10.5.6 连接gitlab获取代码

我们使用上面的 job 进行配置，在“源码管理”部分配置拉取 Gitlab 上的 monitor 仓库，该仓库是一个纯 html 代码项目，首先在 Gitlab 上复制仓库地址。

![1643354202322](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643354202322.png)

然后回到 Jenkins 上 My-freestyle-job 配置页面，下拉到“源码管理”部分，勾选 git选项

![1643354421340](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643354421340.png)

如果报错有以下两种情况：

1.没有安装git

2.key认证失败，需要配置ssh认证

![1643355344574](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643355344574.png)

![1643354689050](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643354689050.png)

根据提示信息显示为 key 认证失败，因为我们使用的 SSH 方式连接仓库，所以需要配置SSH 认证，实际上在前面我们学习 Gitlab 的时候，我们已经配置了 ci-node2 这台机子的 root用户的公钥在 Gitlab 上的 dev 用户。

根据提示添加用户认证后，回到配置仓库页面，选择认证方式为新添加的认证，错误消失。

![1643356282095](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643356282095.png)

保存配置后，回到 job 主页面，点击“立即构建”，构建完成后，我们在工作空间内可以看到从 Gitlab 仓库拉到的代码。

![1643356366912](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643356366912.png)

同时我们在“console output”页面可以看到整个控制台输出内容。

![1643356470898](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643356470898.png)

![1643356532538](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643356532538.png)

在“源码管理”配置部分，我们可以配置从分支获取代码，也可以配置从标签获取代码。

#### 10.5.7 用 linux 脚本实现部署

##### 10.5.7.1 **配置** **ssh** 免密登录

因为我们要使用脚本将 ci-node2 上的程序代码推送到 ci-node1 上，所以需要配置ci-node2 到 ci-node1 的 ssh 免密码登录。

```powershell
ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.88.128
ssh 192.168.88.128
```

**部署脚本如下：**deploy.sh

```powershell
#!/bin/bash
#目标服务器 IP 地址
host=$1
#job 名称
job_name=$2
#包名
name=web-$(date +%F)-$(($RANDOM+10000))
#打包
cd /var/lib/jenkins/workspace/${job_name} && tar czf
/opt/${name}.tar.gz ./*
#发送包到目标服务器
ssh ${host} "cd /var/www/ && mkdir ${name}"
scp /opt/${name}.tar.gz $host:/var/www/${name}
#解包
ssh ${host} "cd /var/www/${name} && tar xf ${name}.tar.gz && rm -f
${name}.tar.gz"
#使用软链接方式部署服务
ssh ${host} "cd /var/www && rm -rf html && ln -s /var/www/${name}
/var/www/html"
```

**Jenkins配置构建：**

接下来我们在 Jenkins 上配置构建执行我们编写的部署脚本，回到 My-freestyle-job 配置页面，配置构建：

![1643360440022](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643360440022.png)

保存配置，回到 job 主页面，点击“立即构建”后，访问 httpd 服务的主页面，我们发现服务已经部署成功。

#### 10.5.8 Git push 触发自动构建

在上面的 job 中，我们已经成功实现了将 Gitlab 中的代码部署到 httpd服务中，但是每次部署需要我们手动去点击“立即构建”，下面我们将实现当 Gitlab 收到push 请求后，就触发 Jenkins 构建，将仓库的变化部署到 httpd 服务中。

##### 10.5.8.1 **Jenkins job** 配置构建触发器

回到 My-freestyle-job 的配置页面，下拉到构建触发器部分，

![1643369634258](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643369634258.png)

勾选 gitlab 触发选项，进入具体配置页面

![1643369924879](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643369924879.png)

配置完成后保存

![1643370081437](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643370081437.png)

进入集成配置页面，复制 jenkins 触发器配置页面的 url 及 Token，配置完成后，在客户端执行push操作。

#### 10.5.9 配置构建后操作

构建完成后，jenkins 可以把构建的结果反馈给 Gitlab，这样在 Gitlab 上就可以查看每一次 push 后构建的执行结果。

首先在 Jenkins 上配置，可以访问 Gitlab，打开 jenkins 系统管理--系统设置页面，下拉找到 Gitlab 部分

![1643371647983](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643371647983.png)

生成API Token

![1643371764280](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643371764280.png)

添加认证

![1643371909657](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643371909657.png)

![1643371997992](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643371997992.png)

其次，在 job 配置页面添加构建后操作

![1643372103642](J:\homework\Linux学习笔记\Linux学习笔记.assets\1643372103642.png)

保存 job 配置，回到 job 主页面，执行“立即构建”。构建成功后，在 Gitlab 仓库，commits 列表页面。



## 附录 注意事项

### 1.  问题记录

#### 1. mob disabled解决办法

安装xorg-x11-xauth

```python
yum install xorg-x11-xauth -y
```

![image-20211009183345880](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211009183345880.png)

打了绿色的勾说明已经解决

#### 2.uwsgi端口被占用

提示：端口被占用，无法启用

```shell
probably another instance of uWSGI is running on the same address (192.168.254.130:8000).
bind(): Address already in use [core/socket.c line 769]
```

解决：

```shell
# 安装fuser这个命令
yum install -y psmisc

# 杀掉这个端口再重启
sudo fuser -k 8001/tcp  # (8001需要填你的端口)
```

#### 3.docker服务启动报错

启动docker报错，查看docker状态报错信息如下：

![image-20211106101208350](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211106101208350.png)

解决方案：卸载docker重新安装新的docker

```shell
# 1.卸载Docker，旧版本docker没安装成功直接卸载掉。
# 查看安装过的包
[root@localhost ~]# yum list installed | grep docker
docker.x86_64                           2:1.13.1-208.git7d71120.el7_9  @extras
docker-client.x86_64                    2:1.13.1-208.git7d71120.el7_9  @extras
docker-common.x86_64                    2:1.13.1-208.git7d71120.el7_9  @extras

# 2.删除安装的Docker相关的软件包
[root@localhost ~]# yum -y remove docker.x86_64 docker-client.x86_64 docker-common.x86_64 -y
# 2.1 删除镜像、容器等
rm -rf /var/lib/docker/

# 3.把yum包更新到最新（时间可能有点长，慢慢等待......）
yum update

# 4.重新安装
yum intall docker -y

# 5.启动docker容器
[root@localhost lib]# systemctl start docker
[root@localhost lib]# systemctl status docker
```

![image-20211106101500854](J:\homework\Linux学习笔记\Linux学习笔记.assets\image-20211106101500854.png)

#### 4. elink命令

```shell
elinks http://192.168.254.130  --dump
```

#### 5.linux没有ip地址

```shell
# 确认ONBOOT是否开启
cd /etc/sysconfig/network-scripts
vi ifcfg-ens33
```

#### 6.vim设置缩进4格

```shell
vim /etc/vimsrc

set tabstop=4  # 表示一个 tab 显示出来是多少个空格的长度，默认 8
set softtabstop=4  # 表示在编辑模式的时候按退格键的时候退回缩进的长度，当使用 expandtab 时特别有用
set shiftwidth=4  # 表示每一级缩进的长度，一般设置成跟 softtabstop 一样
set noexpandtab / expandtab  # 当设置成 expandtab 时，缩进用空格来表示，noexpandtab 则是用制表符表示一个缩进
```

#### 7.以不安全的方式连接keepalived

```shell
# 1.先安装下边的包
[root@localhost src]# yum install ca-certificates -y

# 2.然后再获取tar包
[root@localhost src]# wget http://www.keepalived.org/software/keepalived-2.0.8.tar.gz -P /usr/src/
```

#### 8.  使用git clone项目报错

```powershell
# 错误: remote: The project you were looking for could not be found.

# 解决，清除本地git账户，重新输入用户名与密码
git config --system --unset  credential.helper
```

